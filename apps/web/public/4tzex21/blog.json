{"version":"https://jsonfeed.org/version/1.1","id":"4tzex21","title":"Stories by Mark Rubin on Medium","description":"Stories by Mark Rubin on Medium","language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","category":"Social Sciences","homePageUrl":"https://medium.com/@rubinpsyc?source=rss-c222612e11a4------2","feedUrl":"https://medium.com/feed/@rubinpsyc","favicon":"https://cdn-images-1.medium.com/fit/c/150/150/0*uCUPpslzlmB4BHIe.jpg","generator":"Medium","items":[{"id":"https://medium.com/p/8f55d6d52d09","title":"Exploratory hypothesis tests can be more compelling than confirmatory hypothesis tests","link":"https://rubinpsyc.medium.com/exploratory-hypothesis-tests-can-be-more-compelling-than-confirmatory-hypothesis-tests-8f55d6d52d09?source=rss-c222612e11a4------2","published":"Sat, 27 Aug 2022 00:09:41 GMT","description":"","isPermalink":false,"tags":["replication-crisis","hypothesis-testing","pre-registration","philosophy-of-science"],"authors":[{"name":"Mark Rubin","url":null}],"image":null,"modified":null,"contentHtml":"<p>Researchers often distinguish between:</p><p>(1) Exploratory hypothesis tests — unplanned tests of post hoc hypotheses that may be based on the current results, and</p><p>(2) Confirmatory hypothesis tests — planned tests of a priori hypotheses that are independent from the current results</p><p>This distinction is supposed to be useful because exploratory results are assumed to be more “tentative” and “open to bias” than confirmatory results. In this <a href=\"https://doi.org/10.1080/09515089.2022.2113771\">paper</a>, we challenge this assumption and argue that exploratory results can be <strong><em>more </em></strong>compelling than confirmatory results.</p><p>Our article has three parts. In the first part, we demonstrate that the same data can be used to generate and test a hypothesis in a transparently valid manner. We agree that circular reasoning can invalidate some exploratory hypothesis tests. However, circular reasoning can be identified by checking the <strong><em>contents </em></strong>of the reasoning without knowing the <strong><em>timing </em></strong>of that reasoning (i.e., a priori or post hoc).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*bzE3T0N_RJYeBZBUZPIDXw.png\" /><figcaption>Figure 1. An illustration of two ways in which exploratory data analyses may provide legitimate support for post hoc hypotheses</figcaption></figure><p>In the second part of our article, we argue that exploratory hypothesis tests can have several evidential <strong><em>advantages </em></strong>over confirmatory tests and, consequently, they have the potential to deliver more compelling research conclusions. In particular, exploratory hypothesis tests:</p><p>✅ avoid researcher commitment and prophecy biases</p><p>✅ reduce the motive for data fraud</p><p>✅ are more appropriate following unplanned deviations</p><p>✅ facilitate inference to the best explanation</p><p>✅ allow peer reviewers to contribute to exploratory analyses</p><p>Finally, in the third part of our article, we consider several potential *disadvantages* of exploratory hypothesis tests and conclude that these potential disadvantages may not be problematic. In particular, exploratory hypotheses tests are not necessarily disadvantaged due to:</p><p>✅overfitting</p><p>✅bias</p><p>✅HARKing</p><p>✅unacceptable research practices</p><p>And they:</p><p>✅are usually necessary</p><p>✅can be falsified</p><p>✅can predict anything but may suffer an evaluative cost in doing so</p><p>To be clear, our claim is not that exploratory hypothesis tests are <strong><em>always </em></strong>more compelling than confirmatory tests or even that they are <strong><em>typically </em></strong>more compelling. Our claim is only that exploratory tests <strong><em>can be</em></strong> more compelling in specific research situations. More generally, we encourage researchers to evaluate specific tests and results on a case-by-case basis rather than to follow simplistic heuristics such as “exploratory results are more tentative,” which represents a form of methodolatory.</p><p>Our paper builds on some of my previous work on <a href=\"https://doi.org/10.20982/tqmp.16.4.p376\">preregistration </a>and <a href=\"https://drive.google.com/file/d/1bGIUjHSEAoJYJke6RWtBphXJjZLr1UeX/view\">HARKing</a>. And please check out Szollosi and Donkin’s (2021) <a href=\"https://doi.org/10.1177/1745691620966796\">paper</a> on “the misguided distinction between exploratory and confirmatory research.”</p><p>For more info, please see our open access article:</p><p>Rubin, M., & Donkin, C. (2022). Exploratory hypothesis tests can be more compelling than confirmatory hypothesis tests. <em>Philosophical Psychology</em>. <a href=\"https://doi.org/10.1080/09515089.2022.2113771\">https://doi.org/10.1080/09515089.2022.2113771</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8f55d6d52d09\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/a6947b10385f","title":"One-Sided Significance Tests","link":"https://rubinpsyc.medium.com/two-sided-significance-tests-a6947b10385f?source=rss-c222612e11a4------2","published":"Mon, 04 Apr 2022 00:53:13 GMT","description":"","isPermalink":false,"tags":["significance-testing"],"authors":[{"name":"Mark Rubin","url":null}],"image":null,"modified":null,"contentHtml":"<p>In this paper (<a href=\"https://drive.google.com/file/d/1Qww86RX2l6LSz9jkMvjJ4Rs2hp38mwJp/view?usp=sharing\">Rubin, 2022</a>), I make two related points: (1) researchers should halve two-sided <em>p</em> values if they wish to use them to make directional claims, and (2) researchers should <em>not </em>halve their alpha level if they’re using two one-sided tests to test two directional null hypotheses.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/582/1*Zc7i5746Q9-Uvt7KCytSFQ.jpeg\" /><figcaption>Sometimes, two-sided tests are called “two-tailed” tests! (I’ll get my coat!) Source: <a href=\"https://www.britannica.com/animal/ring-tailed-lemur\">https://www.britannica.com/animal/ring-tailed-lemur</a></figcaption></figure><h4>(1) Researchers should halve two-sided p values when making directional claims</h4><p>Researchers sometimes conduct two-sided significance tests and then use the resulting two-sided <em>p</em> values to make directional claims. I argue that this approach is inappropriate because two-sided <em>p</em> values refer to <em>non-directional</em> hypotheses, rather than <em>directional </em>hypotheses.</p><p>So, for example, if you conduct a two-sided <em>t</em> test and obtain a significant two-sided <em>p</em> value, then your significant result refers to a non-directional null hypothesis (e.g., “men have <em>the same </em>self-esteem as women”), and you should make a corresponding non-directional claim (e.g., “men and women have significantly <em>different </em>self-esteem”). If you wish to make a <em>directional </em>claim (e.g., “men have significantly <em>higher </em>self-esteem than women”), then you should <em>halve </em>your two-sided <em>p</em> value to obtain a one-side <em>p </em>value.</p><p>This first point is important because, if you use a two-sided <em>p</em> value to make a decision about a directional null hypothesis, then (a) your evidence will be weaker than it should be (i.e., your <em>p</em> value will be too large), and (b) your Type II error rate will be higher than necessary. For the same view, please see Georgi Georgiev’s onesided.org website <a href=\"https://www.google.com/url?q=https%3A%2F%2Fwww.onesided.org%2F&sa=D&sntz=1&usg=AOvVaw2JD1rf-0wwN37D48nu3saz\">here</a>.</p><h4>(2) Researchers should not halve their alpha level when using two one-sided tests</h4><p>I also argue that, if you use two one-sided tests to test two directional null hypotheses, then it’s not necessary to adjust your alpha level to compensate for multiple testing, because your decision about rejecting each directional hypothesis is based on a <em>single </em>test result, rather than <em>multiple </em>test results.</p><p>For example, imagine that you use a one-sided test to test the directional null hypothesis that “men have the same or lower self-esteem than women.” In this case, there’s no need to lower your alpha level (e.g., from .050 to .025), because your Type I error rate only refers to a single test of a single null hypothesis. It doesn’t refer to either (a) the other directional null hypothesis (i.e., “men have the same or <em>higher </em>self-esteem than women”) or (b) the non-directional null hypothesis (i.e., “men have the <em>same </em>self-esteem as women).” Consequently, no alpha adjustment is required. For similar views, please see Georgi Georgiev’s piece <a href=\"https://www.google.com/url?q=https%3A%2F%2Fwww.onesided.org%2Farticles%2Fthe-paradox-of-one-sided-v-two-sided-tests-of-significance.php&sa=D&sntz=1&usg=AOvVaw0ijf7GBDf01WIfbBgSoCAR\">here</a> and my paper on multiple testing <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis/multiple-testing\">here</a>.</p><p>For further information, please see:</p><p>Rubin, M. (2022). That’s not a two-sided test! It’s two one-sided tests! <em>Significance, 19</em>(2), 50–53. <a href=\"https://www.google.com/url?q=https%3A%2F%2Fdoi.org%2F10.1111%2F1740-9713.01619&sa=D&sntz=1&usg=AOvVaw3zf1JzRR-EQYkn74ob-tyV\"><strong>Publisher’s version</strong></a><strong>. </strong><a href=\"https://drive.google.com/file/d/1Qww86RX2l6LSz9jkMvjJ4Rs2hp38mwJp/view?usp=sharing\"><strong>Self-archived version</strong></a><strong>.</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a6947b10385f\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/a32bac4af348","title":"What Type of Type I Error?","link":"https://rubinpsyc.medium.com/what-type-of-type-i-error-a32bac4af348?source=rss-c222612e11a4------2","published":"Sun, 25 Jul 2021 00:19:17 GMT","description":"","isPermalink":false,"tags":["what-is-a-type-i-error","neyman-and-pearson","replication-crisis","r-a-fisher"],"authors":[{"name":"Mark Rubin","url":null}],"image":null,"modified":null,"contentHtml":"<p>In this paper (<a href=\"https://www.google.com/url?q=https%3A%2F%2Fdoi.org%2F10.1007%2Fs11229-019-02433-0&sa=D&sntz=1&usg=AFQjCNEVuCuq3elOVD71HY_Ee1DN_5q1Og\">Rubin, 20</a>21), I consider two types of Type I error probability. The <em>Neyman-Pearson Type I error rate</em> refers to the maximum frequency of incorrectly rejecting a null hypothesis if a test was to be repeatedly reconducted on a series of different random samples that are all drawn from the exact same null population. Hence, the Neyman-Pearson Type I error rate refers to a long run of <em>exact </em>replications. In contrast, the<em> Fisherian Type I error probability</em> is the probability of incorrectly rejecting a null hypothesis in relation to a hypothetical population that reflects the relevant characteristics of the particular sample under consideration. Hence, the Fisherian Type I error rate refers to a one-off sample rather than a series of samples that are drawn during a long run of exact replications.</p><p>I argue that social science deals with units of analysis (people, social groups, and social systems) that change over time. As the Greek philosopher Heraclitus put it: “No man ever steps in the same river twice, for it’s not the same river and he’s not the same man.” Rivers and men are what Schmidt (2009, p. 92) called “irreversible units” in that they are complex time-sensitive systems that accumulate history. The scientific investigation of these irreversible units cannot proceed on the assumption that exact replications are possible. Consequently, the Neyman-Pearson Type I error rate is irrelevant in social science, because it relies on a concept of exact replication that cannot take place in the case of people, social groups, and social systems. Why should social scientists be interested in an error rate for an impossible process of resampling from the same fixed and unchanging population?</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/554/1*zfDlYm8FEwiXhSQ8QrnI2g.jpeg\" /><figcaption>Source: <a href=\"https://images.app.goo.gl/RiEP5tPX75AVbmFy6\">https://images.app.goo.gl/RiEP5tPX75AVbmFy6</a></figcaption></figure><p>I argue that the Fisherian Type I error probability is more appropriate in social science because it refers to one-off samples from hypothetical populations. In this case, researchers recognise that every sample comes from a potentially different population. Hence, researchers can apply the Fisherian Type I error probability to each sample-specific provisional decision that they make about rejecting the same substantive null hypothesis in a series of <em>direct </em>replications.</p><p>I conclude that the replication crisis may be partly (not wholly) due to researchers’ unrealistic expectations about replicability based on their consideration of the Neyman-Pearson Type I error rate across a long run of exact replications.</p><p>For further information, please see:</p><p>Rubin, M. (2021). What type of Type I error? Contrasting the Neyman-Pearson and Fisherian approaches in the context of exact and direct replications. <em>Synthese, 198, </em>5809–5834<em>. </em>. <a href=\"https://www.google.com/url?q=https%3A%2F%2Fdoi.org%2F10.1007%2Fs11229-019-02433-0&sa=D&sntz=1&usg=AFQjCNEVuCuq3elOVD71HY_Ee1DN_5q1Og\">https://doi.org/10.1007/s11229-019-02433-0</a> *<a href=\"https://drive.google.com/file/d/10mEkuIElCGE5lYO-24HCYS8NKfytZBpG/view?usp=sharing\">Self-archived version</a>*</p><p>I also discuss the differences between the Fisherian and Neyman-Pearson approaches to hypothesis testing <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis/repeated-sampling\">here</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a32bac4af348\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/590ef01327d2","title":"Repeated Sampling from the Same Population?","link":"https://rubinpsyc.medium.com/repeated-sampling-from-the-same-population-590ef01327d2?source=rss-c222612e11a4------2","published":"Tue, 20 Jul 2021 22:29:59 GMT","description":"","isPermalink":false,"tags":["what-is-a-type-i-error","problems-with-frequentism","reference-class-problem","what-is-frequentism","neyman-pearson"],"authors":[{"name":"Mark Rubin","url":null}],"image":null,"modified":null,"contentHtml":"<p>In this paper (<a href=\"https://drive.google.com/file/d/13cWuP8ilmQWMSORmzr6pLf3mJtn2C0jx/view?usp=sharing\">Rubin, 2020</a>), I consider Fisher’s criticism that the Neyman-Pearson approach to hypothesis testing relies on the assumption of “repeated sampling from the same population.” This criticism is problematic for the Neyman-Pearson approach because it implies that test users need to know, for sure, what counts as the same or equivalent population as their current population. If they don’t know what counts as the same or equivalent population, then they can’t specify a procedure that would be able to repeatedly sample from this population, rather than from other non-equivalent populations, and without this specification the Neyman-Pearson long run error rates become meaningless.</p><p>I argue that, by definition, researchers do not know for sure what are the relevant and irrelevant features of their current populations. For example, in a psychology study, is the population “1st year undergraduate psychology students” or, more narrowly, “Australian 1st year undergraduate psychology students” or, more broadly, “psychology undergraduate students” or, even more broadly, “young people,” etc.? Researchers can make educated guesses about the relevant and irrelevant aspects of their population. However, they must concede that those guesses may be wrong. Consequently, if a researcher imagines a long run of repeated sampling, then they must imagine that they would make incorrect decisions about their null hypothesis due to not only Type I errors and Type II errors, but also Type III errors — errors caused by accidentally sampling from populations that are substantively different to their underspecified alternative and null populations.</p><p>To be clear, the Neyman-Pearson approach <em>does </em>consider Type III errors. However, it considers them <em>outside </em>of each long run of repeated sampling. It does not allow Type III errors to occur <em>inside </em>a long run of repeated sampling, where the sampling must always be from a correctly specified family of “admissible” populations (Neyman, 1977, p. 106; Neyman & Pearson, 1933, p. 294). In my paper, I argue that researchers are unable to imagine a long run of repeated sampling from the same or equivalent populations as their current population because they are unclear about the relevant and irrelevant characteristics of their current population. Consequently, they are unable to rule out Type III errors within their imagined long run.</p><p>Following Fisher, I contrast scientific researchers with quality controllers in industrial production settings. Unlike researchers, quality controllers<em> </em>have clear knowledge about the relevant and irrelevant characteristics of their populations. For example, they are given a clear and unequivocal definition of Batch 57 on a production line, and they don’t consider re-conceptualizing Batch 57 as including or excluding other features. They also know which aspects of their testing procedure are relevant and irrelevant, and they are provided with precise quality control standards that allow them to know, for sure, their smallest effect size of interest. Consequently, the Neyman-Pearson approach is suitable for quality controllers because quality controllers can imagine a testing process that repeatedly draws random samples from the same population over and over again. In contrast, the Neyman-Pearson approach is not appropriate in scientific investigations because researchers do not have a clear understanding of the relevant and irrelevant aspects of their populations, their tests, or the smallest effect size that represents their population. Indeed, they are “researchers” because they are “researching” these things. Hence, it is researchers’ self-declared ignorance and doubt about the nature of their populations that renders Neyman-Pearson long run error rates scientifically meaningless.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*uUO4ovE7vJWkr5NObePNBw.jpeg\" /><figcaption>Source: <a href=\"https://dissolve.com/stock-photo/Quality-control-workers-digital-tablet-examining-blocks-royalty-free-image/101-D1230-1-512\">https://dissolve.com/stock-photo/Quality-control-workers-digital-tablet-examining-blocks-royalty-free-image/101-D1230-1-512</a></figcaption></figure><p>For further information, please see:</p><p>Rubin, M. (2020). “Repeated sampling from the same population?” A critique of Neyman and Pearson’s responses to Fisher. <em>European Journal for Philosophy of Science, 10</em>, Article 42, 1–15<em>. </em><a href=\"https://www.google.com/url?q=https%3A%2F%2Fdoi.org%2F10.1007%2Fs13194-020-00309-6&sa=D&sntz=1&usg=AFQjCNHY2NhGe0PmkpF4-n_IgbhMNd4z3A\">https://doi.org/10.1007/s13194-020-00309-6</a> *<a href=\"https://www.google.com/url?q=https%3A%2F%2Frdcu.be%2Fb7K0s&sa=D&sntz=1&usg=AFQjCNGf6S0omDOBOhaS48krOpG8TgGDQA\">Publisher’s open access view only version</a>* *<a href=\"https://drive.google.com/file/d/13cWuP8ilmQWMSORmzr6pLf3mJtn2C0jx/view?usp=sharing\">Author’s version</a>*</p><p>I also discuss the differences between the Fisherian and Neyman-Pearson approaches to hypothesis testing <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis/what-is-a-type-i-error\">here</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=590ef01327d2\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/2fd75a20f82d","title":"An Evaluation of Four Solutions to the Garden of Forking Paths Problem","link":"https://rubinpsyc.medium.com/an-evaluation-of-four-solutions-to-the-garden-of-forking-paths-problem-2fd75a20f82d?source=rss-c222612e11a4------2","published":"Tue, 20 Jul 2021 22:17:10 GMT","description":"","isPermalink":false,"tags":["forking-paths","pre-registration","multiverse-analyses","familywise-error","neyman-pearson"],"authors":[{"name":"Mark Rubin","url":null}],"image":null,"modified":null,"contentHtml":"<p>In this <a href=\"https://www.google.com/url?q=https%3A%2F%2Fdoi.org%2F10.1037%2Fgpr0000135&sa=D&sntz=1&usg=AFQjCNGPXhRTKBRRFOs9Cvs9a3pW4QQRPQ\">paper</a> (Rubin, 2017), I consider Gelman and Loken’s (2013, 2014) garden of forking paths problem. Forking paths occur when researchers decide which analyses to perform based on information from their sample. For example, a researcher may decide whether or not to drop an item from a scale based on how it affects the scale’s Cronbach alpha coefficient <em>within the current sample. </em>This <em>sample-contingent decision rule</em> causes a forking path in the analysis protocol because, during the course of repeated sampling from the same population, some samples may yield an acceptable Cronbach alpha value, leading to the retention of the item in the scale, and some samples may yield an unacceptable Cronbach alpha, leading to the deletion of the item.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-41uiff_DcjfnK_X6y8JKQ.png\" /><figcaption>Source: <a href=\"https://doi.org/10.1037/gpr0000135\">https://doi.org/10.1037/gpr0000135</a></figcaption></figure><p>Following the garden of forking paths would lead to multiple tests of the same hypothesis during a long run of exact replications of the analysis protocol (e.g., one test that included the item in the scale and one test that excluded the item from the scale). Consequently, the forking paths problem represents a special case of the multiple testing problem. Forking paths increase the familywise Type I error rate for a hypothesis by taking into account not only the test that was actually conducted, but also the test that could have been conducted had a different sample been drawn.</p><p>In my paper, I consider four potential solutions to the forking paths problem: (a) preregistration, (b) sensitivity (robustness, multiverse) analyses, (c) adjusting the alpha level, and (d) abandoning the Neyman-Pearson approach. I conclude that preregistration and sensitivity analyses are ineffective solutions, but that adjusting the alpha level and/or abandoning the Neyman-Pearson approach are effective solutions. In particular, the alpha level can be adjusted to take into account the number of forking paths that are involved in testing the relevant hypothesis (e.g., α/2 for one forking path, α/4 for two forking paths, etc.). In addition, statistical inference approaches that do not rely on the concept of repeated sampling from the same population are not susceptible to the forking paths problem. Hence, the Fisherian and Bayesian approaches to hypothesis testing do not suffer from the forking paths problem because they both condition their probability statements on the test that was actually conducted and the sample that was actually drawn.</p><p>For further information, please see:</p><p>Rubin, M. (2017). An evaluation of four solutions to the forking paths problem: Adjusted alpha, preregistration, sensitivity analyses, and abandoning the Neyman-Pearson approach. <em>Review of General Psychology, 21</em>(4)<em>, </em>321–329. <a href=\"https://www.google.com/url?q=https%3A%2F%2Fdoi.org%2F10.1037%2Fgpr0000135&sa=D&sntz=1&usg=AFQjCNGPXhRTKBRRFOs9Cvs9a3pW4QQRPQ\">https://doi.org/10.1037/gpr0000135</a> *<a href=\"https://drive.google.com/file/d/1wjOrFqqjw5aaISyKZANuRhXpjPkAPl1K/view?usp=sharing\">Self-archived version</a>*</p><p>For more of my work in this area, please see: <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis\">https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2fd75a20f82d\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/2ab5710f9808","title":"The Costs of HARKing: Does it Matter if Researchers Engage in Undisclosed Hypothesizing After the…","link":"https://rubinpsyc.medium.com/the-costs-of-harking-does-it-matter-if-researchers-engage-in-undisclosed-hypothesizing-after-the-2ab5710f9808?source=rss-c222612e11a4------2","published":"Mon, 12 Jul 2021 22:47:47 GMT","description":"","isPermalink":false,"tags":["meta-science","p-hacking","open-science","replication","science"],"authors":[{"name":"Mark Rubin","url":null}],"image":null,"modified":null,"contentHtml":"<h3>The Costs of HARKing: Does it Matter if Researchers Engage in Undisclosed Hypothesizing After the Results are Known?</h3><p>While no-one’s looking, a Texas sharpshooter fires his gun at a barn wall, walks up to his bullet holes, and paints targets around them. When his friends arrive, he points at the targets and claims he’s a good shot.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/357/1*E3s7VDeUsqljaIkTk-v7eQ.png\" /><figcaption>Source: Dirk-Jan Hoek. <a href=\"https://www.flickr.com/photos/23868780@N00/7374874302\">https://www.flickr.com/photos/23868780@N00/7374874302</a></figcaption></figure><p>In 1998, Norbert Kerr discussed an analogous situation in which researchers engage in undisclosed <a href=\"https://www.google.com/url?q=https%3A%2F%2Fjournals.sagepub.com%2Fdoi%2Fpdf%2F10.1207%2Fs15327957pspr0203_4&sa=D&sntz=1&usg=AFQjCNErBvYhNqrCLFAUJfVcc95e49Iv7w\"><em>hypothesizing after the results are known</em></a> or HARKing. In this case, researchers conduct statistical tests, observe their research results (bullet holes), and then construct post hoc hypotheses (paint targets) to fit these results. In their research reports, they then pretend that their post hoc hypotheses are actually a priori hypotheses. This <em>questionable research practice</em> is thought to have contributed to the replication crisis in science, and it provides part of the rationale for researchers to publicly preregister their hypotheses before they conduct their research. In a recent <a href=\"https://www.google.com/url?q=https%3A%2F%2Fdoi.org%2F10.1093%2Fbjps%2Faxz050&sa=D&sntz=1&usg=AFQjCNGBz4GZuUnqKjeSLzxjEmXi0A5e9A\">BJPS article</a>, I discuss the concept of HARKing from a philosophical standpoint and argue against the view that it is problematic for scientific progress.</p><p>I begin my article by noting that scientists do not make absolute, dichotomous judgements about theories and hypotheses being “true” or “false.” Instead, they make relative judgements about theories and hypotheses being more or less true that other theories and hypotheses in accounting for certain phenomena. These judgements can be described as <em>estimates of relative verisimilitude</em> (Cevolani & Festa, 2018).</p><p>I then note that a HARKer is obliged to provide a theoretical rationale for their secretly post hoc hypothesis in the Introduction section of their research report. Despite being secretly post hoc, this theoretical rationale provides a result-independent basis for an initial estimate of the relative verisimilitude of the HARKed hypothesis. (The rationale is “result-independent” because it doesn’t formally refer to the current result. If it did, then the rationale’s post hoc status would no longer be a secret!) The current result can then provide a second, epistemically independent basis for adjusting this initial estimate of verisimilitude upwards or downards (for a similar view, see Lewandowsky, 2019; Oberauer & Lewandowsky, 2019). Hence, readers can estimate the relative verisimilitude of a HARKed hypothesis (a) without taking the current result into account and (b) after taking the current result into account, even if they have been misled about when the researcher deduced the hypothesis. Consequently, readers can undertake a valid updating of the estimated relative verisimilitude of a hypothesis even though, unbeknowst to them, it has been HARKed. Importantly, there’s no “double-counting” (Mayo, 2008), “circular reasoning” (Nosek et al., 2018), or violation of the <em>use novelty</em> principle here (Worrall, 1985, 2014), because the current result has not been used in the formal theoretical rationale for the HARKed hypothesis. Consequently, it’s legitimate to use the current result to change (increase or decrease) the initial estimate of the relative verisimilitude of that hypothesis.</p><p>To translate this reasoning to the Texas sharpshooter analogy, it’s necessary to distinguish HARKing from <em>p</em>-hacking. If our sharpshooter painted a new target around his stray bullet hole but retained his substantive claim that he’s “a good shot,” then he’d be similar to a researcher who conducted multiple statistical tests and then selectively reported only those results that supported their original a priori substantive hypothesis. Frequentist researchers would call this researcher a “<em>p</em>-hacker” rather than a HARKer (<a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis/the-garden-of-forking-paths?authuser=0\">Rubin, 2017b</a>, p. 325; Simmons et al., 2011). To be a HARKer, researchers must also change their original a priori hypothesis or create a totally new one. Hence, a more appropriate analogy is to consider a sharpshooter who changes <em>both </em>their statistical hypothesis (i.e., paints a new target around their stray bullet hole) <em>and </em>their broader substantive hypothesis (their claim). Let’s call her Jane!</p><p>Jane initially believes “I’m a good shot” (H1). However, after missing the target that she was aiming for (T1), she secretly paints a new target (T2) around her bullet hole and declares to her friends: “I’m a good shot, but I can’t adjust for windy conditions. I aimed at T1, but there was a 30 mph easterly cross-wind. So, I knew I’d probably hit T2 instead.” In this case, Jane has generated a new, post hoc hypothesis (H2) and passed it off as an a priori hypothesis. Note that, unlike our original Texas sharpshooter, Jane isn’t being deceptive about her <em>procedure </em>here (i.e., what she actually did): It’s true that she aimed her gun at T1. She’s only being deceptive about the a priori status of H2, which she secretly developed <em>after </em>she missed T1 (i.e., she’s HARKing). Importantly, however, Jane’s deception doesn’t prevent her friends from making a valid initial estimate of the verisimilitude of her HARKed hypothesis and then updating this estimate based on the location of her bullet hole:</p><blockquote>“We know that Jane’s always trained indoors. So, it makes sense that she hasn’t learned to adjust for windy conditions. We also know that (a) Jane was aiming at T1, and (b) there was a 30 mph easterly cross-wind. Our calculations show that, if someone was a good shot, and they were aiming at T1, but they didn’t adjust for an easterly 30 mph cross-wind, then their bullet would hit T2’s location. So, our initial estimated verismilitude for H2 is relatively high. The evidence shows that Jane’s bullet did, in fact, hit T2. Consequently, we can tentatively increase our support for H2: Jane appears to be a good shot who can’t adjust for windy conditions. Of course, we’d also want to test H2 again by asking Jane to hit targets on both windy and non-windy days!”</blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/948/1*-CEx-xrzbzo6cx81MnLWtg.jpeg\" /><figcaption>We can predict the location of the sharpshooter’s bullet hole on the basis of her (secretly HARKed) hypothesis that she is a good shot but cannot adjust for windy conditions. We can then use the location of the bullet hole to increase or decrease our estimated relative verisimilitude for this prediction. Source: <a href=\"https://www.google.com/url?q=https%3A%2F%2Fpixabay.com%2Fphotos%2Fwoman-rifle-shoot-gun-weapon-2577104%2F&sa=D&sntz=1&usg=AFQjCNFiBLEmq2w5mxhFZzyaBsAhVLGWVA\">https://pixabay.com/photos/woman-rifle-shoot-gun-weapon-2577104/</a></figcaption></figure><p>For further information, please see:</p><p>Rubin, M. (2022). The costs of HARKing. <em>The British Journal for the Philosophy of Science, 73. </em><a href=\"https://www.google.com/url?q=https%3A%2F%2Fdoi.org%2F10.1093%2Fbjps%2Faxz050&sa=D&sntz=1&usg=AFQjCNGBz4GZuUnqKjeSLzxjEmXi0A5e9A\">https://doi.org/10.1093/bjps/axz050</a> or *<a href=\"https://www.google.com/url?q=https%3A%2F%2Facademic.oup.com%2Fbjps%2Fadvance-article%2Fdoi%2F10.1093%2Fbjps%2Faxz050%2F5651026%3FguestAccessKey%3Db361c0e4-77f2-4443-9d77-7f603e566f53&sa=D&sntz=1&usg=AFQjCNHDGcxumGe2286Gum7DnqOpJRq1UA\">Publisher’s free access</a>* or *<a href=\"https://drive.google.com/file/d/1bGIUjHSEAoJYJke6RWtBphXJjZLr1UeX/view?usp=sharing\">Self-archived version</a>*</p><p>For more of my work in this area, please see: <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis\">https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2ab5710f9808\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/b4e872ce8a3c","title":"Does Preregistration Improve the Interpretablity and Credibility of Research Findings?","link":"https://rubinpsyc.medium.com/does-preregistration-improve-the-interpretablity-and-credibility-of-research-findings-b4e872ce8a3c?source=rss-c222612e11a4------2","published":"Mon, 12 Jul 2021 22:17:58 GMT","description":"","isPermalink":false,"tags":["open-science","meta-science","methodology","science","does-preregistration-work"],"authors":[{"name":"Mark Rubin","url":null}],"image":null,"modified":null,"contentHtml":"<p>Preregistration entails researchers registering their planned research hypotheses, methods, and analyses in a time-stamped document before they undertake their data collection and analyses. This document is then made available with the published research report in order to allow readers to identify discrepancies between what the researchers originally planned to do and what they actually ended up doing. In a recent article (<a href=\"https://doi.org/10.20982/tqmp.16.4.p376\">Rubin, 2020</a>), I question whether this <em>historical transparency</em> facilitates judgments of credibility over and above what I call the <em>contemporary transparency</em> that is provided by (a) clear rationales for current hypotheses and analytical approaches, (b) public access to research data, materials, and code, and (c) demonstrations of the robustness of research conclusions to alternative interpretations and analytical approaches.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/402/1*YYny0udOTH_iiN4dieeCmA.png\" /></figure><p>My article covers issues such as <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis/the-costs-of-harking\">HARKing</a>, multiple testing, <em>p</em>-hacking, <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis/the-garden-of-forking-paths\">forking paths</a>, <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis/p-values-in-exploratory-analyses\">exploratory analyses</a>, optional stopping, researchers’ biases, selective reporting, test severity, publication bias, and replication rates. I argue for a nuanced approach to these issues. In particular, I argue that only <em>some </em>of these issues are problematic, and only under <em>some </em>conditions. I also argue that, when they are problematic, these issues can be identified via contemporary transparency per se.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/712/1*W15_55y4m_aPZ1E90yp5AA.png\" /></figure><p>I conclude that preregistration’s historical transparency does not facilitate judgments about the credibility of research findings when researchers provide contemporary transparency. Of course, in many cases, researchers do not provide a sufficient degree of contemporary transparency (e.g., no open research data or materials), and in these cases preregistration’s historical transparency may provide some useful information.</p><p>However, I argue that historical transparency is a relatively narrow, researcher-centric form of transparency because it focuses attention on the predictions made by specific researchers at a specific point in time. In contrast, contemporary transparency allows research data to be considered from multiple, unplanned, theoretical and analytical perspectives while maintaining a high degree of research credibility. Hence, I suggest that the open science movement should push more towards contemporary transparency and less towards historical transparency.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Dk63b8_EVkeBDgMbu9fjQg.png\" /></figure><p>For further information, please see:</p><p>Rubin, M. (2020). Does preregistration improve the credibility of research findings? <em>The Quantitative Methods in Psychology, 16</em>(4), 376–390. <a href=\"https://doi.org/10.20982/tqmp.16.4.p376\">https://doi.org/10.20982/tqmp.16.4.p376</a></p><p>I was invited to present my paper at the opening cermony of the Erasmus Research Institute of Management Research Transparency Campaign at Erasmus University Rotterdam on 1st March 2022. A recording of my presentation is below:</p><iframe src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FxsEoLhQrKNQ%3Fstart%3D1%26feature%3Doembed%26start%3D1&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DxsEoLhQrKNQ&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FxsEoLhQrKNQ%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube\" width=\"854\" height=\"480\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/426ebd8a39df3d5e3d87e9c18b29bb7b/href\">https://medium.com/media/426ebd8a39df3d5e3d87e9c18b29bb7b/href</a></iframe><p>For more of my work in this area, please see: <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis\">https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b4e872ce8a3c\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/574f8484f984","title":"When to Adjust Alpha During Multiple Testing","link":"https://rubinpsyc.medium.com/when-to-adjust-alpha-during-multiple-testing-574f8484f984?source=rss-c222612e11a4------2","published":"Mon, 12 Jul 2021 22:04:00 GMT","description":"","isPermalink":false,"tags":["replication","p-hacking","meta-science","science","open-science"],"authors":[{"name":"Mark Rubin","url":null}],"image":null,"modified":null,"contentHtml":"<p>In this new paper (<a href=\"https://doi.org/10.1007/s11229-021-03276-4\">Rubin, 2021</a>), I consider when researchers should adjust their alpha level (significance threshold) during multiple testing and multiple comparisons. I consider three types of multiple testing (disjunction, conjunction, and individual), and I argue that an alpha adjustment is only required for one of these three types.</p><h3>There’s No Need to Adjust Alpha During Individual Testing</h3><p>I argue that an alpha adjustment is <em>not </em>necessary when researchers undertake a single test of an individual null hypothesis, even when many such tests are conducted within the same study.</p><p>For example, in the jelly beans study below, it’s perfectly acceptable to claim that there’s “a link between green jelly beans and acne” using an unadjusted alpha level of .05 given that this claim is based on a <em>single test</em> of the hypothesis that green jelly beans cause acne rather than <em>multiple tests </em>of this hypothesis.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/540/1*nJz_1rvgtOvvlEzbpubPBg.png\" /><figcaption>Retrieved from <a href=\"https://xkcd.com/882/\">https://xkcd.com/882/</a></figcaption></figure><p>For a list of quotes from others that are consistent with my position on individual testing, please see Appendix B <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis/p-values-in-exploratory-analyses\">here</a>.</p><p>To be clear, I’m not saying that an alpha adjustment is <em>never</em> necessary. It is necessary when <em>at least one</em> significant result would be sufficient to support a <em>joint</em> hypothesis that’s composed of several constituent hypotheses that each undergo testing (i.e., disjunction testing). For example, an alpha adjustment would be necessary to conclude that “jelly beans of one or more colours cause acne” because, in this case, a single significant result for at least one of the 20 colours of jelly beans would be sufficient to support this claim, and so a familywise error rate is relevant.</p><h3>Studywise Error Rates are Not Usually Relevant</h3><p>I also argue against the automatic (mindless) use of what I call <em>studywise error rates</em> — the familywise error rate that is associated with all of the hypotheses that are tested in a study. I argue that researchers should only be interested in studywise error rates if they are interested in testing the associated joint studywise hypotheses, and researchers are not usually interested in testing studywise hypotheses because they rarely have any theoretical relevance. As I explain in my paper, “in many cases, the joint studywise hypothesis has no relevance to researchers’ specific research questions, because its constituent hypotheses refer to comparisons and variables that have no theoretical or practical basis for joint consideration.”</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/981/1*aYxQY8dW0TrKc3MoPlUUmA.jpeg\" /><figcaption>Sometimes it doesn’t make sense to combine different hypotheses as part of the same family!</figcaption></figure><p>For example, imagine that a researcher conducts a study in which they test gender, age, and nationality differences in alcohol use. Do they need to adjust their alpha level to account for their multiple testing? I argue “no” unless they want to test a studywise hypothesis that, for example: “<em>Either </em>(a) men drink more than women, (b) young people drink more than older people, <em>or</em> (c) the English drink more than Italians.” If the researcher does not want to test this potentially atheoretical joint hypothesis, then they should not be interested in controlling the associated familywise error rate, and instead they should consider each individual hypothesis separately. As I explain in my paper, “researchers should not be concerned about erroneous answers to questions that they are not asking.”</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/936/1*-DFqzDI8yS18WZqCfI5mpg.jpeg\" /><figcaption>Thomas Wiecki <a href=\"http://twitter.com/twiecki\">@twiecki</a><a href=\"https://twitter.com/twiecki/status/1418462017476759552?s=20\"> https://twitter.com/twiecki/status/1418462017476759552?s=20</a></figcaption></figure><p>For a list of quotes that support my position on studywise error rates, please see Appendix A <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis/p-values-in-exploratory-analyses\">here</a>.</p><p>For further information, please see:</p><p>Rubin, M. (2021). When to adjust alpha during multiple testing: A consideration of disjunction, conjunction, and individual testing. <em>Synthese</em>. <a href=\"https://doi.org/10.1007/s11229-021-03276-4\">https://doi.org/10.1007/s11229-021-03276-4</a> or <a href=\"https://drive.google.com/file/d/1Rl23xQxSs06-uAb0uhUKTxdTFCLrFbrG/view?usp=sharing\">Open Access</a></p><p>For more of my work in this area, please see: <a href=\"https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis\">https://sites.google.com/site/markrubinsocialpsychresearch/replication-crisis</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=574f8484f984\" width=\"1\" height=\"1\" alt=\"\">"}]}