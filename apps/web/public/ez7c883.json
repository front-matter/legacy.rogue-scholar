{"version":"https://jsonfeed.org/version/1.1","id":"ez7c883","title":"Stories by Adam Day on Medium","description":"Stories by Adam Day on Medium","homePageUrl":"https://medium.com/@publisherad?source=rss-ed0928b1b891------2","feedUrl":"https://medium.com/feed/@publisherad","favicon":"https://cdn-images-1.medium.com/fit/c/150/150/1*DJGPkBv9iJDmlE836eaoig.jpeg","items":[{"id":"https://medium.com/p/797bc64e2632","title":"The definition of plagiarism","link":"https://publisherad.medium.com/the-definition-of-plagiarism-797bc64e2632?source=rss-ed0928b1b891------2","published":"Fri, 24 Mar 2023 08:56:52 GMT","description":"","isPermalink":false,"tags":["ai","academic-publishing","chatgpt","research-integrity"],"authors":[{"name":"Adam Day","url":null}],"image":null,"modified":null,"contentHtml":"<p>This is Ralph. How tall is Ralph?</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/1*SJRZ2WmIEOg9GDzVG3Wtew.png\" /></figure><p>It seems simple, you could just hold a ruler up to the screen. But when you look at the ruler and use it to measure Ralph, are you actually measuring <em>Ralph</em>, or are you measuring the ruler and using that as a <em>proxy</em>? How accurate is your measurement? Are you including fur in the measurement? What if Ralph were to stand on his hind legs, like a mighty bear — how tall would he be then?</p><p>This is a problem with measurement science. You can often only measure a proxy of what you want to measure. We see this all the time e.g. with citation metrics being used as proxies for quality — stuff like that.</p><h3>Proxies for misconduct</h3><p>Plagiarism is no exception. A student recently asked me “what percentage of plagiarism is ok?”. There was clearly some confusion. Plagiarism isn’t ok. It’s like asking “what percentage of crime is ok?” or “how many Brussels sprouts would you like?”. The answer is <strong>none</strong>. I think that confusion stems from the fact that we use <em>copying of text</em> to measure plagiarism. So people think that copying text <em>is </em>plagiarism. But it isn’t, it’s a proxy.</p><p>When dealing with re-use of text, there are 3 issues that journal editors are concerned with:</p><ol><li>Copyright infringement. This is a legal matter. For any piece of written work, there is a rights-holder. That might be the original author, their employer or a publisher. If someone copies text without permission of the rights-holder, that is copyright infringement and it’s illegal. Copyright infringement is a great proxy for plagiarism because it’s very easy to find and prove.</li><li>Plagiarism. This is an <em>ethical</em> matter. Plagiarism, put simply, is theft of ideas. Or, it’s presenting someone else’s ideas as if they are your own. This is a far more thorny concept and its much harder to prove directly. What if 2 people just happened to have same idea by pure coincidence? Then there are cases where it’s accepted practice to use other people’s ideas. E.g. it’s ok to copy StackOverflow solutions <em>because that’s what they are for.</em> But it isn’t ok to copy other people’s scientific work without attribution. And that’s the key point: to avoid plagiarising, always cite your sources.</li><li>Quality. Let’s imagine a situation where you have the rights-holder’s permission to reproduce their work. Let’s also say that you’ve cited your source properly and it’s clear that you are not plagiarising anyone. In that situation, can you re-use their work? It still depends. If an editor sees someone re-using so much text that it makes it hard to see the novelty in the paper, they might choose to reject on that basis. I.e. simply because repetition is boring!</li></ol><p>So there are 3 issues that journal editors might be concerned with when authors re-use the work of others. If you think about it, they are all connected, but plagiarism is the only one that is a research integrity issue.</p><p>To answer the question, plagiarism can’t be measured, there isn’t a unit of measurement for it, so there isn’t a % that is ok. It’s like trying to measure Ralph with a ruler.</p><h3>Generative AI</h3><p>Computers are machines which copy. Fundamentally, that’s what they do. When I type, the letters on the keys are copied onto the screen and from there they get copied from my computer to a bunch of servers and then to your computer. Eventually copies of the keys I’ve typed end up in front of your eyes. Here they are! This is why plagiarism is so easy with a computer, copy and paste is a piece of cake. Generative AI is much more complex, but fundamentally, it is just taking human input, processing it, and showing it to other humans.</p><p><a href=\"https://www.reuters.com/legal/getty-images-lawsuit-says-stability-ai-misused-photos-train-ai-2023-02-06/\">Generative AI has come under some fire for copyright infringement</a>. But there’s a separate issue: there’s no doubt that generative AI presents people’s ideas without attribution. I don’t think it would be fair to say that generative AI is <em>just</em> automated plagiarism, but if it doesn’t cite its sources, that is certainly part of what it is.</p><p>Keep in mind that the weakest copyright status typically found on any research paper is the CC-BY license. That’s a license which gives up almost every right an author has *except* it requires that the author is cited. (<a href=\"https://creativecommons.org/licenses/by/2.0/\">“BY” stands for “by attribution”</a>.)</p><p>The problem gets worse when you consider that, even if AI does cite its sources, there’s nothing to stop users removing the citations and then passing off the generated text as their own work.</p><p>I also think there’s quite a distinction here between <em>generative AI </em>and other kinds of machine-learning. You can train lots of tasks (including generative tasks) on someone’s work without passing their work off as your own. Elicit.org, a service which is already a few years old, deserves a shout-out for its AI-based scientific question answering service (<em>which cites its sources!).</em></p><p>It took a few weeks for me to tire of seeing examples of [whatever] generated by ChatGPT. Now when I see new examples, I quickly scroll past the ChatGPT output and look for something written by a human.</p><p>That’s why I’m making it a rule never to use generative AI to make content for this blog. Partly because it’s potential plagiarism, but also because it’s boring.</p><p>There is no AI-generated content in this post. Except Ralph. Ralph was fake.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=797bc64e2632\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/72ab43bd217","title":"We need a chat about ChatGPT","link":"https://publisherad.medium.com/we-need-a-chat-about-chatgpt-72ab43bd217?source=rss-ed0928b1b891------2","published":"Tue, 24 Jan 2023 08:42:58 GMT","description":"","isPermalink":false,"tags":["academic-publishing","chatgpt","artificial-intelligence","research-integrity"],"authors":[{"name":"Adam Day","url":null}],"image":null,"modified":null,"contentHtml":"<p>There’s a quote attributed to Ernest Rutherford: “That which is not measurable is not science. That which is not physics is stamp collecting”. I think his point was that a lot of scientific work is just <em>documenting </em>things. In Rutherford’s day, there was a lot of exciting new <em>creative </em>work happening in physics, so perhaps physics seemed special to him. On the other hand, it was a mean-spirited thing to say about other fields of science and I’m sure he felt very silly when he was handed the Nobel prize in chemistry.</p><p>Generative AI is all the rage right now isn’t it? Essentially, we can now use AI models to do <em>creative</em> tasks, like painting a picture.</p><p>Do you remember that famous series of paintings that Claud Monet did called “computer nerds hard at work”?</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/1*ag7wUHny722vK6beW0UdfQ.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/1*GVoYZVm-u9uQuxJvjhWKIg.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/1*U9V_-9NXaDl0O3CbDU0dwQ.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/1*T5AMW56GTO2GSe6eW5I1VA.png\" /></figure><p>No? Wikipedia says that Monet passed away in 1926. That means he was dead at the time these computers were invented and can’t possibly have painted them, but they certainly look like Monet’s work, don’t they? If I had wanted to fake images like this 12 months ago, I would have needed canvas, oils and a lot of talent with a brush. Today, I just prompted Stable Diffusion to do it for me. No coding skills required. It took seconds.</p><p>Speaking of coding, I was looking for a very specific piece of code recently. After half an hour searching on Google and StackOverflow I was ready to give up searching and write it myself, but then I asked ChatGPT to do it for me. ChatGPT wrote exactly what I needed. Again, it took seconds.</p><p>ChatGPT is a chatbot based on a <em>Large Language Model </em>called GPT-3. You can ask ChatGPT to write just about anything. Poems, stories, homework solutions, research papers… anything!</p><p>The value of new AI models for <strong>creative</strong> tasks like this is immense. Images and text and even video can be generated rapidly with increasingly authentic quality standards.</p><p>I put it to you that when you write a research paper, that <em>isn’t</em> creative work. I mean, it obviously is to an extent, but the purpose of a research paper is to document something. It’s the stamp collecting part of science and that’s an important distinction.</p><p>If I ask ChatGPT for text for an original research paper, it will give it to me. But the text won’t be based on any real novel piece of research, it will be made up and it will be either derivative, false, or both. It will look real and <a href=\"https://www.nature.com/articles/d41586-023-00056-7\">a recent study by Catherine Gao at Northwestern University in Chicago, Illinois</a> showed that scientists were often unable to distinguish real scientific abstracts from those written by ChatGPT.</p><p>By the way, if I prompt Stable Diffusion for fake scientific images, it’ll cough those up too.</p><p>So, AI can already produce believable scientific fakes that are hard to detect. Worse, the people building these AI applications might not even be aware of the extent and dangers of <a href=\"https://medium.com/@publisherad/how-papermills-work-generally-c080bb9fff43\">organised research fraud</a>.</p><p>Through my company, Clear Skies, I’ve already been working on a couple of methods that should be able to detect fraudulent ChatGPT-generated content. But I’m in 2 minds as to whether it’s a good idea to develop this capability because there’s no good reason that AI should be producing this kind of content in the first place.</p><ul><li>AI shouldn’t be trained to generate documentary material at all.</li><li>APIs for these tools should also classify prompts and output to recognise when they are being abused for fraudulent or harmful purposes (and then decline to give output to a user).</li></ul><p>I hope that doesn’t sound like wishful thinking. Controls like these actually do exist on both ChatGPT and Stable Diffusion. So I am optimistic that, as the people building these applications receive feedback, they make adaptations like those above to avoid creating harmful outputs.</p><p>In the meantime, we are already seeing policies on artificially generated content. StackOverflow <a href=\"https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned\">has temporarily banned AI-generated content until such time as it can determine suitable guidelines for use</a>.</p><p>The future of this problem for Schol-Comms is complex. There is a lot to consider:</p><ul><li>Should publishers ban AI-generated content like StackOverflow has done?</li><li>Or should guidelines be more nuanced — like should AI-generated content be treated like quotations where the non-original content in a paper is clearly set apart from the rest of the text? What about AI autocomplete? Should that be banned or quoted, or is that different?</li><li>Do the copyright licenses on scientific papers allow them to be used for training generative AI? (And what should those licenses allow? It’s not like they were written with this use case in mind.)</li></ul><p>It’s a tough one and I’m sure it’ll take time to figure out the best way to handle this. What is clear, though, is that there is plenty to discuss!</p><p>There are a couple of addenda worth making here. There are other AI use-cases that can be trained on research papers. E.g. question-answering. That’s where we can give an open-ended question like “what’s the best treatment for COVID-19” and then an AI model can find all the best papers on that topic and generate a response to the question based on those. That’s a brilliant thing to do and so there’s definitely a good case for training models for some tasks.</p><p>It’s also worth mentioning Meta’s recently released Galactica model. Galactica was supposed to enable tasks like those described above. However: there was clear potential to use the model for research fraud and it had a few other issues (e.g. it produced nonsense from time to time — if we want scientific answers, that’s not very helpful). The dangers were clear and <a href=\"https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/\">I was relieved to see Meta kill Galactica just days after its release</a>. There’s still potential for a project like that, but it’s great to see decisive action from Meta to address the risk.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=72ab43bd217\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/b37a44c4c11f","title":"Video: APIs for Papermill detection (ConTech API Series)","link":"https://publisherad.medium.com/video-apis-for-papermill-detection-contech-api-series-b37a44c4c11f?source=rss-ed0928b1b891------2","published":"Mon, 28 Nov 2022 09:08:01 GMT","description":"","isPermalink":false,"tags":["academic-publishing","research-integrity","peer-review-journals"],"authors":[{"name":"Adam Day","url":null}],"image":null,"modified":null,"contentHtml":"<p>Recently, I gave a presentation on the APIs for Papermill Detection offered by Clear Skies Ltd.</p><p>I also touch on a newer service called the Clear Skies Standard Report. More on that in future posts… :)</p><p>Here’s the video:</p><iframe src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FwxsspSstdcQ%3Ffeature%3Doembed&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DwxsspSstdcQ&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FwxsspSstdcQ%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube\" width=\"854\" height=\"480\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/ee5a04aaed9fc53d2748e64516178ebe/href\">https://medium.com/media/ee5a04aaed9fc53d2748e64516178ebe/href</a></iframe><h4>Would you like to know more?</h4><p>There are a few more presentations coming up:</p><p>I’ll be speaking with Ebru Cucen from Open Credo about some work we did on detecting unusual co-authorships based on <a href=\"https://arxiv.org/abs/2112.13322\">data shared by Anna Abalkina</a>.</p><ul><li><a href=\"https://www.contech.live/contech2022\">ConTech LIVE 29 November</a></li><li><a href=\"https://yowlondon.com/2022/schedule?date=2022-11-30\">YOW! London 30 November</a></li></ul><p>Hope you can make it to one of those!</p><p>I’ll also be moderating the Basel Sustainable Publishing Forum (BSPF) sponsored by MDPI. <a href=\"https://bspfwebinar3.sciforum.net/\">This webinar, on the topic of “technologial solutions to papermills” is free to join</a> and will include presentations from Elsevier, Frontiers and MDPI.</p><p>Hope to see you at one or more of the above!</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*svro4QM7HY9cbqX-\" /></figure><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b37a44c4c11f\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/14993c37ebfa","title":"(The?) 3 kinds of papermills","link":"https://publisherad.medium.com/3-kinds-of-papermills-14993c37ebfa?source=rss-ed0928b1b891------2","published":"Mon, 31 Oct 2022 17:08:36 GMT","description":"","isPermalink":false,"tags":["peer-review-journals","academic-publishing","research-integrity"],"authors":[{"name":"Adam Day","url":null}],"image":null,"modified":null,"contentHtml":"<p>TL;DR: Join me at ConTech Live to hear about a recent project with <a href=\"https://opencredo.com/\">Open Credo</a> to see if we could detect unusual co-authorships in a dataset created by Anna Abalkina. <a href=\"https://www.contech.live/contech2022\">Sign up here!</a></p><p>Papermilling has a few definitions which you see here and there. Sometimes it’s “organised manipulation of the peer-review process” or it might be “manufacturing of fraudulent research papers” and so on.</p><p>I described the general roles involved in papermilling in <a href=\"https://publisherad.medium.com/how-papermills-work-generally-c080bb9fff43\">a previous blog post</a>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/937/1*OSsajzQlkOFyf_LkQ6lBfw.png\" /></figure><p>But there’s actually a lot of nuance in how different papermills operate. Consider what happens when we have multiple agents, forgers and authors operating in a network.</p><p>I’m going to describe here 3 quite different things which get called ‘paper mills’. Perhaps there are more than 3 main classes of papermill? Add a comment if you can think of another one!</p><h3>1. Fabrication from scratch</h3><p>Any organisation which fabricates research papers from scratch for sale. People have been fabricating research for as long as research has been a thing, but it’s the <em>organised</em> part that makes this new. This happens at a vastly greater scale than it used to and the problem continues to grow.</p><p>There are definitely templates and re-used images and text in these papers, but I think ‘from scratch’ is a fair distinction between this and the other types.</p><p>All those fake western blots? That’s this. Those cancer genetics papers from obscure hospital research departments? That’s this too. Take a look at cases highlighted by Jennifer Byrne, Elisabeth Bik, Cheshire, Smut Clyde and others for examples.</p><p>My <a href=\"https://medium.com/p/719b8b3b8253\">Papermill Alarm API</a>, mentioned in <a href=\"https://medium.com/p/b89db7feba9d\">previous blog posts</a>, is mostly trained to highlight this kind of papermilling.</p><h3>2. Fabrication by plagiarism</h3><p>Fabrication from scratch requires skill. What if our forger doesn’t know how to write a fake paper from scratch? This is a <em>sad forger:</em></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/250/1*7_PceWGAeLBoczc3dYi2Jg.png\" /><figcaption>I’ve made him look sad by rotating his mouth 180°. Image manipulation is not my forté.</figcaption></figure><p>So what can he do? How can he turn that frown upside-down?</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/363/1*0b31C3OWhZwrtkz9C3Cfww.png\" /></figure><p>The idea is simple. If he can’t write a fake paper, <em>he can just copy a real one</em>! Or he can get creative and copy several bits of papers and paste the bits into a new manuscript. If he’s worried about getting caught by anti-plagiarism software, he can always drop the text into a paraphrasing tool. This is where we get “Tortured Phrases” (<a href=\"https://arxiv.org/abs/2107.06751\">see the great work of Guillaume Cabanac et al on this subject</a>).</p><p>The difference between organised plagiarism and papermilling-from-scratch is simply that, here, the forger starts with 1 or more existing papers and <em>converts </em>them into a ‘new’ one.</p><h3>3. Authorship brokering</h3><p>This is where authorship slots on papers, fabricated or not, are sold. This is actually just the ‘agent’ part of any papermilling operation, but sometimes we see it as a standalone service.</p><p><a href=\"https://retractionwatch.com/2019/07/18/exclusive-russian-site-says-it-has-brokered-authorships-for-more-than-10000-researchers/\">A famous case of a brokerage advertising authorships for sale was brought to light by Retraction Watch some years ago</a>. Anna Abalkina, a researcher at Freie Universität Berlin <a href=\"https://arxiv.org/abs/2112.13322\">investigated this matter by tracking down the papers advertised on that site</a>. It’s a very interesting case and well-worth reading about.</p><p>It’s fortunate that the papers for sale on that site were advertised so openly. Consider now that anyone can sell authorship on any paper privately. How can we detect that? I’ve learned recently that it might be possible to spot these brokered authorships.</p><h3>Join me at ConTech Live!</h3><p>Earlier this year, SAGE Publishing worked with <a href=\"https://opencredo.com/\">Open Credo</a>, a software development consultancy here in London, on a project to see if we could detect unusual co-authorships using the dataset created by Anna Abalkina in her work. If you’d like to know more, I suggest signing up for <a href=\"https://www.contech.live/contech2022\">our forthcoming ConTech presentation!</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=14993c37ebfa\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/719b8b3b8253","title":"How to use the Papermill Alarm API","link":"https://publisherad.medium.com/how-to-use-the-papermill-alarm-api-719b8b3b8253?source=rss-ed0928b1b891------2","published":"Mon, 17 Oct 2022 08:06:01 GMT","description":"","isPermalink":false,"tags":["academic-publishing","peer-review","research-integrity"],"authors":[{"name":"Adam Day","url":null}],"image":null,"modified":null,"contentHtml":"<h4><strong>Also… what is an API?</strong></h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LJsC8JddbGR0xF8phQ_-yA.jpeg\" /><figcaption>API keys come in many forms. Source: wiki commons</figcaption></figure><p>The Papermill Alarm API, is a service which you can send some article metadata to and which will return an alert telling you if the paper <em>looks like </em>past papermill-products. Anyone can use it, but it definitely helps to have the support of an IT or data professional.</p><p>(By the way, API stands for “application programming interface”, but I think my definition is clearer!)</p><p>APIs like this are used all over the web to transmit data between systems. I’d like to tell you that using an API is as easy as using a typical webform, and I’ll describe a simple method for that in a moment. But the truth is that it can be even easier: it can be completely automated.</p><p>The <strong>easiest </strong>way to use any API is a 3 step process:</p><ol><li>Walk into the IT department of the publishing company you work for</li><li>Shout loudly “HELLO! DOES ANYONE HERE KNOW HOW TO WORK AN API?”</li><li>Observe hands being raised and pick the one you like best.</li></ol><p><em>As a backup option, you might subtly rustle a pack of chocolate digestives, or jammie dodgers. Your IT team will know what that means.</em></p><p>Seriously though, using an API is second-nature to a lot of IT professionals. Publishers use APIs routinely to interact with Crossref, ORCID, ROR and others. So there should be someone in any organisation who can help. The Papermill Alarm even comes with code examples in multiple languages.</p><p>But we can actually use it without any code at all.</p><h4>Before we start</h4><p>Let’s sign up for a subscription to the Papermill Alarm API. You’ll notice there are 2 options.</p><ol><li>POST Batches</li><li>POST Single documents</li></ol><p>If you just want to try the Papermill Alarm out, then let’s go with <strong>single documents</strong>. If you are experienced with APIs, then you will want batch processing. Batches are faster and I’d recommend this option for someone with experience of using APIs.</p><h4>Checking an individual document (no coding required!)</h4><p>Now simply</p><ul><li>navigate to <a href=\"https://rapidapi.com/clear-skies-clear-skies-default/api/papermill-alarm\">the RapidAPI page for the Papermill Alarm</a></li><li>click the ‘single documents’ option on the left side</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*bPj4kd8oVZRNW8o2tV3mtw.png\" /></figure><ul><li>under ‘Request Body’, select ‘Body’ and edit the document in there so that it has 3 keys: “id”, “title” & “abstract”</li></ul><p>This might give us a document like this</p><pre>{<br>“id”: “your_document_id”,<br>“title”: “This is not a title of a paper”,<br>“abstract”: “This is just an example piece of text. Not a real abstract.”<br>}</pre><blockquote>Note that the format here is ‘JSON’ and it is quite strict. We must use double quotes, <strong>not </strong>single quotes. There are colons between each ‘key’ and ‘value’ and then there’s a comma at the end of each line EXCEPT the last one!</blockquote><p>Click ‘test endpoint’.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5M3DQPbqN9WsWefU2qM8fA.png\" /></figure><p>That was easy wasn’t it?</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/736/1*mZNbhde09kKiBN2ZazmdXQ.png\" /></figure><h4>Checking documents in a fast, automated way</h4><p>But this could be a lot of work for a large publishing house with 100s or 1000s of new submissions coming in each day. How should we use the Papermill Alarm API if we have a lot of documents to check?</p><p>This is actually exactly what the API is designed for and, while there wasn’t much work in the above single-document check, it is quite straightforward to check as many documents as we want in a completely automated way.</p><p>Imagine eating breakfast every morning smugly aware that a computer has already automatically flagged papermill-products coming into your work queue.</p><p>I’m providing demo Python code for this in <a href=\"https://github.com/ad48/papermill_alarm_get_started\">a github repository</a>. This example will show you how to check a large dataset. If you have a data source for your new submissions it should be straightforward to set up a scheduled task to check those and alert you when red alerts are triggered.</p><p>My example is intended for someone with experience of Python. But note that RapidAPI also provides code examples in a range of languages to suit the skills of any particular IT professional.</p><p>If you’d like to hear more about scholarly APIs for papermill detection, you should join me for <a href=\"https://www.contech.live/api\">this ConTech Live API series presentation</a>. See you there!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=719b8b3b8253\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/c507b12aa4ac","title":"Papermills in peer-review","link":"https://publisherad.medium.com/papermills-in-peer-review-c507b12aa4ac?source=rss-ed0928b1b891------2","published":"Mon, 12 Sep 2022 07:32:47 GMT","description":"","isPermalink":false,"tags":["naturallanguageprocessing","peer-review","research-integrity","academic-publishing"],"authors":[{"name":"Adam Day","url":null}],"image":null,"modified":null,"contentHtml":"<blockquote>Dissecting a joke is a bit like dissecting a frog. No one needs to know what’s inside and the frog dies. — Jimmy Carr</blockquote><p>Last week, a paper I wrote on the subject of peer-review fraud was published in the journal <em>Scientometrics </em>(<a href=\"https://rdcu.be/cVeAZ\">free link here</a>, <a href=\"https://arxiv.org/abs/2202.03310\">preprint here</a>)<em>.</em></p><p>It was an interesting project to work on. I found a lot of examples where one referee would write a report during peer-review and then another referee would write an identical report in some other peer-review of some other paper. (I wasn’t the first person to observe this behaviour, but I think this was the first method to detect it automatically.)</p><p>If you look at all the data, you start to see networks of these individuals copying peer-review comments. At least one of these clusters is definitely the work of a paper mill (see if you can guess which one!)</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/748/1*FYnTk1JQMAlYCBrV7a0hdQ.png\" /><figcaption>Each triangular node represents a referee account. Each red link shows that 2 referees have, at some point, written identical comments in peer-review.</figcaption></figure><p>I think of this as ‘tip-of-the-iceberg’ analysis because only a subset of paper-mill accounts and papers will ever show up when we use this method.</p><p>Be that as it may, there is a huge bonus here. Say we find one referee account duplicating a single comment from that big papermill cluster. In that case, we can investigate that referee’s publication history, the other papers they’ve reviewed, the authors who have recommended them to review, their co-authors and so on. So a single result can be the first domino.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/726/1*JZ_4Lel5UL2fwuxcfPbRhA.png\" /><figcaption>With a bit of coaxing, we get a vast number of user accounts to investigate. This is the same diagram as above once we add some more search methods and connect author and co-author accounts. Authors are green nodes, co-authors are grey. There’s some noise here, so a little bit of simple graph analysis lets us prioritise the accounts with the most connections.<a href=\"https://rdcu.be/cVeAZ\"> Details in the paper</a>.</figcaption></figure><h3>When to share</h3><p>I may have laboured this point excessively in past blog posts, but one thing that bothered me a lot in working on this paper was whether to share the results of the work at all. Simply put: sharing <em>any</em> misconduct investigation details publicly has the potential to educate organised research fraudsters on how to circumvent detection. It’s therefore possible that sharing the method undermines it. Methods are like frogs.</p><p>Publishers often advertise their plagiarism detection tools on their submission pages. I’m sure that this is an effective deterrent some of the time, but I wonder if it is also partly responsible for the rise of paraphrasing in plagiarism (because plagiarists learned that plagiarism detectors can’t detect paraphrasing terribly well).</p><p>On the other hand, one might argue that bad actors will inevitably learn about and find their way around any detection method eventually. It’s not clear what difference advertising it makes.</p><p>And there are the positive benefits to sharing:</p><ul><li>Even if fraudsters work around this detection method, doing so is <em>harder</em> than simply copy/pasting review comments as they were doing before.</li><li>Sharing the method is one of the only ways to get it used broadly. It won’t have much effect if only one publisher uses it.</li><li>One thing that can’t be worked around is the data already in publishers’ systems. This analysis can be run retrospectively and suspect accounts can be flagged and investigated.</li><li>Even if a method is well-known, there’s always a future, naïve, new generation of fraudsters who don’t know about it.</li></ul><p>What’s the ‘right’ thing to do? As with all ethical matters: it’s debatable. But our decision was that, considering the above, publishing the method would move the needle in the right direction.</p><h3>This brings me to a somewhat bigger question.</h3><p>I spent many years working as a journal editor and, when I found a case of suspected misconduct, I would follow a well-defined procedure to investigate it.</p><ul><li>First, discuss the case carefully with various stakeholders.</li><li>Then, as a rule, contact the authors and show them the evidence that they had committed that misconduct.</li></ul><p>We would never <em>accuse </em>in that first message. We would simply show the authors the evidence and ask for an explanation. I’ll tell you now, that if I ever presented an author with evidence like that, I was 100% sure that they had done something wrong and that the evidence proved it clearly. (Nevertheless, I would receive some wonderfully creative excuses!)</p><p>But here’s the problem: we can all agree that it’s right and fair to show someone the evidence if they are being accused of wrongdoing. But, what if you are not dealing with an author? What if you are dealing with an organisation that mills thousands of fake papers every year? In that case, the evidence you send them is just teaching them how to improve their business and you are not communicating with the ‘author’ at all.</p><p>So, there’s the dilemma.</p><ul><li>We don’t always know if we are dealing with a real author, or a fake account controlled by a papermill.</li><li>So do we present the evidence and risk harming our chances of detecting fraud in the future?</li><li>Or don’t present the evidence and risk labelling someone as ‘guilty’ without giving them a fair opportunity to see or respond to the evidence?</li></ul><p>Having thought about this a lot, I think the only practical option is to aim to <em>never</em> share any information with suspected papermills. How that’s done (and particularly how to do it <em>ethically</em>) is, as I say: debatable.</p><p>As for methods: it depends on how fragile the method is. If it’s easy to work around a detection method, I’m certainly not going to share it. If it’s hard to work around it, then we might <em>move the needle</em> by sharing the method particularly if it’s easy to use.</p><p>Right now, the methods exist to detect the overwhelming majority of papermill products.</p><ul><li>Some of those methods should never see the light of day.</li><li>Some of those methods might be published.</li><li>Others might simply be shared privately.</li></ul><h4>Here are some examples of methods I think should be shared!</h4><p>My recent paper (mentioned above):</p><ul><li><a href=\"https://rdcu.be/cVeAZ\">Exploratory analysis of text duplication in peer‑review reveals peer‑review fraud and paper mills</a></li></ul><p>Some recent blog posts</p><ul><li><a href=\"https://publisherad.medium.com/how-to-find-all-the-papermills-step-1-f00c624facb1\">How to find all the papermills</a></li><li><a href=\"https://publisherad.medium.com/the-papermill-alarm-patterns-in-pubmed-b89db7feba9d\">The Papermill Alarm: patterns in Pubmed</a></li></ul><p>Any questions, get in touch!</p><ul><li><a href=\"https://twitter.com/AdamSci12\">Twitter</a></li><li><a href=\"https://www.linkedin.com/in/%F0%9F%98%B7-adam-day-45529a17/\">LinkedIn</a></li><li><a href=\"https://clear-skies.co.uk/\">Clear Skies</a> (my side-business — not related to my work at SAGE)</li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c507b12aa4ac\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/b89db7feba9d","title":"The Papermill Alarm: Patterns in PubMed","link":"https://publisherad.medium.com/the-papermill-alarm-patterns-in-pubmed-b89db7feba9d?source=rss-ed0928b1b891------2","published":"Tue, 23 Aug 2022 07:02:46 GMT","description":"","isPermalink":false,"tags":["research-integrity","naturallanguageprocessing","data-visualization","academic-publishing"],"authors":[{"name":"Adam Day","url":null}],"image":null,"modified":null,"contentHtml":"<p>This post is about <a href=\"https://rapidapi.com/clear-skies-clear-skies-default/api/papermill-alarm\">The Papermill Alarm</a>: an API for detecting potential papermill-products.</p><p>There’s a field of study called ‘stylometry’ where we look at the statistical properties of someone’s writing and use that to model their ‘style’. People write in idiosyncratic ways. So, for example, I often start sentences with ‘so, for example’ when I could more succinctly say ‘for example’. The stylistic properties of written work can therefore, potentially, give away the identity of the author. <a href=\"https://www.scientificamerican.com/article/how-a-computer-program-helped-show-jk-rowling-write-a-cuckoos-calling/\">Famously, it’s been shown that J K Rowling shares stylistic traits with Robert Galbraith</a>, something which is confirmed by the fact that both of them, without exception, are the same person.</p><p>Stylometry methods might seem dated these days, but don’t discount them. They have their uses!</p><p>I recently made a blog post about 2 of the main roles I see in papermilling — Agents and Forgers. Forging fake research papers is a skill. You have to know enough of the science to be able to write a believable fake. (It’s depressing. Think about it: the people forging those fakes must have had real scientific ambitions at some point.)</p><p>The thing is, though, that if you can write a believable fake paper about cancer genetics, you probably can’t write a believable fake about geology, or about engineering. So, what do you do? (Keep writing fakes about cancer genetics, of course!) The result is that we find unusual clusters of samey papers.</p><h3>Let’s take a look at PubMed</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Jr9E1K-nAJzk_LH_JIIFAw.png\" /></figure><p>What you’re seeing here is a map of PubMed. More specifically, this is the titles of PubMed papers put through 2 processes:</p><ul><li>The Univeral Sentence Encoder is used to turn each title into a 1024 dimensional vector. That’s too many dimensions for me. If I try to visualise 1024 dimensions in my head, it makes my eyes water.</li><li>A process called ‘UMAP’ is used to compress those vectors into simple 2-dimensional x and y coordinates. Much better!</li></ul><p>Maps like this are easy to make and a lot of fun to explore. They do lose a lot of data in that compression stage and so, while the map can be used for this kind of rough visual exploration, it shouldn’t be thought of as a tool for precision science. Also, keep in mind that we are just looking at <em>titles </em>here. The Papermill Alarm models abstracts as well.</p><p>You might recognise some of the features of PubMed on this map.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Y4YZ49eiSDrgdtjroprxMA.png\" /></figure><p>The bright green data-points are the titles of <a href=\"https://docs.google.com/spreadsheets/d/1zKxfaqug4ZhwHyGzslF38pFyC8xtU8lzmmOFMGYITDI/edit#gid=0\">papers that have been identified as suspicious in various lists of papers by Elisabeth Bik, Smut Clyde and others</a>. Note that these cases are concentrated in specific regions of PubMed.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*B4gNCQZyHA1uMUjykCZACw.png\" /><figcaption>There are 2 new clusters here at the foot of the map. One concerns polymers and the other one mathematics. These 2 don’t appear to overlap PubMed.</figcaption></figure><p><em>I really can’t overstate the importance of the kind of work that people like Elisabeth Bik, Smut Clyde, several anonymous PubPeer commenters (and countless referees and journal editors behind the scenes) do in carefully reviewing these papers for flaws!</em></p><p>Wouldn’t it be nice to flag papers which share the stylistic features of those groups? So that, if a paper fitting one of those specific regions comes into your peer-review system, you can be alerted that it looks a bit like a papermill? That would be good wouldn’t it? That’s what <a href=\"https://rapidapi.com/clear-skies-clear-skies-default/api/papermill-alarm\">The Papermill Alarm</a> does!</p><p>Here is a map showing a random sample of what The Papermill Alarm thinks look like papermills (in red). Note the clear correlation with those green dots.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*oHngLlm7tW3_wbocQNjbiA.png\" /></figure><h3>The key takeaways</h3><p>You can see now that there is clear agreement between the red alerts and the papers that have already been picked out by hand.</p><p>There’s also definite scope to expand the Papermill Alarm to fields outside of PubMed. E.g. those green clusters at the bottom.</p><p>Finally, the Papermill Alarm is also detecting <em>growth</em> in papers that trigger red alerts. That’s how I can be confident that you will see alerts for more papermill papers by the Papermill Alarm in the future. (I should be clear that I don’t intend this as a quantitative estimate of the scale of papermilling.)</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*OTSy5DD_PGMIKcyZ\" /></figure><p>Any questions? Get in touch!</p><ul><li><a href=\"https://twitter.com/AdamSci12\">Twitter</a></li><li><a href=\"https://www.linkedin.com/in/%F0%9F%98%B7-adam-day-45529a17/\">LinkedIn</a></li></ul><h3>Appendix: Some caveats about the visualisations</h3><p>Reminder: this is just for illustrative purposes.</p><ul><li>The 50,000 blue dots, which represent all of PubMed are also only a small proportion of the total. There are millions of papers in PubMed.</li><li>Red alerts make up only around 1% of the total PubMed output. This might not be obvious from looking at the above because everything is subsampled. Red dots are also deliberately brighter than blues.</li><li>The 2,000 red alerts (red dots) you’re seeing are only a small (randomly sampled) percentage of all the papers picked up by the Papermill Alarm.</li><li>There are around 3,000 green dots.</li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b89db7feba9d\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/469b3f206055","title":"Patterns and Evidence (and how to find papermills before peer-review)","link":"https://publisherad.medium.com/patterns-and-evidence-469b3f206055?source=rss-ed0928b1b891------2","published":"Fri, 05 Aug 2022 08:03:37 GMT","description":"","isPermalink":false,"tags":["academic-publishing","rapidapi","research-integrity","naturallanguageprocessing","api-development"],"authors":[{"name":"Adam Day","url":null}],"image":null,"modified":null,"contentHtml":"<p>I once saw a brilliant presentation about how simple data analysis can detect credit card fraud**. The presentation showed a <em>pattern</em> in how people use their credit cards. Given a large number of people who had been victims of credit card fraud, this pattern showed there was just 1 store in-particular where they had all used their cards. There was no <em>observational evidence</em> of someone at that store stealing card details. No camera footage, no eye-witnesses, but the pattern was obvious in the data.</p><p>The pattern is <em>circumstantial evidence</em>: a useful lead that tells law-enforcement where to look for the <em>observational evidence</em> they need.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/424/1*dRQZ1u6E11FXrouAsZdRVA.png\" /><figcaption><a href=\"https://commons.wikimedia.org/wiki/File:Fingerprint-146242.svg\">image CC-0</a></figcaption></figure><p>With misconduct detection, we are also often looking at patterns rather than direct evidence of misconduct. Take plagiarism detection:</p><ul><li>We detect a pattern (which is an overlap in text between 2 documents)</li><li>There are various levels to which it’s ok to copy text with proper attribution and so simply finding overlapping text between 2 documents isn’t evidence of plagiarism on its own (but I still think it’s a good place to start!).</li><li>Finding the pattern saves a lot of time by telling us when to look much more carefully at a pair of documents to get evidence of misconduct.</li></ul><h3>But where to start with papermills?</h3><p>Recently, I shared <a href=\"https://publisherad.medium.com/how-to-find-all-the-papermills-step-1-f00c624facb1\">a simple method to identify most of the existing published papermill content</a>. This method is particularly nice, because — while we are detecting a pattern (text duplication again) the investigation required to confirm misconduct is as simple as it gets.</p><p>But, from a publisher’s point of view, it would be much better to identify papermills early on — before peer-review. So I recently released a simple API that does just that. The Papermill Alarm identifies unusual patterns in the text of new submissions and issues a warning if those patterns appear consistent with papermills.</p><blockquote>Here it is: <a href=\"https://rapidapi.com/clear-skies-clear-skies-default/api/papermill-alarm/\">https://rapidapi.com/clear-skies-clear-skies-default/api/papermill-alarm/</a></blockquote><p>For example, you may have seen <a href=\"https://scienceintegritydigest.com/2020/02/21/the-tadpole-paper-mill/\">Elisabeth Bik’s excellent work on the ‘tadpole’ papermill</a>. This shows a very obvious pattern in the titles of several papers which appeared to come from a papermill. <a href=\"https://forbetterscience.com/tag/smut-clyde/\">You can see similar patterns in the many papers highlighted by Smut Clyde’s insightful work</a>.</p><p>The Papermill Alarm</p><ul><li>receives article metadata (from you)</li><li>detects patterns like those described above (and more)</li><li>and returns a warning when it finds them.</li></ul><p>Checking your papers is simple. The Papermill Alarm highlights papers that you might want to check in-detail and greatly reduces the amount of time you might have to spend looking for such cases.</p><p>That being said, I don’t want to overstate what a tool like this can do.</p><ul><li>It doesn’t find observational evidence, it detects patterns that are consistent with papermills.</li><li>While results are already extremely useful, it’s still an early-iteration of something with a lot of potential for development.</li></ul><p>I’ll say more about testing the Papermill Alarm in a future post. In the meantime, if you have any questions at all, <a href=\"https://twitter.com/AdamSci12\">get in touch</a>!</p><p>** P.S. I found the presentation I mentioned at the start. It’s possible that one of the balding heads in the audience is mine! <a href=\"https://youtu.be/6eOr7lp4MPg?t=805\">https://youtu.be/6eOr7lp4MPg?t=805</a> <a href=\"https://www.slideshare.net/tdunning/cheap-learningdunning9182015\">https://www.slideshare.net/tdunning/cheap-learningdunning9182015</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=469b3f206055\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/f00c624facb1","title":"How to find all the papermills (step 1)","link":"https://publisherad.medium.com/how-to-find-all-the-papermills-step-1-f00c624facb1?source=rss-ed0928b1b891------2","published":"Mon, 25 Jul 2022 08:03:29 GMT","description":"","isPermalink":false,"tags":["peer-review","crossref","academic-publishing","research-integrity"],"authors":[{"name":"Adam Day","url":null}],"image":null,"modified":null,"contentHtml":"<p>The first time I flew over the North Atlantic was quite an experience. Through the clouds, I could see some little white boats out sailing in the sea. It was puzzling: from 30,000 feet, those boats must have been huge for me to be able to see them at all. Suddenly, the clouds were gone and, through clear skies, I could see a really big ‘little white boat’ and I understood at once what I had been looking at.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*DePhN8tzJjWHH38gUksYLA.jpeg\" /><figcaption>Image CC-BY: wiki commons <a href=\"https://commons.wikimedia.org/wiki/File:Sarqarput-strait-iceberg-aerial.jpg\">https://commons.wikimedia.org/wiki/File:Sarqarput-strait-iceberg-aerial.jpg</a></figcaption></figure><p>With papermills, people often say that they only see ‘the tip of the iceberg’. (It’s almost a cliché!) If fake papers were easy to detect, then forgers would just learn to get better at writing them. I guess that’s what actually happened over time and it’s why we need to be super-careful about sharing detection methods.</p><p>This is why I’ve thought very carefully about making this post (and why I’ve no intention of publishing steps 2, 3 etc).</p><h4>Here’s what we know:</h4><ul><li><a href=\"https://medium.com/p/21ebd625ad40\">Papermills use duplicate submission routinely</a></li><li><a href=\"https://youtu.be/6yA3phww3m8?t=1579\">Detecting duplicate submission is easy</a></li><li><a href=\"https://medium.com/p/9ab21e84d0c9\">Retrospective analysis is a necessity</a></li></ul><p>With this in mind, it’s actually very simple for publishers to see the iceberg.</p><ol><li>Make your submission dates visible in Crossref.</li></ol><p>It might not be trivial to do, but it shouldn’t be hard or controversial. Most journals make their submission dates visible in article XML, so this wouldn’t make any data visible that isn’t already, it just makes it easier to work with. In fact, several publishers are doing this already.</p><p>2. Search for your submissions in Crossref.</p><p>This is also not hard to do. The results will show you when your submissions were published by others and (with the above step having been done) when they were submitted to those other publishers. Conveniently, there are some pre-built options that will make doing this trivial (see ‘Article Tracking Options’ below!).</p><p>3. Finally, if you find a duplicate submission, contact the publisher of that duplicate submission and show them: the submission and decision dates as well as the full-text. For them to confirm duplicate submission requires only a quick manual comparison of the manuscripts. It’s much, <strong>much</strong> quicker than a full misconduct investigation.</p><h3>What does this give us?</h3><p>Well, there are 2 things:</p><ul><li>All of the papermill submissions (at least all of those that have been using duplicate submission — there are exceptions).</li><li>Any other authors using duplicate submission as a hedge.</li></ul><p>Isn’t this great? No fancy AI, no need to actually detect and prove the fakery. Here is all the evidence you need to prove a paper has breached your terms of service. Furthermore, if we take this to its logical conclusion(which might be an up-front cross-publisher full-text check made at the submission of any article), you can see that there really isn’t any way around it. The paper-mills will have to stop sending the same paper to different publishers concurrently. That will slow them down considerably. It’s also why I’m comfortable sharing this publicly.</p><p>But how to tell the two apart — papermills and ‘other’ duplicates? Well, I’m not going to share that here, but what I will say is that it isn’t hard. The iceberg is obvious once you see it through clear skies ;)</p><p>At this point, it’s up to you how you proceed. You might want to analyse the data to learn more about papermills. You might want to conduct misconduct investigations. You might see opportunities to collaborate with other publishers. After all, this method does require that we all work together.</p><h3>Article tracking options</h3><ol><li>The SAGE Rejected article tracker.</li></ol><p>This was a project that started out as <a href=\"https://publisherad.medium.com/rejected-article-tracking-with-the-crossref-api-22d75a874904\">a blog post that I wrote some years ago in my spare time</a>. It later became <a href=\"https://joss.theoj.org/papers/10.21105/joss.03348\">an open source project from SAGE Publishing with its own peer-reviewed paper</a>. It’s free and it will get the job done (if a little slowly). Note that, in this case, it’s important to track all articles, this includes accepted articles, new submissions and (importantly!) abandoned submissions as well as rejected articles. Remember how<a href=\"https://medium.com/p/9ab21e84d0c9\"> I said that we sometimes see the same paper published in 2 places</a>? This is how to find those cases.</p><p>2. <a href=\"https://rapidapi.com/clear-skies-clear-skies-default/api/article-tracker/details\">My own “Article Tracker” API</a>.</p><p>This is part of a side-project (not related to my work at SAGE) and I hadn’t intended to make it publicly available, but after seeing the value of this tool in detecting papermills, here it is. It’s the best tool of its kind as far as I know. I hope that it helps with this problem.</p><p>The Article Tracker has the following features:</p><ul><li>Fast: in testing, I’ve been able to track around 500 articles per minute.</li><li>Accurate: the current accuracy is around 96–98% on test data. There is a planned upgrade which will push accuracy even higher.</li><li>Does duplicate submission checking (as described above) out of the box.</li><li>I’ve set pricing at a low level where I think any publishing house should be able to easily afford to search for all of their submissions as well as historic submissions without running into limits.</li></ul><p>This blog post is actually the first time I’ve advertised the Article Tracker’s existence, so you might consider this to be a soft-launch of the service. Any questions, comments or feedback would be very welcome.</p><ul><li><a href=\"https://clear-skies.co.uk/contact/\">You can contact me through the Article Tracker’s parent company</a> or</li><li><a href=\"https://twitter.com/AdamSci12\">find me on Twitter</a>.</li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f00c624facb1\" width=\"1\" height=\"1\" alt=\"\">"},{"id":"https://medium.com/p/21ebd625ad40","title":"Papermills: What’s wrong with duplicate submission?","link":"https://publisherad.medium.com/papermills-whats-wrong-with-duplicate-submission-21ebd625ad40?source=rss-ed0928b1b891------2","published":"Wed, 22 Jun 2022 08:21:08 GMT","description":"","isPermalink":false,"tags":["peer-review","academic-publishing","research-integrity"],"authors":[{"name":"Adam Day","url":null}],"image":null,"modified":null,"contentHtml":"<p>You want to submit a paper to a journal for consideration. It takes, say, 3 months on average for a paper to go through peer-review, so if your paper gets rejected, you’ve lost 3 months of your time. Wouldn’t it be better to hedge your bets? Submit the paper to multiple journals all at once. Then, hopefully, one will accept the paper. What’s wrong with this hedge?</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*bzyWsV4uzFXudMTOhSMggw.jpeg\" /><figcaption>(image credit: <a href=\"https://www.flickr.com/photos/livingwallsnz/6387078175/\">Twining Valley Nurseries CC-BY-NC-ND 2.0</a>)</figcaption></figure><p>Duplicate submission is a very simple form of misconduct. It’s where an author of a research manuscript submits that manuscript for peer-review to 2 or more journals. It sounds almost sensible — wouldn’t it be a more efficient way to get papers through peer-review quickly?</p><h4>I’ll give you the list!</h4><p>1. You only need to think about it to see an obvious problem. Each paper goes to 2 or more referees, so duplicating that process starts to absorb a lot of the available referee-time. If everyone starts doing it, eventually we reach a point where the resources don’t exist to deal with new submissions. (And there are already reports of journals struggling to find peer-reviewers for submitted manuscripts.)</p><p>2. Duplicate submission sometimes leads to the same paper being published in 2 different places. That’s bad. The author will almost certainly end up with 1 or both papers retracted if that happens.</p><p>3. There’s a more cynical reason to make duplicate submissions. Let’s imagine an author has a paper that is, well, not great and they just want to get it published to pad out their CV. Let’s also imagine that the author <em>knows</em> that the paper isn’t all that great. What they can do is duplicate-submit it to multiple journals and wait for the various responses. Maybe 1 journal rejects, another 3 ask for major revisions and journal #5 simply asks for some minor corrections. The author can just look at which response is most lenient (i.e. journal #5) and proceed with that journal — abandoning all other submissions and completely ignoring the feedback and hard work of the referees. Let’s think about this scenario for a moment. It’s clearly deliberate manipulation of the publishing process. If you were a journal editor for journal #5 and you found that you had published that paper, should it affect your opinion on the validity of the science in that paper? I think it should. The author wouldn’t be hedging their bets if they were confident about the work!</p><p>4. Duplicate submission is a breach of contract with the publisher. It’s not a breach of some clause buried deep in a list of terms & conditions that no one reads. It’s usually an explicit check-box on any submission form, so it’s very clear that the author would know that it isn’t considered ok. We therefore know that it’s dishonest behaviour that causes people to duplicate-submit and <em>not </em>simply a naïve mistake.</p><p>But where’s the harm? The above sounds fairly harmless at the level of an individual author. It’s a slap-on-the wrist offense, isn’t it? Here’s a thought for you:</p><blockquote>If you know that you can’t detect a lot of misconduct, and you catch someone doing <strong>one </strong>dishonest<strong> </strong>thing, what are the chances that that is the only dishonest thing they have done?</blockquote><p>5. We know that papermills, organisations which create fake research manuscripts for money, use duplicate submission routinely. No doubt for the reasons outlined in (3), above. It’s so that they can find the path of least resistance through the peer-review system. Papermills are pervasive enough that I think detecting duplicate submission is very likely to represent papermill detection in many cases.</p><p>So, let’s think about the value of cracking-down on duplicate submission:</p><ul><li>It would free up a lot of referee-time.</li><li>It would prevent low-quality work being published.</li><li>It would allow identification of a large swathe of the fake research published by papermills. That would help to characterise the problem and determine the best reaction.</li></ul><p>But how do we do that?</p><h3>As I said last week</h3><p>I’ve got a few plans for blog posts about papermills. I’m being very careful about what I post — I think it’s important to avoid posting anything that could be used by papermills to avoid getting caught.</p><p>However, if I think of something valuable to share, I’ll do that.</p><ul><li>If you’re a publisher reading this, tell your friends.</li><li>Also… find the person in your organisation who uploads your data to Crossref. Find them and tell them to follow this blog.</li><li>You might also <em>particularly</em> recommend this blog to your data scientists, developers, engineers and any other technical folks with an interest in research integrity. There will be things for them here.</li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=21ebd625ad40\" width=\"1\" height=\"1\" alt=\"\">"}]}