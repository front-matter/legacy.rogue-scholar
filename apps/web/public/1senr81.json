{"version":"https://jsonfeed.org/version/1.1","id":"1senr81","title":"JSC Accelerating Devices Lab","description":"Various notes from the Accelerating Devices Lab (X-Dev) of Jülich Supercomputing Centre","homePageUrl":"https://x-dev.pages.jsc.fz-juelich.de//","feedUrl":"https://x-dev.pages.jsc.fz-juelich.de/feed.xml","favicon":null,"items":[{"id":"https://x-dev.pages.jsc.fz-juelich.de//2022/11/02/gpu-vendor-model-compat","title":"GPU Vendor/Programming Model Compatibility Table","link":"https://x-dev.pages.jsc.fz-juelich.de//2022/11/02/gpu-vendor-model-compat","published":"2022-11-02T08:36:12+00:00","description":"For a recent talk at DKRZ in the scope of the natESM project, I created a table summarizing the current state of using a certain programming model on a GPU of a certain vendor, for C++ and Fortran. Since it...","isPermalink":false,"tags":["HPC","GPU"],"authors":[{"name":"Andreas","url":null}],"image":"https://x-dev.pages.jsc.fz-juelich.de//assets/img/posts/2022-11-02-gpu-compat-matrix/gpu-vendor-model-matrix.png","modified":"2022-11-02T08:36:12+00:00","contentHtml":"<p>For a recent <a href=\"https://indico.dkrz.de/event/48/\">talk at DKRZ</a> in the scope of the <a href=\"https://www.nat-esm.de/services/documentation\">natESM project</a>, I created a table summarizing the current state of using a certain programming model on a GPU of a certain vendor, for C++ and Fortran. Since it lead to quite a discussion in the session, I made a standalone version of it with some updates and elaborations here and there.</p>\n\n<p>I present, the <strong>GPU Vendor/Programming Model Compatibility Table</strong>!</p>\n\n<h2 id=\"compatibility-table\">Compatibility Table</h2>\n\n<p><em>Read below for some <a href=\"#caveats\">caveats</a> and <a href=\"#technical-background\">technical background</a>! There is also a <a href=\"/assets/img/posts/2022-11-02-gpu-compat-matrix/gpu-vendor-model-matrix.pdf\">PDF</a> and an <a href=\"/assets/img/posts/2022-11-02-gpu-compat-matrix/gpu-vendor-model-matrix.svg\">SVG</a> version available.</em></p>\n\n<hr />\n\n<section id=\"compat-matrix\">\n\t<dl>\n\t\t<dt><svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg></dt>\n\t\t<dd>Full vendor support</dd>\n\t\t<dt><svg height=\"7.92\" overflow=\"visible\" version=\"1.1\" width=\"15.85\"><g transform=\"translate(0,7.92) matrix(1 0 0 -1 0 0) translate(7.92,0) translate(0,4.75)\" fill=\"#D3C65D\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M -7.92 3.17 C -7.92 -1.21 -4.38 -4.75 0 -4.75 C 4.38 -4.75 7.92 -1.21 7.92 3.17 Z\" style=\"stroke:none\"></path></g></svg></dt>\n\t\t<dd>Indirect, but comprehensive support, by vendor</dd>\n\t\t<dt><svg height=\"12.64\" overflow=\"visible\" version=\"1.1\" width=\"12.64\"><g transform=\"translate(0,12.64) matrix(1 0 0 -1 0 0) translate(6.32,0) translate(0,6.32)\" fill=\"#FBBC6A\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 6.32 6.32 L -6.32 6.32 L -6.32 -6.32 L 6.32 -6.32 Z\" style=\"stroke:none\"></path></g></svg></dt>\n\t\t<dd>Vendor support, but not (yet) entirely comprehensive</dd>\n\t\t<dt><svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg></dt>\n\t\t<dd>Comprehensive support, but not by vendor</dd>\n\t\t<dt><svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg></dt>\n\t\t<dd>Limited, probably indirect support -- but at least some</dd>\n\t\t<dt><svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg></dt>\n\t\t<dd>No direct support available, but of course one could ISO-C-bind your way through it or directly link the libraries</dd>\n\t\t<dt>C</dt>\n\t\t<dd>C++ (sometimes also C)</dd>\n\t\t<dt>F</dt>\n\t\t<dd>Fortran</dd>\n\t</dl>\n\t<table id=\"compat-table\">\n\t\t<thead>\n\t\t\t\n\t\t\t<tr>\n\t\t\t\t<td class=\"empty\"></td>\n\t\t\t\t<td colspan=\"2\" class=\"level-1\">CUDA</td>\n\t\t\t\t<td colspan=\"2\" class=\"level-1\">HIP</td>\n\t\t\t\t<td colspan=\"2\" class=\"level-1\">SYCL</td>\n\t\t\t\t<td colspan=\"2\" class=\"level-1\">OpenACC</td>\n\t\t\t\t<td colspan=\"2\" class=\"level-1\">OpenMP</td>\n\t\t\t\t<td colspan=\"2\" class=\"level-1\">Standard</td>\n\t\t\t\t<td colspan=\"2\" class=\"level-1\">Kokkos</td>\n\t\t\t\t<td colspan=\"2\" class=\"level-1\">ALPAKA</td>\n\t\t\t\t<td class=\"level-1 etc\">etc</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td class=\"empty\"></td>\n\t\t\t\t<td class=\"level-2\">C</td>\n\t\t\t\t<td class=\"level-2\">F</td>\n\t\t\t\t<td class=\"level-2\">C</td>\n\t\t\t\t<td class=\"level-2\">F</td>\n\t\t\t\t<td class=\"level-2\">C</td>\n\t\t\t\t<td class=\"level-2\">F</td>\n\t\t\t\t<td class=\"level-2\">C</td>\n\t\t\t\t<td class=\"level-2\">F</td>\n\t\t\t\t<td class=\"level-2\">C</td>\n\t\t\t\t<td class=\"level-2\">F</td>\n\t\t\t\t<td class=\"level-2\">C</td>\n\t\t\t\t<td class=\"level-2\">F</td>\n\t\t\t\t<td class=\"level-2\">C</td>\n\t\t\t\t<td class=\"level-2\">F</td>\n\t\t\t\t<td class=\"level-2\">C</td>\n\t\t\t\t<td class=\"level-2\">F</td>\n\t\t\t\t<td class=\"level-2\">Python</td>\n\t\t\t</tr>\n\t\t</thead>\n\t\t<tbody>\n\t\t\t<tr>\n\t\t\t\t<td class=\"vendor\">NVIDIA</td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"CUDA C/C++ is supported on NVIDIA GPUs through the CUDA Toolkit\"><a href=\"#desc-cudac\">1</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"CUDA Fortran, a proprietary Fortran extension, is supported on NVIDIA GPUs via the NVIDIA HPC SDK\"><a href=\"#desc-cudafortran\">2</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"7.92\" overflow=\"visible\" version=\"1.1\" width=\"15.85\"><g transform=\"translate(0,7.92) matrix(1 0 0 -1 0 0) translate(7.92,0) translate(0,4.75)\" fill=\"#D3C65D\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M -7.92 3.17 C -7.92 -1.21 -4.38 -4.75 0 -4.75 C 4.38 -4.75 7.92 -1.21 7.92 3.17 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"HIP programs can directly use NVIDIA GPUs via a CUDA backend; HIP is maintained by AMD\"><a href=\"#desc-nvidiahip\">3</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"No such thing like HIP for Fortran, but AMD offers Fortran interfaces to HIP and ROCm libraries in hipfort\"><a href=\"#desc-nvidiahipfortran\">4</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"SYCL can be used on NVIDIA GPUs with experimental support either in SYCL directly or in DPC++, or via hipSYCL\"><a href=\"#desc-nvidiasycl\">5</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"No such thing like SYCL for Fortran\"><a href=\"#desc-syclfortran\">6</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"OpenACC C/C++ supported on NVIDIA GPUs directly (and best) through NVIDIA HPC SDK; additional, somewhat limited support by GCC C compiler and in LLVM through Clacc\"><a href=\"#desc-openaccc\">7</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"OpenACC Fortran supported on NVIDIA GPUs directly (and best) through NVIDIA HPC SDK; additional, somewhat limited support by GCC Fortran compiler and Flacc\"><a href=\"#desc-openaccfortran\">8</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"12.64\" overflow=\"visible\" version=\"1.1\" width=\"12.64\"><g transform=\"translate(0,12.64) matrix(1 0 0 -1 0 0) translate(6.32,0) translate(0,6.32)\" fill=\"#FBBC6A\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 6.32 6.32 L -6.32 6.32 L -6.32 -6.32 L 6.32 -6.32 Z\" style=\"stroke:none\"></path></g></svg>\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"OpenMP in C++ supported on NVIDIA GPUs through NVIDIA HPC SDK (albeit with a few limits), by GCC, and Clang; see OpenMP ECP BoF on status in 2022.\"><a href=\"#desc-nvidiaopenmpc\">9</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"12.64\" overflow=\"visible\" version=\"1.1\" width=\"12.64\"><g transform=\"translate(0,12.64) matrix(1 0 0 -1 0 0) translate(6.32,0) translate(0,6.32)\" fill=\"#FBBC6A\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 6.32 6.32 L -6.32 6.32 L -6.32 -6.32 L 6.32 -6.32 Z\" style=\"stroke:none\"></path></g></svg>\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"OpenMP in Fortran supported on NVIDIA GPUs through NVIDIA HPC SDK (but not full OpenMP feature set available), by GCC, and Flang\"><a href=\"#desc-nvidiaopenmpfortran\">10</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"pSTL features supported on NVIDIA GPUs through NVIDIA HPC SDK\"><a href=\"#desc-nvidiastandardc\">11</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Standard Language parallel features supported on NVIDIA GPUs through NVIDIA HPC SDK\"><a href=\"#desc-nvidiastandardfortran\">12</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Kokkos supports NVIDIA GPUs by calling CUDA as part of the compilation process\"><a href=\"#desc-nvidiakokkosc\">13</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Kokkos is a C++ model, but an official compatibility layer (Fortran Language Compatibility Layer, FLCL) is available.\"><a href=\"#desc-nvidiakokkosfortran\">14</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Alpaka supports NVIDIA GPUs by calling CUDA as part of the compilation process; also, an OpenMP backend can be used\"><a href=\"#desc-nvidiaalpakac\">15</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"Alpaka is a C++ model\"><a href=\"#desc-nvidiaalpakafortran\">16</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"12.64\" overflow=\"visible\" version=\"1.1\" width=\"12.64\"><g transform=\"translate(0,12.64) matrix(1 0 0 -1 0 0) translate(6.32,0) translate(0,6.32)\" fill=\"#FBBC6A\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 6.32 6.32 L -6.32 6.32 L -6.32 -6.32 L 6.32 -6.32 Z\" style=\"stroke:none\"></path></g></svg>\n\t\t\t\t\t<svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"There is a vast community of offloading Python code to NVIDIA GPUs, like CuPy, Numba, cuNumeric, and many others; NVIDIA actively supports a lot of them, but has no direct product like CUDA for Python; so, the status is somewhere in between\"><a href=\"#desc-nvidiapython\">17</a></sup></td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td class=\"vendor\">AMD</td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"7.92\" overflow=\"visible\" version=\"1.1\" width=\"15.85\"><g transform=\"translate(0,7.92) matrix(1 0 0 -1 0 0) translate(7.92,0) translate(0,4.75)\" fill=\"#D3C65D\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M -7.92 3.17 C -7.92 -1.21 -4.38 -4.75 0 -4.75 C 4.38 -4.75 7.92 -1.21 7.92 3.17 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"hipify by AMD can translate CUDA calls to HIP calls which runs natively on AMD GPUs\"><a href=\"#desc-amdcudac\">18</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"AMD offers a Source-to-Source translator to convert some CUDA Fortran functionality to OpenMP for AMD GPUs (gpufort); in addition, there are ROCm library bindings for Fortran in hipfort OpenACC/CUDA Fortran Source-to-Source translator\"><a href=\"#desc-amdcudafortran\">19</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"HIP is the preferred native programming model for AMD GPUs\"><a href=\"#desc-amdhipc\">20</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"No such thing like HIP for Fortran, but AMD offers Fortran interfaces to HIP and ROCm libraries in hipfort\"><a href=\"#desc-nvidiahipfortran\">4</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"7.92\" overflow=\"visible\" version=\"1.1\" width=\"15.85\"><g transform=\"translate(0,7.92) matrix(1 0 0 -1 0 0) translate(7.92,0) translate(0,4.75)\" fill=\"#D3C65D\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M -7.92 3.17 C -7.92 -1.21 -4.38 -4.75 0 -4.75 C 4.38 -4.75 7.92 -1.21 7.92 3.17 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"SYCL can use AMD GPUs, for example with hipSYCL or DPC++ for HIP AMD\"><a href=\"#desc-amdsyclc\">21</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"No such thing like SYCL for Fortran\"><a href=\"#desc-syclfortran\">6</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"OpenACC C/C++ can be used on AMD GPUs via GCC or Clacc; also, Intel's OpenACC to OpenMP Source-to-Source translator can be used to generate OpenMP directives from OpenACC directives\"><a href=\"#desc-amdopenaccc\">22</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg>\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"OpenACC Fortran can be used on AMD GPUs via GCC; also, AMD's gpufort Source-to-Source translator can move OpenACC Fortran code to OpenMP Fortran code, and also Intel's translator can work\"><a href=\"#desc-amdopenaccfortran\">23</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"AMD offers a dedicated, Clang-based compiler for using OpenMP on AMD GPUs: AOMP; it supports both C/C++ (Clang) and Fortran (Flang, example)\"><a href=\"#desc-amdopenmp\">24</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"AMD offers a dedicated, Clang-based compiler for using OpenMP on AMD GPUs: AOMP; it supports both C/C++ (Clang) and Fortran (Flang, example)\"><a href=\"#desc-amdopenmp\">24</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Intel's DPC++ (oneAPI) can be compiled with an experimental HIP AMD backend, allowing to launch STL algorithms to AMD GPUs; caveats from Intel's STL support apply\"><a href=\"#desc-amdstandardc\">25</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"Currently, no (known) way to launch Standard-based parallel algorithms on AMD GPUs\"><a href=\"#desc-amdstandardfortran\">26</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Kokkos supports AMD GPUs through HIP\"><a href=\"#desc-amdkokkosc\">27</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Kokkos is a C++ model, but an official compatibility layer (Fortran Language Compatibility Layer, FLCL) is available.\"><a href=\"#desc-nvidiakokkosfortran\">14</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Alpaka supports AMD GPUs through HIP or through an OpenMP backend\"><a href=\"#desc-amdalpakac\">28</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"Alpaka is a C++ model\"><a href=\"#desc-nvidiaalpakafortran\">16</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"AMD does not officially support GPU programming with Python (also not semi-officially like NVIDIA), but third-party support is available, for example through Numba (currently inactive) or a HIP version of CuPy\"><a href=\"#desc-amdpython\">29</a></sup></td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td class=\"vendor\">Intel</td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"7.92\" overflow=\"visible\" version=\"1.1\" width=\"15.85\"><g transform=\"translate(0,7.92) matrix(1 0 0 -1 0 0) translate(7.92,0) translate(0,4.75)\" fill=\"#D3C65D\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M -7.92 3.17 C -7.92 -1.21 -4.38 -4.75 0 -4.75 C 4.38 -4.75 7.92 -1.21 7.92 3.17 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"SYCLomatic translates CUDA code to SYCL code, allowing it to run on Intel GPUs; also, Intel's DPC++ Compatibility Tool can transform CUDA to SYCL\"><a href=\"#desc-intelcudac\">30</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"No direct support, only via ISO C bindings, but at least an example can be found on GitHub; it's pretty scarce and not by Intel itself, though\"><a href=\"#desc-intelcudafortran\">31</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"CHIP-SPV supports mapping CUDA and HIP to OpenCL and Intel's Level Zero, making it run on Intel GPUs\"><a href=\"#desc-intelhipc\">32</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"No such thing like HIP for Fortran\"><a href=\"#desc-intelhipfortran\">33</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"SYCL is the prime programming model for Intel GPUs; actually, SYCL is only a standard, while Intel's implementation of it is called DPC++ (Data Parallel C++), which extends the SYCL standard in various places; actually actually, Intel namespaces everything oneAPI these days, so the full proper name is Intel oneAPI DPC++ (which incorporates a C++ compiler and also a library)\"><a href=\"#desc-intelsyclc\">34</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"No such thing like SYCL for Fortran\"><a href=\"#desc-syclfortran\">6</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"OpenACC can be used on Intel GPUs by translating the code to OpenMP with Intel's Source-to-Source translator\"><a href=\"#desc-intelopenacc\">35</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"OpenACC can be used on Intel GPUs by translating the code to OpenMP with Intel's Source-to-Source translator\"><a href=\"#desc-intelopenacc\">35</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Intel has extensive support for OpenMP through their latest compilers\"><a href=\"#desc-intelopenmp\">36</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"11.92\" overflow=\"visible\" version=\"1.1\" width=\"11.92\"><g transform=\"translate(0,11.92) matrix(1 0 0 -1 0 0) translate(5.96,0) translate(0,5.96)\" fill=\"#85924E\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 0 M 5.96 0 C 5.96 3.29 3.29 5.96 0 5.96 C -3.29 5.96 -5.96 3.29 -5.96 0 C -5.96 -3.29 -3.29 -5.96 0 -5.96 C 3.29 -5.96 5.96 -3.29 5.96 0 Z M 0 0\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Intel has extensive support for OpenMP through their latest compilers\"><a href=\"#desc-intelopenmp\">36</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"12.64\" overflow=\"visible\" version=\"1.1\" width=\"12.64\"><g transform=\"translate(0,12.64) matrix(1 0 0 -1 0 0) translate(6.32,0) translate(0,6.32)\" fill=\"#FBBC6A\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 6.32 6.32 L -6.32 6.32 L -6.32 -6.32 L 6.32 -6.32 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Intel supports pSTL algorithms through their DPC++ Library (oneDPL; GitHub). It's heavily namespaced and not yet on the same level as NVIDIA\"><a href=\"#desc-intelstandardc\">37</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"12.64\" overflow=\"visible\" version=\"1.1\" width=\"12.64\"><g transform=\"translate(0,12.64) matrix(1 0 0 -1 0 0) translate(6.32,0) translate(0,6.32)\" fill=\"#FBBC6A\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 6.32 6.32 L -6.32 6.32 L -6.32 -6.32 L 6.32 -6.32 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"With Intel oneAPI 2022.3, Intel supports DO CONCURRENT with GPU offloading\"><a href=\"#desc-intelstandardfortran\">38</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Kokkos supports Intel GPUs through SYCL\"><a href=\"#desc-intelkokkosc\">39</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Kokkos is a C++ model, but an official compatibility layer (Fortran Language Compatibility Layer, FLCL) is available.\"><a href=\"#desc-nvidiakokkosfortran\">14</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"13.4\" overflow=\"visible\" version=\"1.1\" width=\"15.48\"><g transform=\"translate(0,13.4) matrix(1 0 0 -1 0 0) translate(7.74,0) translate(0,4.47)\" fill=\"#C7DB7F\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -7.74 -4.47 L 7.74 -4.47 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Alpaka v0.9.0 introduces experimental SYCL support; also, Alpaka can use OpenMP backends\"><a href=\"#desc-intelalpakac\">40</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"9.45\" overflow=\"visible\" version=\"1.1\" width=\"9.45\"><g transform=\"translate(0,9.45) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)\" fill=\"#000000\" stroke=\"#EB5F73\" stroke-width=\"0.8pt\" color=\"#000000\"><path d=\"M 0 0 L 8.34 8.34\" style=\"fill:none\"></path></g></svg><sup class=\"footnote\" title=\"Alpaka is a C++ model\"><a href=\"#desc-nvidiaalpakafortran\">16</a></sup></td>\n\t\t\t\t<td class=\"status\">\n\t\t\t\t\t<svg height=\"16.17\" overflow=\"visible\" version=\"1.1\" width=\"17\"><g transform=\"translate(0,16.17) matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,7.23)\" fill=\"#F38966\" stroke=\"#000000\" stroke-width=\"0.4pt\" color=\"#000000\"><path d=\"M 0 8.94 L -2.45 3.37 L -8.5 2.76 L -3.97 -1.29 L -5.25 -7.23 L 0 -4.17 L 5.25 -7.23 L 3.97 -1.29 L 8.5 2.76 L 2.45 3.37 Z\" style=\"stroke:none\"></path></g></svg><sup class=\"footnote\" title=\"Not a lot of support available at the moment, but notably DPNP, a SYCL-based drop-in replacement for Numpy, and numba-dpex, an extension of Numba for DPC++.\"><a href=\"#desc-intelpython\">41</a></sup></td>\n\t\t\t</tr>\n\t\t</tbody>\n\t</table>\n\t<ul>\n\t\t<li id=\"desc-cudac\"><span class=\"number\">1:</span> <span class=\"description\">CUDA C/C++ is supported on NVIDIA GPUs through the <a href=\"https://developer.nvidia.com/cuda-toolkit\">CUDA Toolkit</a></span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-cudafortran\"><span class=\"number\">2:</span> <span class=\"description\">CUDA Fortran, a proprietary Fortran extension, is supported on NVIDIA GPUs via the <a href=\"https://developer.nvidia.com/hpc-sdk\">NVIDIA HPC SDK</a></span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiahip\"><span class=\"number\">3:</span> <span class=\"description\"><a href=\"https://github.com/ROCm-Developer-Tools/HIP\">HIP</a> programs can directly use NVIDIA GPUs via a CUDA backend; HIP is maintained by AMD</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiahipfortran\"><span class=\"number\">4:</span> <span class=\"description\">No such thing like HIP for Fortran, but AMD offers Fortran interfaces to HIP and ROCm libraries in <a href=\"https://github.com/ROCmSoftwarePlatform/hipfort\">hipfort</a></span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiasycl\"><span class=\"number\">5:</span> <span class=\"description\">SYCL can be used on NVIDIA GPUs with <em>experimental</em> support either in <a href=\"https://github.com/codeplaysoftware/sycl-for-cuda/blob/cuda/sycl/doc/GetStartedWithSYCLCompiler.md#build-sycl-toolchain-with-support-for-nvidia-cuda\">SYCL</a> directly or in <a href=\"https://github.com/intel/llvm/blob/sycl/sycl/doc/GetStartedGuide.md#build-dpc-toolchain-with-support-for-nvidia-cuda\">DPC++</a>, or via <a href=\"https://github.com/illuhad/hipSYCL\">hipSYCL</a></span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-syclfortran\"><span class=\"number\">6:</span> <span class=\"description\">No such thing like SYCL for Fortran</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-openaccc\"><span class=\"number\">7:</span> <span class=\"description\">OpenACC C/C++ supported on NVIDIA GPUs directly (and best) through NVIDIA HPC SDK; additional, somewhat limited support by <a href=\"https://gcc.gnu.org/wiki/OpenACC\">GCC C compiler</a> and in LLVM through <a href=\"https://csmd.ornl.gov/project/clacc\">Clacc</a></span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-openaccfortran\"><span class=\"number\">8:</span> <span class=\"description\">OpenACC Fortran supported on NVIDIA GPUs directly (and best) through NVIDIA HPC SDK; additional, somewhat limited support by GCC Fortran compiler and <a href=\"https://ieeexplore.ieee.org/document/9651310\">Flacc</a></span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiaopenmpc\"><span class=\"number\">9:</span> <span class=\"description\">OpenMP in C++ supported on NVIDIA GPUs through NVIDIA HPC SDK (albeit <a href=\"https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-user-guide/index.html#openmp-use\">with a few limits</a>), by GCC, and Clang; see <a href=\"https://www.openmp.org/wp-content/uploads/2022_ECP_Community_BoF_Days-OpenMP_RoadMap_BoF.pdf\">OpenMP ECP BoF on status in 2022</a>.</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiaopenmpfortran\"><span class=\"number\">10:</span> <span class=\"description\">OpenMP in Fortran supported on NVIDIA GPUs through NVIDIA HPC SDK (but not full OpenMP feature set available), by GCC, and Flang</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiastandardc\"><span class=\"number\">11:</span> <span class=\"description\">pSTL features supported on NVIDIA GPUs through <a href=\"https://docs.nvidia.com/hpc-sdk/compilers/c++-parallel-algorithms/\">NVIDIA HPC SDK</a></span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiastandardfortran\"><span class=\"number\">12:</span> <span class=\"description\">Standard Language parallel features supported on NVIDIA GPUs through NVIDIA HPC SDK</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiakokkosc\"><span class=\"number\">13:</span> <span class=\"description\"><a href=\"https://github.com/kokkos/kokkos\">Kokkos</a> supports NVIDIA GPUs by calling CUDA as part of the compilation process</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiakokkosfortran\"><span class=\"number\">14:</span> <span class=\"description\">Kokkos is a C++ model, but an official compatibility layer (<a href=\"https://github.com/kokkos/kokkos-fortran-interop\"><em>Fortran Language Compatibility Layer</em>, FLCL</a>) is available.</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiaalpakac\"><span class=\"number\">15:</span> <span class=\"description\"><a href=\"https://github.com/alpaka-group/alpaka\">Alpaka</a> supports NVIDIA GPUs by calling CUDA as part of the compilation process; also, an OpenMP backend can be used</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiaalpakafortran\"><span class=\"number\">16:</span> <span class=\"description\">Alpaka is a C++ model</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-nvidiapython\"><span class=\"number\">17:</span> <span class=\"description\">There is a vast community of offloading Python code to NVIDIA GPUs, like <a href=\"https://cupy.dev/\">CuPy</a>, <a href=\"https://numba.pydata.org/\">Numba</a>, <a href=\"https://developer.nvidia.com/cunumeric\">cuNumeric</a>, and many others; NVIDIA actively supports a lot of them, but has no direct product like <em>CUDA for Python</em>; so, the status is somewhere in between</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdcudac\"><span class=\"number\">18:</span> <span class=\"description\"><a href=\"https://github.com/ROCm-Developer-Tools/HIPIFY\">hipify</a> by AMD can translate CUDA calls to HIP calls which runs natively on AMD GPUs</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdcudafortran\"><span class=\"number\">19:</span> <span class=\"description\">AMD offers a Source-to-Source translator to convert some CUDA Fortran functionality to OpenMP for AMD GPUs (<a href=\"https://github.com/ROCmSoftwarePlatform/gpufort\">gpufort</a>); in addition, there are ROCm library bindings for Fortran in <a href=\"https://github.com/ROCmSoftwarePlatform/hipfort\">hipfort</a> OpenACC/CUDA Fortran Source-to-Source translator</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdhipc\"><span class=\"number\">20:</span> <span class=\"description\"><a href=\"https://github.com/ROCm-Developer-Tools/HIP\">HIP</a> is the preferred native programming model for AMD GPUs</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdsyclc\"><span class=\"number\">21:</span> <span class=\"description\">SYCL can use AMD GPUs, for example with <a href=\"https://github.com/illuhad/hipSYCL\">hipSYCL</a> or <a href=\"https://github.com/intel/llvm/blob/sycl/sycl/doc/GetStartedGuide.md#build-dpc-toolchain-with-support-for-hip-amd\">DPC++ for HIP AMD</a></span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdopenaccc\"><span class=\"number\">22:</span> <span class=\"description\">OpenACC C/C++ can be used on AMD GPUs via GCC or Clacc; also, <a href=\"https://github.com/intel/intel-application-migration-tool-for-openacc-to-openmp\">Intel's OpenACC to OpenMP Source-to-Source translator</a> can be used to generate OpenMP directives from OpenACC directives</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdopenaccfortran\"><span class=\"number\">23:</span> <span class=\"description\">OpenACC Fortran can be used on AMD GPUs via GCC; also, AMD's <a href=\"https://github.com/intel/intel-application-migration-tool-for-openacc-to-openmp\">gpufort</a> Source-to-Source translator can move OpenACC Fortran code to OpenMP Fortran code, and also Intel's translator can work</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdopenmp\"><span class=\"number\">24:</span> <span class=\"description\">AMD offers a dedicated, Clang-based compiler for using OpenMP on AMD GPUs: <a href=\"https://github.com/ROCm-Developer-Tools/aomp\">AOMP</a>; it supports both C/C++ (Clang) and Fortran (Flang, <a href=\"https://github.com/ROCm-Developer-Tools/aomp/tree/aomp-dev/examples/fortran/simple_offload\">example</a>)</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdstandardc\"><span class=\"number\">25:</span> <span class=\"description\">Intel's DPC++ (oneAPI) can be <a href=\"https://intel.github.io/llvm-docs/GetStartedGuide.html#build-dpc-toolchain-with-support-for-hip-amd\">compiled with an experimental HIP AMD backend</a>, allowing to launch STL algorithms to AMD GPUs; caveats from Intel's STL support apply</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdstandardfortran\"><span class=\"number\">26:</span> <span class=\"description\">Currently, no (known) way to launch Standard-based parallel algorithms on AMD GPUs</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdkokkosc\"><span class=\"number\">27:</span> <span class=\"description\">Kokkos supports AMD GPUs through HIP</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdalpakac\"><span class=\"number\">28:</span> <span class=\"description\">Alpaka supports AMD GPUs through HIP or through an OpenMP backend</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-amdpython\"><span class=\"number\">29:</span> <span class=\"description\">AMD does not officially support GPU programming with Python (also not semi-officially like NVIDIA), but third-party support is available, for example through <a href=\"https://numba.pydata.org/numba-doc/latest/roc/index.html\">Numba</a> (currently inactive) or a <a href=\"https://docs.cupy.dev/en/latest/install.html?highlight=rocm#building-cupy-for-rocm-from-source\">HIP version of CuPy</a></span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelcudac\"><span class=\"number\">30:</span> <span class=\"description\"><a href=\"https://github.com/oneapi-src/SYCLomatic\">SYCLomatic</a> translates CUDA code to SYCL code, allowing it to run on Intel GPUs; also, Intel's <a href=\"https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compatibility-tool.html\">DPC++ Compatibility Tool</a> can transform CUDA to SYCL</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelcudafortran\"><span class=\"number\">31:</span> <span class=\"description\">No direct support, only via ISO C bindings, but at least an example can be <a href=\"https://github.com/codeplaysoftware/SYCL-For-CUDA-Examples/tree/master/examples/fortran_interface\">found on GitHub</a>; it's pretty scarce and not by Intel itself, though</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelhipc\"><span class=\"number\">32:</span> <span class=\"description\"><a href=\"https://github.com/CHIP-SPV/chip-spv\">CHIP-SPV</a> supports mapping CUDA and HIP to OpenCL and Intel's Level Zero, making it run on Intel GPUs</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelhipfortran\"><span class=\"number\">33:</span> <span class=\"description\">No such thing like HIP for Fortran</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelsyclc\"><span class=\"number\">34:</span> <span class=\"description\"><a href=\"https://www.khronos.org/sycl/\">SYCL</a> is the prime programming model for Intel GPUs; actually, SYCL is only a standard, while Intel's implementation of it is called <a href=\"https://www.intel.com/content/www/us/en/developer/tools/oneapi/data-parallel-c-plus-plus.html\">DPC++</a> (<em>Data Parallel C++</em>), which extends the SYCL standard in various places; actually actually, Intel namespaces everything <em>oneAPI</em> these days, so the <em>full</em> proper name is Intel oneAPI DPC++ (which incorporates a C++ compiler and also a library)</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelopenacc\"><span class=\"number\">35:</span> <span class=\"description\">OpenACC can be used on Intel GPUs by translating the code to OpenMP with <a href=\"https://github.com/intel/intel-application-migration-tool-for-openacc-to-openmp\">Intel's Source-to-Source translator</a></span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelopenmp\"><span class=\"number\">36:</span> <span class=\"description\">Intel has <a href=\"https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-cpp-fortran-compiler-openmp/top.html\">extensive support for OpenMP</a> through their latest compilers</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelstandardc\"><span class=\"number\">37:</span> <span class=\"description\">Intel supports pSTL algorithms through their <a href=\"https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-library.html#gs.fifrh5\">DPC++ Library</a> (oneDPL; <a href=\"https://github.com/oneapi-src/oneDPL\">GitHub</a>). It's heavily namespaced and not yet on the same level as NVIDIA</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelstandardfortran\"><span class=\"number\">38:</span> <span class=\"description\">With <a href=\"https://www.intel.com/content/www/us/en/developer/articles/release-notes/fortran-compiler-release-notes.html\">Intel oneAPI 2022.3</a>, Intel supports DO CONCURRENT with GPU offloading</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelkokkosc\"><span class=\"number\">39:</span> <span class=\"description\">Kokkos supports Intel GPUs through SYCL</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelalpakac\"><span class=\"number\">40:</span> <span class=\"description\"><a href=\"https://github.com/alpaka-group/alpaka/releases/tag/0.9.0\">Alpaka v0.9.0</a> introduces experimental SYCL support; also, Alpaka can use OpenMP backends</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t\t<li id=\"desc-intelpython\"><span class=\"number\">41:</span> <span class=\"description\">Not a lot of support available at the moment, but notably <a href=\"https://intelpython.github.io/dpnp/\">DPNP</a>, a SYCL-based drop-in replacement for Numpy, and <a href=\"https://github.com/IntelPython/numba-dpex\">numba-dpex</a>, an extension of Numba for DPC++.</span><a href=\"#compat-table\" class=\"back\" title=\"Back to table\">↺</a></li>\n\t</ul>\n</section>\n\n<hr />\n\n<h2 id=\"caveats\">Caveats</h2>\n\n<p>Although the table and its descriptions does a decent job in summarizing the state-of-the-art (I think), there are some caveats going along with it.</p>\n\n<ul>\n  <li>This is the state as of <strong>Nov 2022</strong>; things are moving along quickly and might be outdated when you read this</li>\n  <li>This is (partly) opinionated by my practical experience with things, chat me up if you a disagree with my assessment</li>\n  <li>Most importantly: It does not say anything about performance; adding performance to the mix (which is, like, a very important metric in H<strong>P</strong>C) would make this two-dimensional table three-dimensional, and would be really hard to judge</li>\n</ul>\n\n<h2 id=\"technical-background\">Technical Background</h2>\n\n<p>As the origin of the table is in slides (which I, of course, create with LaTeX), but I also want to present it here (in HTML form), I looked for a way to generate one from the other. Nothing really worked perfectly – <a href=\"https://math.nist.gov/~BMiller/LaTeXML/\">LaTeXML</a> looks great, but is still a little complicated. So, I did what any reasonable programmer would do and <del>spend way too much time to</del> script my way out of things.</p>\n\n<p>I recreated the table as a <a href=\"https://github.com/AndiH/gpu-lang-compat/blob/main/compat.yml\">machine-readable YAML file</a> which is transformed to TeX and HTML by using respective templates with Jinja. Jinja is really amazing and I’m a huge fan. All the data, all files, and all scripts are in a GitHub repository: <a href=\"https://github.com/AndiH/gpu-lang-compat\">https://github.com/AndiH/gpu-lang-compat</a>. Feel free to remix, it’s MIT!</p>\n\n<h2 id=\"changelog\">Changelog</h2>\n\n<ul>\n  <li>2022-Nov-04: <a href=\"https://github.com/AndiH/gpu-lang-compat/commit/f9745a921b4318eb1e7836190ab88ff63081e80a#diff-75b63e419ed3437d2959e3db4e9084fece46ad51d1affb68be6307d92c2c5a62\">Fixed</a> wrong icon for HIP with Fortran on NVIDIA GPUs</li>\n  <li>2022-Nov-08: <a href=\"https://github.com/AndiH/gpu-lang-compat/commit/7b49eff3f03611d4735e38c820b30fa99e625d40\">Added <code class=\"language-plaintext highlighter-rouge\">hipfort</code></a>, fixed a wrong symbol for ALPAKA, added OpenMP for ALPAKA (<a href=\"https://github.com/AndiH/gpu-lang-compat/commit/00c4b014392ebe36ee9b8d84cd606b976eeabd70\">commit for both</a>)</li>\n  <li>2022-Nov-12: <a href=\"https://github.com/AndiH/gpu-lang-compat/commit/01942ded2ff6d342484e715377f175d36c9e63e4\">Fine-tuned colors</a> of symbols; tagged <a href=\"https://github.com/AndiH/gpu-lang-compat/releases/tag/v1.2\">new v1.2 release</a></li>\n  <li>2022-Dec-06: <a href=\"https://github.com/AndiH/gpu-lang-compat/commit/fd0f56b6baa80bcac9c9a50686d687975e0ccb96\">Update Intel’s support for Standard Parallelism</a>, displayed wrongly because a bug in my scripts</li>\n  <li>2022-Dec-07: <a href=\"https://github.com/AndiH/gpu-lang-compat/commit/d52e4a2ddbef40a46bff6d2cda15a7f3cf019e53\">Update Standard Parallelism support</a> on AMD GPUs via Intel DPC++</li>\n</ul>\n\n<style type=\"text/css\">\n\tsection#compat-matrix dl {\n\t  column-count: 2;\n\t}\n\tsection#compat-matrix dl dt {\n\t  float: left;\n\t  width: 20px;\n\t  text-align: center;\n\t}\n\n\tsection#compat-matrix dl dd {\n\t  margin-bottom: 1pt;\n\t  width: ;\n\t  margin-left: 18pt;\n\t}\n\tsection#compat-matrix table {\n\t  border-collapse: collapse;\n\t}\n\n\tsection#compat-matrix tr {\n\t  border: none;\n\t}\n\n\tsection#compat-matrix td {\n\t  border-right: 1px solid lightgray;\n\t  padding: 1px;\n\t  text-align: center;\n\t}\n\tsection#compat-matrix table td:last-of-type {\n\t  border: none;\n\t}\n\n\tsection#compat-matrix table tr td.level-1, table tr td.vendor {\n\t  font-weight: bold;\n\t}\n\n\tsection#compat-matrix table tr td.level-2 {\n\t  font-weight: 600;\n\t}\n\tsection#compat-matrix table tr td.etc {\n\t  font-style: italic;\n\t}\n\tsection#compat-matrix sup.footnote {\n\t\tfont-size: 70%;\n\t}\n\tsection#compat-matrix ul li .number {\n\t  font-weight: bolder;\n\t}\n\tsection#compat-matrix ul li .back {\n\t  font-size: smaller;\n\t  color: lightgray;\n\t  bottom: 2px;\n\t  position: relative;\n\t  padding-left: 2px;\n\t}\n\tsection#compat-matrix ul li .back:hover {\n\t  color: gray;\n\t}\n</style>"},{"id":"https://x-dev.pages.jsc.fz-juelich.de//2022/10/10/maelstrom-hpc-intro","title":"Talk: Introduction to HPC","link":"https://x-dev.pages.jsc.fz-juelich.de//2022/10/10/maelstrom-hpc-intro","published":"2022-10-10T08:22:18+00:00","description":"TL;DR: I held a HPC intro talk. Slides are below. In MAELSTROM, we connect three areas of science: 🌍Weather and climate simulation with 🤖Machine Learning methods and workflows using 📈HPC techniques and...","isPermalink":false,"tags":["HPC","MAELSTROM"],"authors":[{"name":"Andreas","url":null}],"image":null,"modified":"2022-10-10T08:22:18+00:00","contentHtml":"<p><em>TL;DR: I held a HPC intro talk. Slides are <a href=\"#slides\">below</a>.</em></p>\n\n<p>In MAELSTROM, we connect three areas of science: <em>🌍Weather and climate simulation</em> with <em>🤖Machine Learning methods and workflows</em> using <em>📈HPC techniques and resources</em>. <a class=\"lightbox\" href=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/hpc-hook.png\"><img alt=\"Green slide with: High Performance Computing is computing with a powerful machine using the available resources efficiently\" title=\"What is HPC?\" src=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/hpc-hook.tn.png\" srcset=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/hpc-hook.tn.png 1x, /assets/slides/posts/2022-10-10-maelstrom-hpc-intro/hpc-hook.tn.2x.png 2x, /assets/slides/posts/2022-10-10-maelstrom-hpc-intro/hpc-hook.tn.4x.png 4x\" /></a>Halfway into the project, we held a boot camp at JSC to teach this Venn diagram to a group of students a few days ago. Some were ML experts, but had never used a HPC system. Others came from climate science, but had never applied ML methods to the problem. Using the applications of MEALSTROM as examples, participants of the boot camp could hands-on learn about all these cool things - <em>at once</em>. In addition, to give participants some context, lectures were held to introduce weather and climate simulations, ML methods (especially focusing on large scales), and HPC. Guess what I presented? Right! HPC!</p>\n\n<p>As I’ve never had the opportunity to introduce the <em>general</em> field of HPC (I’m usually doing just the GPU stuff), I needed to create a presentation from scratch. It was quite some work, but I’m really happy with the result. There is much more to teach about HPC, but one can only do so much in 60 minutes.</p>\n\n<p>As a hook, I tried using a definition of HPC I came up with: <strong>High Performance Computing is computing with a powerful machine using the available resources efficiently</strong>. It might be a little contrived for this talk at hand, but I wanted to focus both on the powerful machines themselves <u>and</u> using them efficiently. The latter part is sometimes forgotten, but ever so important, especially in times of sky-rocketing energy prizes. <a class=\"lightbox\" href=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/software-pyramid.png\"><img alt=\"A pyramid list of things one needs to do for good performance\" title=\"The Pyramid of Resource Utilization\" src=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/software-pyramid.tn.png\" srcset=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/software-pyramid.tn.png 1x, /assets/slides/posts/2022-10-10-maelstrom-hpc-intro/software-pyramid.tn.2x.png 2x, /assets/slides/posts/2022-10-10-maelstrom-hpc-intro/software-pyramid.tn.4x.png 4x\" /></a>The slides start by first comparing personal computers with HPC computers, getting interactive feedback from the audience on the way and assessing their experience with HPC. Then, I focus on a few historical important supercomputers, making the way to our JSC machines and finally to Frontier. The latter I use as an example to explain a little about GPUs. To focus on the software-side of things (<em>using resources efficiently</em>), I came up with a weird, inverted <em>pyramid of resource utilization</em>: 1) exploit all capabilities of a processing entity, 2) parallelize, 3) distribute. For each point, the slides show an example on how to achieve it and important technologies involved.</p>\n\n<p><a class=\"lightbox\" href=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/hpc-nodes.png\"><img width=\"180px\" alt=\"A pyramid list of things one needs to do for good performance\" title=\"The Pyramid of Resource Utilization\" src=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/hpc-nodes.tn.png\" srcset=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/hpc-nodes.tn.png 1x, /assets/slides/posts/2022-10-10-maelstrom-hpc-intro/hpc-nodes.png 2x, /assets/slides/posts/2022-10-10-maelstrom-hpc-intro/hpc-nodes.png 4x\" /></a>Just as usual, I made the slides with LaTeX Beamer; which I particularly enjoy when I’m able to use <code class=\"language-plaintext highlighter-rouge\">\\foreach</code> to create little boxes and repeating graphics – and there are plenty of these ones in this deck. TikZ is an amazing package which I use more and more of<sup id=\"fnref:tikz\" role=\"doc-noteref\"><a href=\"#fn:tikz\" class=\"footnote\" rel=\"footnote\">1</a></sup>, to the detriment typesetting durations… <code class=\"language-plaintext highlighter-rouge\">overlay, remember picture</code> is basically in my muscle memory by now. For a first time, I also used <code class=\"language-plaintext highlighter-rouge\">tikzexternalize</code> to <em>save</em> the diagram of a HPC node to a file and re-use it afterwards; LaTeX wouldn’t want to generate it 96 times (<em>boooh</em>), so I inserted a hidden slide before, generated the image with <code class=\"language-plaintext highlighter-rouge\">tikzexternalize</code> there, and then re-used it with an <code class=\"language-plaintext highlighter-rouge\">\\includegraphics</code> 96 times – with <code class=\"language-plaintext highlighter-rouge\">\\foreach</code>, of course.</p>\n\n<p>Find the <a href=\"/assets/slides/posts/2022-10-07-maelstrom-hpc-intro/aherten-maelstrom-hpc--minified.pdf\">slides</a> embedded below<sup id=\"fnref:minified\" role=\"doc-noteref\"><a href=\"#fn:minified\" class=\"footnote\" rel=\"footnote\">2</a></sup> and in <a href=\"http://hdl.handle.net/2128/32001\">referable form as hdl.handle.net/2128/32001 at our library</a>.</p>\n\n<iframe id=\"slides\" width=\"740\" height=\"440\" src=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/aherten-maelstrom-hpc--minified.pdf\">Your browser is unable to display the PDF. Instead, please download it.</iframe>\n<p><a href=\"/assets/slides/posts/2022-10-10-maelstrom-hpc-intro/aherten-maelstrom-hpc--minified.pdf\">Download slides</a>.</p>\n\n<link rel=\"stylesheet\" href=\"/assets/js/lightbox/simpleLightbox.min.css\" />\n\n<script src=\"/assets/js/lightbox/simpleLightbox.min.js\"></script>\n\n<script type=\"text/javascript\">\n\tnew SimpleLightbox({elements: 'a.lightbox'});\n</script>\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:tikz\" role=\"doc-endnote\">\n      <p>It makes placing things free-floating on a slide so much easier. <a href=\"#fnref:tikz\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:minified\" role=\"doc-endnote\">\n      <p>This is actually a minified version of the slides, using low-res versions of the images; add this to your shell <code class=\"language-plaintext highlighter-rouge\">function minify-pdf () {in=\"$1\"; out=\"${1%.*}\"; gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/prepress -dNOPAUSE -dQUIET -dBATCH -sOutputFile=\"$out--minified.pdf\" $in;}</code>! <a href=\"#fnref:minified\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>"},{"id":"https://x-dev.pages.jsc.fz-juelich.de//2022/10/06/poster-jlesc-opengptx","title":"Poster: OpenGPT-X - Training Large Language Models on HPC Systems","link":"https://x-dev.pages.jsc.fz-juelich.de//2022/10/06/poster-jlesc-opengptx","published":"2022-10-06T09:32:00+00:00","description":".pub {  line-height: 40px;  height: 40px;  background-color: #e2e2ec;  border-left: 6px solid #8b8bb6;  margin-bottom: 20px;  padding: 4px 12px; }   Poster publication: ...","isPermalink":false,"tags":["Workshop","OpenGPTX"],"authors":[{"name":"Carolin","url":null}],"image":null,"modified":"2022-10-06T09:32:00+00:00","contentHtml":"<style>\n\n.pub {\n  line-height: 40px;\n  height: 40px;\n  background-color: #e2e2ec;\n  border-left: 6px solid #8b8bb6;\n  margin-bottom: 20px;\n  padding: 4px 12px;\n}\n\n\n</style>\n\n<div class=\"pub\">\n  <p><strong><i class=\"fa-sharp fa-solid fa-file-image\"></i> Poster publication:</strong>  <a href=\"http://hdl.handle.net/2128/32006\">http://hdl.handle.net/2128/32006</a>  </p>\n</div>\n\n<p>The <a href=\"https://publish.illinois.edu/14th-jlesc-workshop/\">14th JLESC workshop</a> (JLESC: Joint Laboratory for Extreme-Scale Computing) was hosted by the National Center for Supercomputing Applications (NCSA) in Urbana, Illinois from 28th September to 30th September.</p>\n\n<p><a href=\"/assets/slides/posts/2022-10-06-poster-jlesc-opengptx/opengptx-poster.pdf\"><img alt=\"JLESC 2022 Poster about OpenGPT-X by Carolin Penke\" style=\"width: 100px;\" title=\"OpenGPT-X Poster\" src=\"/assets/slides/posts/2022-10-06-poster-jlesc-opengptx/opengptx-poster.tn.png\" srcset=\"/assets/slides/posts/2022-10-06-poster-jlesc-opengptx/opengptx-poster.tn.png 1x, /assets/slides/posts/2022-10-06-poster-jlesc-opengptx/opengptx-poster.tn.2x.png 2x, /assets/slides/posts/2022-10-06-poster-jlesc-opengptx/opengptx-poster.tn.4x.png 4x\" /></a>We had the opportunity to present the <a href=\"https://opengpt-x.de/en/\">OpenGPT-X</a> project in form of a poster.</p>\n\n<p>On it, you can find information about the project partners within OpenGPT-X, its goals and the use cases of large language models it explores. Various available language models are presented.</p>\n\n<p>Recent breakthroughs became possible due to the novel neural network architecture called <a href=\"/2022/07/13/transformers-matmul.html\">transformer</a>, based on so-called self-attention layers. These allow for the parallel processing of input using highly efficient matrix products.</p>\n\n<p>In order to scale to a full supercomputer, three dimensions of parallelism are intertwined: Data parallelism, pipeline parallelism, and tensor parallelism. The total number of used GPU devices is given by multiplying these three parallel degrees.</p>\n\n<p>Novel AI architectures explored at the Jülich Supercomputing Centre include AMD Instinct MI250 GPUs and Graphcore IPUs.</p>\n\n<p>Using the Megatron-Deepspeed training framework one can easily achieve about 50% of peak performance on Nvidia A100 GPUs. In our tests, including 32 GPUs on 8 nodes of JUWELS Booster, the highest throughput (in terms of TFLOP/s) is achieved when the focus is given to data parallelism, and pipeline parallelism is used to reduce the memory footprint of the model.</p>\n\n<p>Project challenges include hardware related spurious errors and energy consumption.</p>\n\n<p>Our runs in OpenGPT-X are made possible through compute time on JUWELS Booster, given by the Gauss Centre for Supercomputing e.V. (<a href=\"http://www.gauss-centre.eu\">www.gauss-centre.eu</a>) through the John von Neumann Institute for Computing at JSC.</p>\n\n<iframe width=\"720\" height=\"1170\" src=\"/assets/slides/posts/2022-10-06-poster-jlesc-opengptx/opengptx-poster.pdf\">Your browser is unable to display the PDF. Instead, please download it.</iframe>"},{"id":"https://x-dev.pages.jsc.fz-juelich.de//2022/10/05/doi-jekyll","title":"DOIng it Right! (DOIs for This Blog)","link":"https://x-dev.pages.jsc.fz-juelich.de//2022/10/05/doi-jekyll","published":"2022-10-05T14:35:47+00:00","description":"1This blog is an experiment. We want to share bits and pieces of our work; the reports we write, the presentations we hold, or the little discoveries we make, or even some first, water-testing investigations;...","isPermalink":false,"tags":["X-Dev","DOI","Open-Science"],"authors":[{"name":"Andreas","url":null}],"image":null,"modified":"2022-10-05T14:35:47+00:00","contentHtml":"<p><sup id=\"fnref:doisrus\" role=\"doc-noteref\"><a href=\"#fn:doisrus\" class=\"footnote\" rel=\"footnote\">1</a></sup>This blog is an experiment. We want to share bits and pieces of our work; the reports we write, the presentations we hold, or the little discoveries we make, or even some first, water-testing investigations; and all the rest. It’s a <em>documentation</em> of what we do. Little bits of science, collected in the open, and sometimes even not that little.</p>\n\n<p><a class=\"lightbox\" href=\"/assets/img/posts/2022-10-05-doi-jekyll/xdevblog-doi-example.png\"><img alt=\"Screenshot showing the last blog post with a DOI in the top right corner.\" title=\"Example of DOI integration in X-Dev Blog.\" src=\"/assets/img/posts/2022-10-05-doi-jekyll/xdevblog-doi-example.tn.png\" srcset=\"/assets/img/posts/2022-10-05-doi-jekyll/xdevblog-doi-example.tn.png 1x, /assets/img/posts/2022-10-05-doi-jekyll/xdevblog-doi-example.tn.2x.png 2x, /assets/img/posts/2022-10-05-doi-jekyll/xdevblog-doi-example.tn.4x.png 4x\" /></a>In the spirit of Open Science, wouldn’t it be great to acknowledge these <em>little bits of science</em> blog posts and have the option to refer to them in a scientifically sound way? Like… with DOIs, Digital Object Identifiers; the gold standard for <em>referring to scientific work</em>? Well, 🥁, <strong>the posts in this blog have DOIs now</strong>, including their metadata stored in a metadata repository!</p>\n\n<p><a class=\"lightbox\" href=\"/assets/img/posts/2022-10-05-doi-jekyll/datacite-commons-search.png\"><img alt=\"Screenshot of DataCite Commons.\" title=\"The parent entry of the blog at DataCite Commons.\" src=\"/assets/img/posts/2022-10-05-doi-jekyll/datacite-commons-search.tn.png\" srcset=\"/assets/img/posts/2022-10-05-doi-jekyll/datacite-commons-search.tn.png 1x, /assets/img/posts/2022-10-05-doi-jekyll/datacite-commons-search.tn.2x.png 2x, /assets/img/posts/2022-10-05-doi-jekyll/datacite-commons-search.tn.4x.png 4x\" /></a>Thanks to help from our Forschunsgzentrum Library, we are able to use <a href=\"https://datacite.org/\">DataCite</a> as DOI provider. I built a little Python tool which uses the DataCite API to register metadata of a blog post and <em>mint</em> a DOI. The DOI is shown in the header of each post, next to the license of the post (which is also new); the first part of the suffix of the DOI always containing <code class=\"language-plaintext highlighter-rouge\">xdvblg</code>. I have released the Python tool as Open Source software as well, with a <a href=\"https://zenodo.org/record/7140228\">Zenodo DOI attached</a>. It should be suitable for any other Jekyll-based blog as well!</p>\n\n<p>I created DOIs retroactively for all previous blog posts, allowing us to link and refer to them a little bit <em>more properly</em> in scientific contexts<sup id=\"fnref:shortlink\" role=\"doc-noteref\"><a href=\"#fn:shortlink\" class=\"footnote\" rel=\"footnote\">2</a></sup> from now on and have the metadata discoverable. Let’s see, if it sticks!</p>\n\n<p>Read on for some technical details and design decisions.</p>\n\n<p><a href=\"https://datacite.org/\">DataCite</a> is a service to store metadata of publications and create an optional DOI for it. Metadata can be viewed through <a href=\"https://search.datacite.org/\">their website</a> or queried via APIs.</p>\n\n<p>The Python tool, which I call <code class=\"language-plaintext highlighter-rouge\">doi-jekyll</code>, <a href=\"https://github.com/FZJ-JSC/doi-jekyll\">is hosted on GitHub</a> and released with an MIT license. Snapshots at <a href=\"https://zenodo.org/record/7140228\">Zenodo and according DOIs</a> are automatically created for every GitHub release via the <a href=\"https://github.com/FZJ-JSC/doi-jekyll/blob/main/.zenodo.json\">Zenodo GitHub integration</a>. With this blog post I <a href=\"https://github.com/FZJ-JSC/doi-jekyll/releases/tag/v1.0.0\">released v1.0</a>!</p>\n\n<p><a class=\"lightbox\" href=\"/assets/img/posts/2022-10-05-doi-jekyll/github-doi-jekyll.png\"><img alt=\"Screenshot of the doi-jekyll tool at GitHub.\" title=\"doi-jekyll is hosted on GitHub.\" src=\"/assets/img/posts/2022-10-05-doi-jekyll/github-doi-jekyll.tn.png\" srcset=\"/assets/img/posts/2022-10-05-doi-jekyll/github-doi-jekyll.tn.png 1x, /assets/img/posts/2022-10-05-doi-jekyll/github-doi-jekyll.tn.2x.png 2x, /assets/img/posts/2022-10-05-doi-jekyll/github-doi-jekyll.tn.4x.png 4x\" /></a><code class=\"language-plaintext highlighter-rouge\">doi-jekyll</code> is a command line application which can be installed via <code class=\"language-plaintext highlighter-rouge\">pip</code><sup id=\"fnref:pypi\" role=\"doc-noteref\"><a href=\"#fn:pypi\" class=\"footnote\" rel=\"footnote\">3</a></sup>. It parses metadata of different locations within a Jekyll blog tree structure, assembles them as a validating instance according to the <a href=\"https://schema.datacite.org/\">DataCite Metadata Scheme</a>, submits the metadata to DataCite, and registers an auto-generated DOI. To build the metadata, data from an individual blog post (like title, license, abstract), from an author file (like name, ORCID ID), and from the blog itself (like blog title, blog DOI, but also API endpoint) are collected. The blog DOI<sup id=\"fnref:blogdoi\" role=\"doc-noteref\"><a href=\"#fn:blogdoi\" class=\"footnote\" rel=\"footnote\">4</a></sup> is given as a <em>Collection</em> metadata of which every blog post inherits, creating a relationship between blog posts and the blog itself. The API with the latest Schema version is only available via an XML API (the JSON API is stuck on an older version, which doesn’t support the cool relational info). Because of this, the metadata is assembled in <code class=\"language-plaintext highlighter-rouge\">doi-jekyll</code> as a Python dictionary and then internally converted to XML via <a href=\"https://github.com/martinblech/xmltodict\"><code class=\"language-plaintext highlighter-rouge\">xmltodict</code></a>; a little bit of an extra effort, but working with Python dictionaries is so much easier compared to XML<sup id=\"fnref:xml\" role=\"doc-noteref\"><a href=\"#fn:xml\" class=\"footnote\" rel=\"footnote\">5</a></sup>.</p>\n\n<p><a class=\"lightbox\" href=\"/assets/img/posts/2022-10-05-doi-jekyll/datacite-json-metadata-xdvblg-mn.png\"><img alt=\"Screenshot of JSON data at DataCite\" title=\"JSON Metadata of the X-Dev Blog per DataCite.\" src=\"/assets/img/posts/2022-10-05-doi-jekyll/datacite-json-metadata-xdvblg-mn.tn.png\" srcset=\"/assets/img/posts/2022-10-05-doi-jekyll/datacite-json-metadata-xdvblg-mn.tn.png 1x, /assets/img/posts/2022-10-05-doi-jekyll/datacite-json-metadata-xdvblg-mn.tn.2x.png 2x, /assets/img/posts/2022-10-05-doi-jekyll/datacite-json-metadata-xdvblg-mn.tn.4x.png 4x\" /></a>It took me a little bit of trial and error to assemble a validating metadata package which conforms to the DataCite Metadata Schema; luckily, DataCite has a test instance (called <a href=\"https://doi.test.datacite.org/\"><em>Fabrica Test</em></a>) to fiddle around. While the interface of <code class=\"language-plaintext highlighter-rouge\">doi-jekyll</code> is made for this blog, it <em>should</em> work for any Jekyll blog; it has plenty of command line (and other) options to configure usage. A few examples: For testing, <code class=\"language-plaintext highlighter-rouge\">--dry-run</code> allows to skip communication with DataCite, but do all the rest; and <code class=\"language-plaintext highlighter-rouge\">--skip-url</code> will just register metadata at DataCite, but not mint a DOI. With <code class=\"language-plaintext highlighter-rouge\">--additional-metadata</code>, further metadata can be specified to integrate into the to-be-uploaded metadata; an according key <code class=\"language-plaintext highlighter-rouge\">doi-additional-metadata</code> in the YAML front matter of the post is available. To document and show some examples, the GitHub repository features an <a href=\"https://github.com/FZJ-JSC/doi-jekyll/tree/main/examples\">example Jekyll blog</a> which has an example of all necessary files.</p>\n\n<p>Let’s see where this weird journey of <em>sciencifying</em> blog posts leads us. The first person to place a <code class=\"language-plaintext highlighter-rouge\">xdvblg</code> DOI reference in a paper gets a cupcake!</p>\n\n<link rel=\"stylesheet\" href=\"/assets/js/lightbox/simpleLightbox.min.css\" />\n\n<script src=\"/assets/js/lightbox/simpleLightbox.min.js\"></script>\n\n<script type=\"text/javascript\">\n\tnew SimpleLightbox({elements: 'a.lightbox'});\n</script>\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:doisrus\" role=\"doc-endnote\">\n      <p>Alternative title of the post: DOIs “R” Us. It did not make the cut. <a href=\"#fnref:doisrus\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:shortlink\" role=\"doc-endnote\">\n      <p>Or as fancy shortlinks with attached metadata! <a href=\"#fnref:shortlink\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:pypi\" role=\"doc-endnote\">\n      <p>Not from the Python Package Index, yet; one needs to use the GitHub URL directly. <a href=\"#fnref:pypi\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:blogdoi\" role=\"doc-endnote\">\n      <p>The <em>parent</em> DOI of the blog itself is <a href=\"https://doi.org/10.34732/xdvblg-mn\">https://doi.org/10.34732/xdvblg-mn</a>, <code class=\"language-plaintext highlighter-rouge\">mn</code> like <code class=\"language-plaintext highlighter-rouge\">main</code>, you know? <a href=\"#fnref:blogdoi\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:xml\" role=\"doc-endnote\">\n      <p>Sigh, XML, amirite? <a href=\"#fnref:xml\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>"},{"id":"https://x-dev.pages.jsc.fz-juelich.de//2022/08/01/mi250-first-performances","title":"First Benchmarks with AMD Instinct MI250 GPUs at JSC","link":"https://x-dev.pages.jsc.fz-juelich.de//2022/08/01/mi250-first-performances","published":"2022-08-01T13:05:02+00:00","description":"A few months ago, we extended the JURECA Evaluation Platform1 at JSC by two nodes with AMD Instinct MI250 GPUs (four GPUs each). The nodes are Gigabyte G262-ZO0 servers, each with a dual socket AMD EPYC 7443...","isPermalink":false,"tags":["X-Dev","AMD"],"authors":[{"name":"Andreas","url":null}],"image":null,"modified":"2022-08-01T13:05:02+00:00","contentHtml":"<p>A few months ago, we extended the <a href=\"https://apps.fz-juelich.de/jsc/hps/jureca/evaluation-platform-overview.html#mi200-nodes\">JURECA Evaluation Platform</a><sup id=\"fnref:eval-platform\" role=\"doc-noteref\"><a href=\"#fn:eval-platform\" class=\"footnote\" rel=\"footnote\">1</a></sup> at JSC by two nodes with AMD Instinct MI250 GPUs (four GPUs each). The nodes are <a href=\"https://www.gigabyte.com/Enterprise/GPU-Server/G262-ZO0-rev-A00\">Gigabyte G262-ZO0</a> servers, each with a dual socket AMD EPYC 7443 processor (24 cores per socket, SMT-2) and with four MI250 GPUs (128 GB memory).</p>\n\n<ol id=\"markdown-toc\">\n  <li><a href=\"#osu-bandwidth-micro-benchmark\" id=\"markdown-toc-osu-bandwidth-micro-benchmark\">OSU Bandwidth Micro-Benchmark</a>    <ol>\n      <li><a href=\"#a100-comparison\" id=\"markdown-toc-a100-comparison\">A100 Comparison</a></li>\n    </ol>\n  </li>\n  <li><a href=\"#gpu-stream-variant\" id=\"markdown-toc-gpu-stream-variant\">GPU STREAM Variant</a>    <ol>\n      <li><a href=\"#data-size-scan\" id=\"markdown-toc-data-size-scan\">Data Size Scan</a></li>\n      <li><a href=\"#threads-and-data-sizes\" id=\"markdown-toc-threads-and-data-sizes\">Threads and Data Sizes</a></li>\n    </ol>\n  </li>\n  <li><a href=\"#conclusion\" id=\"markdown-toc-conclusion\">Conclusion</a></li>\n  <li><a href=\"#technical-details\" id=\"markdown-toc-technical-details\">Technical Details</a>    <ol>\n      <li><a href=\"#osu-microbenchmarks\" id=\"markdown-toc-osu-microbenchmarks\">OSU Microbenchmarks</a></li>\n      <li><a href=\"#stream-variant\" id=\"markdown-toc-stream-variant\">STREAM Variant</a></li>\n      <li><a href=\"#evaluation-notebooks\" id=\"markdown-toc-evaluation-notebooks\">Evaluation Notebooks</a></li>\n      <li><a href=\"#post-changelog\" id=\"markdown-toc-post-changelog\">Post Changelog</a></li>\n    </ol>\n  </li>\n</ol>\n\n<p>We’ve deployed the nodes somewhat silently in the spring and are polishing them and getting to know them ever since. Starting off with a pre-GA software stack, by now we run the publicly available ROCm 5.2. There are still some minor issues with the nodes, but the GPUs themselves are running reasonably well to finally show some very basic <strong>benchmarks</strong>!</p>\n\n<p>Still on pre-GA software, we also held an <em>AMD Porting Workshop</em>, in which we worked together with application developers and AMD to enable first users for the system. Despite the unfinished, preliminary software environment, we could achieve some interesting results. Check them out on <a href=\"https://indico-jsc.fz-juelich.de/event/282/other-view?view=standard\">the workshop’s Indico</a>!</p>\n\n<p>But now, let’s understand the devices better by looking at the <a href=\"#osu-bandwidth-micro-benchmark\">OSU bandwidth micro-benchmark</a> and a <a href=\"#gpu-stream-variant\">GPU variant of the STREAM benchmark</a>. Plenty of graphs follow, click on them to enlarge. Find some technical details at the end.</p>\n\n<h1 id=\"osu-bandwidth-micro-benchmark\">OSU Bandwidth Micro-Benchmark</h1>\n\n<p>First off, the one-directional bandwidth micro-benchmark from the <a href=\"https://mvapich.cse.ohio-state.edu/benchmarks/\">OSU microbenchmark suite</a>, <code class=\"language-plaintext highlighter-rouge\">osu_bw</code>. It is usually used for testing MPI connections, but can also be <em>ab</em>used to get a glimpse of inter-device bandwidths. See a dedicated section at the end for technical details.</p>\n\n<div class=\"absrel\" id=\"osubw-amd\">\n<a href=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_mi250.svg\"><img style=\"float: initial\" alt=\"Multiple measurement of osu_bw Microbenchmark on AMD MI250 GCDs\" title=\"Multiple measurement of osu_bw Microbenchmark on AMD MI250 GCDs.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_mi250.tn.svg\" /></a>\n<a href=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_mi250-rel.svg\"><img style=\"float: initial\" class=\"hidden\" alt=\"Multiple measurement of osu_bw Microbenchmark on AMD MI250 GCDs\" title=\"Multiple measurement of osu_bw Microbenchmark on AMD MI250 GCDs.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_mi250-rel.tn.svg\" /></a>\n<label class=\"form-switch\" title=\"Click to toggle between relative and absolute numbers in cells.\">\n  <input type=\"checkbox\" onclick=\"toggleAbsRel(this)\" />\n  <i></i>\n  Relative\n</label>\n</div>\n\n<p>The picture shows bandwidth data for two message sizes, large (64 MiB, left) and small (4 MiB, right). Each color-coded box contains the bandwidth of a message going from GPU with certain ID to another GPU with a certain ID. Also included are messages going from the GPU to itself – for example from GPU 0 to GPU 0<sup id=\"fnref:nostream\" role=\"doc-noteref\"><a href=\"#fn:nostream\" class=\"footnote\" rel=\"footnote\">2</a></sup>.</p>\n\n<p>One immediately sees that there are not four GPU IDs but eight. That is a feature of the MI250 GPUs: Each MI250 is built as a multi-chip module (MCM) with two GPU dies contained in each MI250 device <em>package</em>. Each GPU die is very similar to an AMD Instinct MI100 GPU and it has access to half (64 GB) of the total memory. From a software perspective, each MI250 GPU is actually displayed as two GPUs and needs to be used as such. For most practical purposes, it is much simpler to think of the system with four MI250 GPUs as a system of eight MI250 GPU<em>lets</em>. The proper name for <em>GPUlet</em> is GPU Complex Die (GCD), which is displayed in the picture.</p>\n\n<p>Even on a birds-eye view one can immediately see the clusters of two GCDs which belong together and form a GPU; like GPU 0 and 1, displayed in a blue 2-by-2 box, and GPU 2 and 3, etc., all on the main diagonal. The reason: GCDs on one GPU are connected well to each other with many links and have great bandwidths; for the large message size usually around 155 GiB/s.</p>\n\n<p>Implicitly, the clusters tell us even more about the inter-GPU connections: There are not only blue 2-by-2 boxes, but also green and yellow boxes. Focusing on the first row with bandwidths from GCD 0 to other GCDs, one can see that to GCD 2+3 and GCD 6+7 the bandwidths are each around 40 GiB/s, and to GCD 4+5 the bandwidths are around 80 GiB/s.</p>\n\n<figure class=\"float\"><a href=\"/assets/img/posts/2022-08-01-mi250-perf/amd-mi250-xgmi-links.svg\"><img alt=\"Block diagram of AMD MI250 Node Topolgy for Mainstream HPC Installations.\" title=\"Infinity Fabric Links in a AMD MI250 Node. Shared by AMD.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/amd-mi250-xgmi-links.tn.svg\" /></a><footer><small>Diagram shared by AMD.</small></footer></figure>\n<p>The entire structure is the result of the complex connection topology of the GPUs. Each GCD has eight Infinity Fabric <em>ports</em>, with each Infinity Fabric link having a peak bandwidth of 50 GB/s<sup id=\"fnref:xgmi\" role=\"doc-noteref\"><a href=\"#fn:xgmi\" class=\"footnote\" rel=\"footnote\">3</a></sup> in one direction. On a GPU, the two GCDs are connected with four Infinity Links, amounting to a peak bandwidth of 200 GB/s (or 400 GB/s, if you add up both directions). Going out of the MCM, things are a bit more convoluted. There are GCDs which are connected to other GCDs with two direct links (like GCD 1 → GCD 4) and GCDs connected to other GCDs with one direct link (like GCD 0 → GCD 2). Through their respective partner GCD, there might be other indirect links. And in addition, there are Infinity Fabric links going to the PCIe switch and then to the network or CPU. If you look closely, you can also see the indirect connections in the bandwidth pattern of the picture (like GCD 0 → GCD 4 being slightly faster than GCD 0 → GCD 5, although 4 and 5 are part of the same package).</p>\n\n<p>All in all, it’s a hell of a complex pattern and I’m curious about the load imbalances of future Multi-GCD applications…</p>\n\n<p>Now that we know how the patterns come to be, we can look at bandwidth usage relative to the various peaks. Enable relative numbers by clicking on the “Relative” toggle below the <a href=\"#osubw-amd\">picture up top</a>. We can see that there’s good utilization around 90% for the direct connections, and 80% for the indirect connections. For the smaller message size it’s somewhat similar compared to the larger message size, albeit 20 percentage points (pp) lower for the direct connections (indirect: 10 pp).</p>\n\n<h2 id=\"a100-comparison\">A100 Comparison</h2>\n\n<p>I also ran the micro-benchmark in the same fashion on <a href=\"https://apps.fz-juelich.de/jsc/hps/jureca/configuration.html#hardware-configuration-of-the-system-name-dc-module-phase-2-as-of-may-2021\">a usual GPU node of JURECA DC</a> with four NVIDIA A100s.</p>\n\n<div class=\"absrel\" id=\"osubw-nvidia\">\n<a href=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_nvidia_a100.svg\"><img style=\"float: initial\" alt=\"Multiple measurement of osu_bw Microbenchmark on NVIDIA A100 GPUs.\" title=\"Multiple measurement of osu_bw Microbenchmark on NVIDIA A100 GPUs.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_nvidia_a100.tn.svg\" /></a>\n<a href=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_nvidia_a100-rel.svg\"><img style=\"float: initial\" class=\"hidden\" alt=\"Multiple measurement of osu_bw Microbenchmark on NVIDIA A100 GPUs.\" title=\"Multiple measurement of osu_bw Microbenchmark on NVIDIA A100 GPUs.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_nvidia_a100-rel.tn.svg\" /></a>\n<label class=\"form-switch\" title=\"Click to toggle between relative and absolute numbers in cells.\">\n  <input type=\"checkbox\" onclick=\"toggleAbsRel(this)\" />\n  <i></i>\n  Relative\n</label>\n</div>\n\n<p>The first thing to notice is the uniformity of the connections. In the node design we deploy on JURECA DC, there are always four NVLink 3 connections between each GPU – 87 GiB/s for all possible connections (for large message sizes). Using the memory on the same GPU, 592 GiB/s are reached; roughly 130 GiB/s more than on an MI250 GCD. In terms of relative performance – which can be viewed when flipping the switch below the picture – the links to other GPUs can be utilized by 93%, or 41% for the own-memory accesses.</p>\n\n<p>Time will tell if there is more software tuning room available for the MI250s or if the difference is part of the architectural choices. Noteworthy: One MI250 (i.e. two GCDs) has a TDP of 560 W, while one A100 has 400 W.</p>\n\n<details>\n\t<summary>Expand here to display pictures to compare MI250 to A100 next to each other.</summary>\n\t<div class=\"absrel\" id=\"osubw-amdnvidia-large\">\n\t<a href=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_vs_a100-large-abs.svg\"><img style=\"float: initial\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on MI250 GCD\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on MI250 GCD.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_vs_a100-large-abs.tn.svg\" /></a>\n\t<a href=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_vs_a100-large-rel.svg\"><img style=\"float: initial\" class=\"hidden\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_vs_a100-large-rel.tn.svg\" /></a>\n\t<label class=\"form-switch\" title=\"Click to toggle between relative and absolute numbers in cells.\">\n\t  <input type=\"checkbox\" onclick=\"toggleAbsRel(this)\" />\n\t  <i></i>\n\t  Relative\n\t</label>\n\t</div>\n\t<div class=\"absrel\" id=\"osubw-amdnvidia-small\">\n\t<a href=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_vs_a100-small-abs.svg\"><img style=\"float: initial\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_vs_a100-small-abs.tn.svg\" /></a>\n\t<a href=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_vs_a100-small-rel.svg\"><img style=\"float: initial\" class=\"hidden\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU. Relative values.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/osu_bw_amd_vs_a100-small-rel.tn.svg\" /></a>\n\t<label class=\"form-switch\" title=\"Click to toggle between relative and absolute numbers in cells.\">\n\t  <input type=\"checkbox\" onclick=\"toggleAbsRel(this)\" />\n\t  <i></i>\n\t  Relative\n\t</label>\n\t</div>\n</details>\n\n<h1 id=\"gpu-stream-variant\">GPU STREAM Variant</h1>\n\n<p>Another simple benchmark to test certain aspects about a device’s memory is the STREAM benchmark, of which I ran <a href=\"https://github.com/AndiH/CUDA-Cpp-STREAM\">my own GPU variant</a> on the MI250s. I used an old CUDA code which I <em>HIPified</em> <a href=\"https://github.com/ROCm-Developer-Tools/HIPIFY\">with the <code class=\"language-plaintext highlighter-rouge\">hipify-perl</code> tool</a>; it ran without a single further change. Quite amazing.</p>\n\n<h2 id=\"data-size-scan\">Data Size Scan</h2>\n\n<div class=\"amdnvidia\" id=\"stream-scan-amd\">\n<a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_scan_amd_mi250.svg\"><img style=\"float: initial\" alt=\"Two plots (linear, logarithmic) with results of all four STREAM Microbenchmarks for increasing amounts of data. An inset in the linear plot focuses on the maximum bandwidths for very large data sizes.\" title=\"GPU STREAM Variant for AMD MI250 GCD\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_scan_amd_mi250.tn.svg\" /></a>\n<a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_scan_nvidia_a100.svg\"><img style=\"float: initial\" class=\"hidden\" alt=\"Two plots (linear, logarithmic) with results of all four STREAM Microbenchmarks for increasing amounts of data. An inset in the linear plot focuses on the maximum bandwidths for very large data sizes.\" title=\"GPU STREAM Variant for NVIDIA A100 GPU\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_scan_nvidia_a100.tn.svg\" /></a>\n<label class=\"form-switch\" title=\"Click to toggle between MI250 and A100 results.\">\n  <input type=\"checkbox\" onclick=\"toggleAbsRel(this)\" />\n  <i></i>\n  A100\n</label>\n</div>\n\n<p>One GCD reaches around 1.42 TB/s for the copy kernel and about 1.34 TB/s for the triad kernel when the message size is large enough, as the inset view of the above linear plot shows (left). For triad, this is about 82% of the theoretically available peak. The double-logarithmic plot (right) shows well that the increase to the maximum bandwidth is <em>regular</em> (and according to a power law) and that the maximum is reached around \\(2^{26}\\) Byte (64 MiB).</p>\n\n<p>Below the plot, there’s a switch to show results for A100. The GPU has a lower peak bandwidth compared to MI250, but reaches nearly identical values for copy (1.42 TB/s) and triad (1.35 TB/s) kernels of the benchmark – resulting in utilization of 87% of the available peak. The data point at \\(2^{23}\\) Byte (8 MiB) is a weird, systematic outlier which reaches the peak (or even beyond).</p>\n\n<p>It is interesting, how closely a MI250 GCD matches the performance of an A100 GPU. In the following plot, I compare the triad bandwidth behaviors directly.</p>\n\n<p><a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_scan_compare_mi250_a100.svg\"><img style=\"float: initial\" alt=\"Linear and double-logarithmic comparison of Triad bandwidth. The A100 is always a tiny bit faster, except for at-peak bandwidths. There, it is a tiny-tiny bit faster (1.399 TB/s vs 1.349 TB/s).\" title=\"GPU STREAM Variant Comparison for Triad Kernel on MI250 GCD and A100 GPU\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_scan_compare_mi250_a100.tn.svg\" /></a></p>\n\n<p>Especially in the double-log plot one can see that the A100 is always a tiny amount faster. After the weird <em>outlier</em>, it much closer matches MI250 GCD bandwidth. Still, for the final value, the A100 is about 3,6% faster than the MI250 GCD.</p>\n\n<h2 id=\"threads-and-data-sizes\">Threads and Data Sizes</h2>\n\n<div class=\"absrel\" id=\"stream-amd\">\n<a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_mi250-abs.svg\"><img style=\"float: initial\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on MI250 GCD\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on MI250 GCD.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_mi250-abs.tn.svg\" /></a>\n<a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_mi250-rel.svg\"><img style=\"float: initial\" class=\"hidden\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on MI250 GCD\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on MI250 GCD. Relative values.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_mi250-rel.tn.svg\" /></a>\n<label class=\"form-switch\" title=\"Click to toggle between relative and absolute numbers in cells.\">\n  <input type=\"checkbox\" onclick=\"toggleAbsRel(this)\" />\n  <i></i>\n  Relative\n</label>\n</div>\n\n<p>To understand how well the memory can be accessed depending on the number of threads per block (<em>work items</em> in a <em>workgroup</em> in AMD terminology), the picture above shows four plots – one for each of the STREAM kernels. On the x axis, always three data sizes are shown; 0.5 GiB, 2 GiB, and 8 GiB – values on the larger side of things and on the plateau in the previous STREAM plots. On y, four semi-typical values for threads-per-block are chosen.</p>\n\n<p>It appears that 256 threads per block is always a good choice. So that’s going to be my go-to default for the future. You can view relative values for the link usage by flipping the switch below the picture – the usage is between 88% and 76%. It’s worthwhile to run a simple test like this once for your actual application, as the number of threads can in most cases be chosen somewhat freely, and may offer improvement of up to 7 pp (see add kernel for 2 GiB).</p>\n\n<div class=\"absrel\" id=\"stream-nvidia\">\n<a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_a100-abs.svg\"><img style=\"float: initial\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_a100-abs.tn.svg\" /></a>\n<a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_a100-rel.svg\"><img style=\"float: initial\" class=\"hidden\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU. Relative values.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_a100-rel.tn.svg\" /></a>\n<label class=\"form-switch\" title=\"Click to toggle between relative and absolute numbers in cells.\">\n  <input type=\"checkbox\" onclick=\"toggleAbsRel(this)\" />\n  <i></i>\n  Relative\n</label>\n</div>\n\n<p>On a first glimpse, the behavior of the A100 looks very similar. And – as expected – it is able to achieve higher bandwidths and higher relative usage. Note the different color scales: The lower bound for A100 is 1270 GiB/s and not 1140 GiB/s of MI250. On a second look, there seem to be some different underlying trend in the behavior on the A100. For the one-vector kernels (copy, scale), the A100 seems to prefer fewer threads and larger messages. For the two-vector kernels (add, triad), the last column for 8 GiB is interesting, as the bandwidth drops by 20 GiB going from 128 threads to more threads. All of this is probably not very relevant for real-world applications, but fun to see!</p>\n\n<details>\n\t<summary>Expand here to display pictures to compare MI250 to A100 directly by toggle of switch.</summary>\n\t<div class=\"absrel\" id=\"stream-amdnvidia\">\n\t<a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_mi250-abs.svg\"><img style=\"float: initial\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on MI250 GCD\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on MI250 GCD.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_mi250-abs.tn.svg\" /></a>\n\t<a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_a100-abs.svg\"><img style=\"float: initial\" class=\"hidden\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_a100-abs.tn.svg\" /></a>\n\t<label class=\"form-switch\" title=\"Click to toggle between relative and absolute numbers in cells.\">\n\t  <input type=\"checkbox\" onclick=\"toggleAbsRel(this)\" />\n\t  <i></i>\n\t  A100\n\t</label>\n\t</div>\n\t<div class=\"absrel\" id=\"stream-rel-amdnvidia\">\n\t<a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_mi250-rel.svg\"><img style=\"float: initial\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_mi250-rel.tn.svg\" /></a>\n\t<a href=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_a100-rel.svg\"><img style=\"float: initial\" class=\"hidden\" alt=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU\" title=\"STREAM Kernels for 3 data sizes and 4 numbers of threads per block on A100 GPU. Relative values.\" src=\"/assets/img/posts/2022-08-01-mi250-perf/stream_datavsthreads_a100-rel.tn.svg\" /></a>\n\t<label class=\"form-switch\" title=\"Click to toggle between relative and absolute numbers in cells.\">\n\t  <input type=\"checkbox\" onclick=\"toggleAbsRel(this)\" />\n\t  <i></i>\n\t  A100\n\t</label>\n\t</div>\n</details>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>AMD Instinct MI250, the GPU design which breaks the Exascale barrier in Frontier<sup id=\"fnref:mi250x\" role=\"doc-noteref\"><a href=\"#fn:mi250x\" class=\"footnote\" rel=\"footnote\">4</a></sup>, are quite powerful GPUs, featuring up to 90 TFLOP/s performance in FP64. We deployed two nodes with four MI250s in JURECA DC as part of an Evaluation Platform at beginning of 2022. After some setup time, the nodes can now be used for tests. Results from an early <a href=\"https://indico-jsc.fz-juelich.de/event/282/other-view?view=standard\">Porting workshop can be found online</a> and <a href=\"https://www.researchgate.net/publication/362275548_Accuracy_and_performance_of_the_lattice_Boltzmann_method_with_64-bit_32-bit_and_customized_16-bit_number_formats\">Moritz Lehmann has just published a paper</a> with results obtained on the machine.</p>\n\n<p>I used the <em>bandwidth</em> experiment of the OSU Microbenchmarks to study connections between the GPUs of a node with MPI. One can see that each MI250 consists of two Graphics Compute Dies (GCD) which are basically two individual <em>GPUlets</em> on a GPU. The obtainable bandwidths are diverse, due to the complex connection matrix between the GCDs. Bandwidths between GCDs on the same GPU are usually about 150 GiB/s, and between GCDs of different GPUs between 80 GiB/s and 40 GiB/s. I also showed results for A100 GPUs which have much more homogeneous connections, with always 87 GiB/s between the GPUs.</p>\n\n<p>As a second experiment, I ran a CUDA variant of the STREAM benchmark, which I <em>HIPified</em> easily for AMD. When increasing the data size, one can see that the memory bus is saturated at around 64 MiB data sizes, and eventually a 1.42 TiB/s bandwidth is reached – about 87% of the available peak of the GCD<sup id=\"fnref:membw\" role=\"doc-noteref\"><a href=\"#fn:membw\" class=\"footnote\" rel=\"footnote\">5</a></sup>. Looking at different number of threads per block, 256 threads seems to be a good choice, memory-wise. In comparison to A100 GPUs, one sees that the obtained bandwidth is surprisingly similar (the A100 is slightly faster, though) – but with a peak bandwidth a little lower for the A100.</p>\n\n<p>Each GCD seems to be similar to an A100 in many ways. For the <em>connection</em>-targeted benchmarks shown, a MI250 GCD is usually a little slower and less efficient than the A100. But using 30% less power. Quite interesting devices.</p>\n\n<h1 id=\"technical-details\">Technical Details</h1>\n\n<p>Benchmarks were performed on the AMD Instinct MI250 nodes of JURECA DC’s Evaluation Platform. While the systems run publicly available software and firmware versions, the benchmarks were run while we still got to know the systems. Please let me know if you discover errors or have significantly different results on another machine. The evaluation notebooks are linked below.</p>\n\n<p>The following software and versions were used</p>\n\n<ul>\n  <li>ROCm 5.2.0</li>\n  <li>ROCm driver 5.16.9.22.20</li>\n  <li>CUDA 11.5</li>\n  <li>CUDA driver 510.47.03.</li>\n  <li>UCX 1.12.1 (with <code class=\"language-plaintext highlighter-rouge\">UCX_TLS=rc_x,self,sm,rocm_copy,rocm_ipc</code> for ROCm and <code class=\"language-plaintext highlighter-rouge\">UCX_TLS=rc_x,self,sm,cuda_ipc,gdr_copy,cuda_copy</code> for CUDA)</li>\n  <li>OpenMPI 4.1.2</li>\n</ul>\n\n<h2 id=\"osu-microbenchmarks\">OSU Microbenchmarks</h2>\n\n<p>Version 5.9; compiled as per <a href=\"https://github.com/openucx/ucx/wiki/Build-and-run-ROCM-UCX-OpenMPI#rocm-enabled-osu\">official OpenUCX instructions</a>:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>./configure <span class=\"nt\">--enable-rocm</span> <span class=\"nt\">--with-rocm</span><span class=\"o\">=</span>/opt/rocm <span class=\"nv\">CC</span><span class=\"o\">=</span><span class=\"si\">$(</span>which mpicc<span class=\"si\">)</span> <span class=\"nv\">CXX</span><span class=\"o\">=</span><span class=\"si\">$(</span>which mpicxx<span class=\"si\">)</span> <span class=\"nv\">LDFLAGS</span><span class=\"o\">=</span><span class=\"s2\">\"-L</span><span class=\"nv\">$EBROOTOPENMPI</span><span class=\"s2\">/lib/ -lmpi -L/opt/rocm/lib </span><span class=\"si\">$(</span>hipconfig <span class=\"nt\">-C</span><span class=\"si\">)</span><span class=\"s2\">\"</span> <span class=\"nv\">CPPFLAGS</span><span class=\"o\">=</span><span class=\"s2\">\"-std=c++11\"</span>\n</code></pre></div></div>\n\n<p>Run by setting <code class=\"language-plaintext highlighter-rouge\">HIP_VISIBLE_DEVICES=A,B</code>, like:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">HIP_VISIBLE_DEVICES</span><span class=\"o\">=</span>0,1 <span class=\"se\">\\</span>\nsrun <span class=\"nt\">-n</span> 2 mpi/pt2pt/osu_bw <span class=\"nt\">-d</span> rocm <span class=\"nt\">-m</span> 4194304:4194304 D D\n</code></pre></div></div>\n\n<h2 id=\"stream-variant\">STREAM Variant</h2>\n\n<p>Base code from my GitHub – <a href=\"https://github.com/AndiH/CUDA-Cpp-STREAM\">github.com/AndiH/CUDA-Cpp-STREAM</a> – and then compiled the following for AMD</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>hipify-perl CUDA-Cpp-STREAM/stream.cu <span class=\"o\">></span> stream.cu.hip\n<span class=\"nv\">HIP_PLATFORM</span><span class=\"o\">=</span>amd hipcc <span class=\"nt\">--offload-arch</span><span class=\"o\">=</span>gfx90a <span class=\"nt\">-o</span> hip-stream stream.cu.hip\n</code></pre></div></div>\n\n<p>Run by looping through data sizes:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>./stream <span class=\"nt\">-n</span> <span class=\"k\">$((</span><span class=\"m\">2</span><span class=\"o\">**</span><span class=\"m\">0</span><span class=\"k\">))</span> <span class=\"nt\">-t</span> <span class=\"nt\">--csv</span> <span class=\"nt\">-f</span> | <span class=\"nb\">tee </span>file.csv <span class=\"o\">&&</span> <span class=\"se\">\\</span>\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..28<span class=\"o\">}</span><span class=\"p\">;</span> <span class=\"k\">do</span> <span class=\"se\">\\</span>\n\t./stream <span class=\"nt\">-n</span> <span class=\"k\">$((</span><span class=\"m\">2</span><span class=\"o\">**</span><span class=\"nv\">$i</span><span class=\"k\">))</span> <span class=\"nt\">--csv</span> <span class=\"nt\">-f</span><span class=\"p\">;</span> <span class=\"se\">\\</span>\n<span class=\"k\">done</span> | <span class=\"nb\">tee</span> <span class=\"nt\">-a</span> file.csv\n</code></pre></div></div>\n\n<h2 id=\"evaluation-notebooks\">Evaluation Notebooks</h2>\n\n<p>The graphs presented here are created in Jupyter Notebooks with Pandas, Matplotlib, and Seaborn. Find the Notebooks here for reference, including the evaluation and raw data.</p>\n\n<ul>\n  <li>OSU bandwidth micro-benchmark: <a href=\"https://nbviewer.org/url/x-dev.pages.jsc.fz-juelich.de/assets/misc/posts/2022-08-01-mi250-perf/osu-bw-overview.ipynb\">nbviewer</a>, <a href=\"/assets/misc/posts/2022-08-01-mi250-perf/osu-bw-overview.ipynb\">raw</a></li>\n  <li>STREAM GPU variant: <a href=\"https://nbviewer.org/url/x-dev.pages.jsc.fz-juelich.de/assets/misc/posts/2022-08-01-mi250-perf/stream-analysis.ipynb\">nbviewer</a>, <a href=\"/assets/misc/posts/2022-08-01-mi250-perf/stream-analysis.ipynb\">raw</a></li>\n</ul>\n\n<h2 id=\"post-changelog\">Post Changelog</h2>\n\n<p>Since publication of this blog post, the following edits were made</p>\n\n<ul>\n  <li>2022-Sep-21: Replaced MI250 node connections diagram by a corrected version, shared directly by AMD (not yet in a Whitepaper).</li>\n</ul>\n\n<script type=\"text/javascript\">\n\tfunction toggleAbsRel(obj) {\n\t\tvar imgs = obj.parentElement.parentElement.getElementsByTagName('img');\n\t\tfor (const img of imgs) {\n\t\t  img.classList.toggle('hidden');\n\t\t}\n\t}\n</script>\n\n<style type=\"text/css\">\n\t.form-switch {\n\t  display: inline-block;\n\t  cursor: pointer;\n\t  -webkit-tap-highlight-color: transparent;\n\t  color: gray;\n\t  font-size: smaller;\n\t  float: right;\n\t}\n\t.from-switch .highlight {\n\t\tcolor: #7b8a7e;\n\t}\n\t.form-switch i {\n\t  position: relative;\n\t  display: inline-block;\n\t  margin-right: .5rem;\n\t  width: 40px;\n\t  height: 20px;\n\t  background-color: #e6e6e6;\n\t  border-radius: 23px;\n\t  vertical-align: text-bottom;\n\t}\n\t.form-switch i::before {\n\t  content: \"\";\n\t  position: absolute;\n\t  left: 0;\n\t  width: 36px;\n\t  height: 16px;\n\t  background-color: #fff;\n\t  border-radius: 11px;\n\t  transform: translate3d(2px, 2px, 0) scale3d(1, 1, 1);\n\t  transition: all 0.25s ease-in-out;\n\t}\n\t.form-switch i::after {\n\t  content: \"\";\n\t  position: absolute;\n\t  left: 0;\n\t  width: 16px;\n\t  height: 16px;\n\t  background-color: #fff;\n\t  border-radius: 11px;\n\t  box-shadow: 0 1px 2px rgba(0, 0, 0, 0.24);\n\t  transform: translate3d(2px, 2px, 0);\n\t  transition: all 0.2s ease-in-out;\n\t}\n\t.form-switch:active i::after {\n\t  width: 22px;\n\t  transform: translate3d(2px, 2px, 0);\n\t}\n\t.form-switch:active input:checked + i::after { transform: translate3d(10px, 2px, 0); }\n\t.form-switch input { display: none; }\n\t.form-switch input:checked + i { background-color: #C4EACB; }\n\t.form-switch input:checked + i::before { transform: translate3d(12px, 2px, 0) scale3d(0, 0, 0); }\n\t.form-switch input:checked + i::after { transform: translate3d(22px, 2px, 0); }\n</style>\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:eval-platform\" role=\"doc-endnote\">\n      <p>Actually, the Evaluation Platform was created together with the AMD nodes! <a href=\"#fnref:eval-platform\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:nostream\" role=\"doc-endnote\">\n      <p>The data rate on each GPU itself gives only a rough idea about the memory bandwidth; it’s not a <em>proper</em> memory benchmark because of the implementation and indirections – STREAM is much better suited for that. For STREAM, see further down in the text. <a href=\"#fnref:nostream\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:xgmi\" role=\"doc-endnote\">\n      <p>Infinity Fabric is also called <em>xGMI</em>. One xGMI lane can do 25 Gbit/s, and there seem to be 16 lanes per link. So, one Infinity Fabric connection can do 50 GB/s. <a href=\"#fnref:xgmi\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:mi250x\" role=\"doc-endnote\">\n      <p>Actually, Frontier does not deploy MI250s but MI250Xs. The difference is mainly in the number of compute units: MI250X has 220 and MI250 has 208. There are performance difference because of this (like 95.7 TFLOP/s peak vs. 90.5 TFLOP/s), but no direct differences relating to memory. An additional difference in the design of Frontier is relating to the CPU: The GPUs are directly connected via a coherent Infinity Link to a single CPU – not PCIe, no two CPU sockets. <a href=\"#fnref:mi250x\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:membw\" role=\"doc-endnote\">\n      <p>The advertised 3276.8 GB/s peak memory bandwidth are actually for the full GPU. I divided by two to get the per-GCD bandwidth; 1638 GB/s. <a href=\"#fnref:membw\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>"},{"id":"https://x-dev.pages.jsc.fz-juelich.de//2022/07/21/optima-d35","title":"OOPS Version 1 Release","link":"https://x-dev.pages.jsc.fz-juelich.de//2022/07/21/optima-d35","published":"2022-07-21T09:15:42+00:00","description":"A few days ago, OPTIMA announced the release of deliverable 3.5, to which I contributed. This deliverable is part of a set of five deliverables under work package 3. But first, let’s talk about...","isPermalink":false,"tags":["OPTIMA","Deliverable"],"authors":[{"name":"Albert","url":null}],"image":null,"modified":"2022-07-21T09:15:42+00:00","contentHtml":"<p>A few days ago, <a href=\"/projects/optima.html\">OPTIMA</a> announced the release of <a href=\"https://optima-hpc.eu/deliverables/\">deliverable 3.5</a>, to which I contributed. This deliverable is part of a set of five deliverables under work package 3. But first, let’s talk about <strong>OPTIMA</strong>.</p>\n\n<p>OPTIMA is an EU-funded project whose goal is to prove that several HPC applications can take advantage of the future highly heterogeneous FPGA-populated HPC systems. In addition, by using newly introduced tools and runtimes, application porting/development can be almost as simple as developing software for conventional HPC systems incorporating GPUs.</p>\n\n<p><img src=\"/assets/img/posts/2022-07-06-optima-d35/optima_concept.png\" alt=\"OPTIMA's Hardware/Software Setup\" /></p>\n\n<p>Deliverable 3.5 is the first version of an Open-Source library called OOPS (<em>Optima Open Source</em>) for FPGA-based HPC systems. This library contains a set of optimised software routines for industrial and scientific applications, taking advantage of OPTIMA hardware platforms.</p>\n\n<p>The OOPS library follows a standard C-based application programming interface (API) and supports the latest Xilinx Alveo FPGA cards such as U55C and U280. This first version of OOPS contains the following kernels. Initial tests show similar or better performance of a single compute unit in comparison to single-thread CPU versions for most of the kernels. In fact, as shown in details in the deliverable, this first version uses just a fraction of the FPGA resources.</p>\n\n<p><img src=\"/assets/img/posts/2022-07-06-optima-d35/kernels.png\" alt=\"Overview of implemented algorithms in OOPS\" /></p>\n\n<p>The library will continue to receive more updates and bug fixes in the future, the immediate ones focusing on optimisation to achieve excellent energy-performance-rations. Later updates will include  adding device-specific implementations such as utilising High Bandwith Memory, adding more solvers such as Jacobi Preconditioner and allowing massive parallel processing using more compute units.</p>\n\n<p>More details about OPTIMA and the deliverable are available in PDF form <a href=\"https://optima-hpc.eu/wp-content/uploads/2022/07/D3.5.pdf\">here</a>.</p>"},{"id":"https://x-dev.pages.jsc.fz-juelich.de//2022/07/13/transformers-matmul","title":"A mathematician’s introduction to transformers and large language models","link":"https://x-dev.pages.jsc.fz-juelich.de//2022/07/13/transformers-matmul","published":"2022-07-13T10:18:42+00:00","description":"About This blog post is based on a presentation I held at the “New Trends in Computational Science in Engineering and Industrial Mathematics” workshop in Magdeburg on 01/07/2022. My goal is to give a brief...","isPermalink":false,"tags":["Workshop","OpenGPTX"],"authors":[{"name":"Carolin","url":null}],"image":null,"modified":"2022-07-13T10:18:42+00:00","contentHtml":"<h1 id=\"about\">About</h1>\n\n<p><a href=\"/assets/img/posts/2022-07-13-transformers-matmul/titleslide.png\"><img alt=\"Ä Tännschen Please - Matrix Operations at the core of Natural Language Processing in OpenGPT-X by Carolin Penke\" title=\"Title slide of presentation\" src=\"/assets/img/posts/2022-07-13-transformers-matmul/titleslide.tn.png\" srcset=\"/assets/img/posts/2022-07-13-transformers-matmul/titleslide.tn.png 1x, /assets/img/posts/2022-07-13-transformers-matmul/titleslide.tn.2x.png 2x, /assets/img/posts/2022-07-13-transformers-matmul/titleslide.tn.4x.png 4x\" /></a>\nThis blog post is based on a presentation I held at the “<a href=\"https://indico3.mpi-magdeburg.mpg.de/event/6/\">New Trends in Computational Science in Engineering and Industrial Mathematics</a>” workshop in Magdeburg on 01/07/2022. My goal is to give a brief introduction to the state of current large language models, the OpenGPT-X project, and the transformer neural network architecture for people unfamiliar with the subject.</p>\n\n<ol id=\"markdown-toc\">\n  <li><a href=\"#about\" id=\"markdown-toc-about\">About</a></li>\n  <li><a href=\"#what-is-a-language-model\" id=\"markdown-toc-what-is-a-language-model\">What is a language model?</a></li>\n  <li><a href=\"#deep-learning-architectures\" id=\"markdown-toc-deep-learning-architectures\">Deep learning architectures</a></li>\n  <li><a href=\"#attention-please\" id=\"markdown-toc-attention-please\">Attention please!</a></li>\n  <li><a href=\"#from-attention-to-transformers\" id=\"markdown-toc-from-attention-to-transformers\">From attention to transformers</a></li>\n  <li><a href=\"#recent-developments-in-large-language-models\" id=\"markdown-toc-recent-developments-in-large-language-models\">Recent developments in large language models</a></li>\n  <li><a href=\"#takeaways-and-learnings\" id=\"markdown-toc-takeaways-and-learnings\">Takeaways and learnings</a></li>\n  <li><a href=\"#sources\" id=\"markdown-toc-sources\">Sources</a></li>\n</ol>\n\n<p>The audience at the workshop had a mathematics background and is assumed to have a good understanding of linear algebra, but not necessarily of neural networks. Basically, the target audience is past me from before I started working on this project with the goal of understanding the math behind transformers. The questions I want to answer are:</p>\n<ol>\n  <li>Where are matrix products performed in training large language models?</li>\n  <li>What makes transformers well-suited for high performance computing (HPC)?</li>\n</ol>\n\n<p>If you find any mistakes or unclear points feel free to let me know in order to improve this post.</p>\n\n<h1 id=\"what-is-a-language-model\">What is a language model?</h1>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\">Natural language processing</a> deals with making the human language accessible for computations.<sup id=\"fnref:Coursera\" role=\"doc-noteref\"><a href=\"#fn:Coursera\" class=\"footnote\" rel=\"footnote\">1</a></sup> <sup id=\"fnref:Speech\" role=\"doc-noteref\"><a href=\"#fn:Speech\" class=\"footnote\" rel=\"footnote\">2</a></sup> Having a computer understand what you say can help in many situations. Applications of NLP include intelligent speakers, chatbots, translation, text generation, summarization and much more.</p>\n\n<p>A <a href=\"https://en.wikipedia.org/wiki/language_model\">language model</a> forms the back bone of these applications. A language model is just a probability distribution. Given a sequence of words \\(w_{1:(t-1)}=(w_1,\\dots,w_{t-1})\\), a language model gives the probability of all the words in your vocabulary \\(V\\) to follow this sequence,</p>\n\n\\[P(w_t| w_{1:(t-1)}),\\qquad w_1,\\dots,w_{t-1},w_{t}\\in V.\\]\n\n<p>With such a language model one can generate new texts: Start with a sentence, then choose the word with the highest probability (or sample according to probabilities) and feed the new appended sequence back into the model to generate the next word.  The language model can be used to assign a probability to a sentence (using the chain rule of conditional probabilities) as</p>\n\n\\[P(w_{1:n}) = \\prod_{i=1}^{n} P(w_i|w_{1:(i-1)}).\\]\n\n<p>One can imagine this to be helpful in grammar corrections for example.</p>\n\n<p>There are different ways to arrive at such a language model. One could think about putting all rules of grammar and the meaning of words into a computer program. However, this is extremely difficult to do. The approach that caught on in recent years and produced very impressive language models does not require encoding explicit grammar or world knowledge. Instead, neural networks are trained on huge amounts of text and learn to form proper sentences just from the data they see.</p>\n\n<p>In order to understand the broader context of the transformer architecture in NLP applications, we clarify some terms related to training and application of large language models.</p>\n<ol>\n  <li><em>Pre-training</em>: The goal of pre-training is to provide a general language model that has a good understanding of how language is used in a variety of settings.</li>\n  <li><em>Fine-tuning</em>: In fine-tuning, a pre-trained model is trained further on a (comparatively) small set of task-specific data. Before the emergence of pre-trained models, neural networks were trained from scratch for each specific application (also called <em>downstream task</em>). Using a pre-trained model uses compute resources more efficiently and can avoid overfitting. Fine-tuning can involve continued training of the whole network or parts of it (<em>layer freezing</em>). This step is also called adaptation and may also include adapting the neural network’s architecture.</li>\n  <li><em>Inference</em>: When the model is deployed, for example in form of a chatbot in an online shop, inference describes computing the output (the answer of the chatbot) given a user’s input, using the trained model. This corresponds to a forward-pass of the neural network.</li>\n</ol>\n\n<p>The learning methodology described by the first two steps (pre-training followed by fine-tuning) is called sequential transfer learning.<sup id=\"fnref:Transfer\" role=\"doc-noteref\"><a href=\"#fn:Transfer\" class=\"footnote\" rel=\"footnote\">3</a></sup></p>\n\n<p>All these steps need computing resources.  The computational device of choice is typically the GPU due to the massive parallelism it provides and hardware features that make it extremely efficient in performing matrix multiplications. We will see below (in the section <a href=\"#attention-please\">Attention please!</a>) how matrix multiplications form the core of training the model.  Pre-training of large models is the most computationally demanding step and happens on a supercomputer such as <a href=\"https://www.fz-juelich.de/en/ias/jsc/systems/supercomputers/juwels\">JUWELS</a> at Forschungszentrum Jülich using lots (hundreds) of GPUs in parallel. Fine-tuning and inference may happen on server systems with a handful of GPUs.</p>\n\n<h1 id=\"deep-learning-architectures\">Deep learning architectures</h1>\n\n<p><a title=\"Dake, Mysid, CC BY 1.0 <https://creativecommons.org/licenses/by/1.0>, via Wikimedia Commons\"><img width=\"256\" alt=\"Neural network\" src=\"/assets/img/posts/2022-07-13-transformers-matmul/neural_network-wikimedia.svg\" />\n </a></p>\n\n<p>Neural networks are everywhere. You might be familiar with the basic ideas. There are many great resources to learn the foundations.<sup id=\"fnref:DL-CourseR\" role=\"doc-noteref\"><a href=\"#fn:DL-CourseR\" class=\"footnote\" rel=\"footnote\">4</a></sup> <sup id=\"fnref:DL-Course-MIT\" role=\"doc-noteref\"><a href=\"#fn:DL-Course-MIT\" class=\"footnote\" rel=\"footnote\">5</a></sup>  The goal of training a neural network is to learn input-output relations from data. When a neural network is well-trained, a vector representing input data is fed to an input layer.  In illustrations this is on the left (like the one to the right by <a href=\"https://commons.wikimedia.org/wiki/File:Neural_network.svg\">Dake & Mysid on Wikimedia Commons</a>). Then it is processed by passing several hidden layers until it reaches an output layer. Moving from one layer to the next means multiplying the vector with a matrix, adding another vector and applying a non-linear activation function. This is called a forward-pass or forward-propagation.</p>\n\n<p>The elements of the matrices are called weights, the elements of the additive vector are called biases. Weights and biases are the parameters that are learned during training. For your training data, the output given by the network should closely match the real desired output, i.e. the loss function (measure of difference between network’s output and desired output) should be minimal. If this is not yet the case, we change the parameters to achieve a smaller loss. This is done using gradient descent. The gradient of the loss function with respect to the parameters is computed. The parameters are updated by adding the gradient multiplied by a step size (called learning rate). The actual computation of the gradients uses the chain rule from calculus and involves starting at the output layer and moving backwards through the network. This is why computing the gradients is called backward propagation.</p>\n\n<p>In practice, more useful heuristics are added to this process, and it works very well for many tasks. However, it is difficult to use the fully-connected neural network for NLP tasks. One problem is that the input size is fixed, and we would like to process longer as well as shorter word sequences as input. In general, a dense neural network does not represent the nature of language very well.</p>\n\n<p>Luckily, this standard feed-forward neural network is only the most basic neural network architecture of many that were devised over the years for various applications.</p>\n\n<p><a href=\"/assets/img/posts/2022-07-13-transformers-matmul/developmentcircle.svg\"><img alt=\"Development cycle. How new neural network architectures emerge: Somebody has an intuition -> It turns out tu work well -> Widespread adoption -> Further research in how it works -> Somebody has an intuition -> ...\" title=\"Development cycle\" width=\"256\" src=\"/assets/img/posts/2022-07-13-transformers-matmul/developmentcircle.svg\" /></a></p>\n\n<p>In the field of NLP and language modelling, until recently, sequential models were the state of the art. These include <em>recurrent neural networks</em> (RNNs) and <em>long short-term memory</em> (LSTM) networks.<sup id=\"fnref:LSTMs\" role=\"doc-noteref\"><a href=\"#fn:LSTMs\" class=\"footnote\" rel=\"footnote\">6</a></sup></p>\n\n<p>RNNs apply the same neural network (with learned parameters) to every word in a sequence of words. Additionally, this neural network takes an internal state as input, which comes as output from the neural network associated to the previous word. This way the network can learn to use information from earlier words in the sequence. When one writes down the gradient of the loss function with respect to the parameters using the chain rule, one can see that the newest word has the most influence. The influence of the previous words diminishes exponentially. Intuitively, this makes sense: For choosing the next word, the most recent word is on average more important than a word further in the past. However, in practice, language is more nuanced. Some specific words in the past can be very important for choosing future words, and a smart neural network should know how to look for them. Just think of a very long relative clause for example. Older words having less influence on the gradients is therefore more of a bug than a feature, and this is called the <em>vanishing gradients</em> problem.</p>\n\n<p>LSTMs alleviate this issue by introducing an extra cell state (serving as “memory”) whose exact influence is determined by gates that are defined by more learnable parameters.</p>\n\n<p>One drawback remains: Both RNNs and LSTMs process their input data sequentially. Consider the forward pass: In order to apply the neural network (a series of matrix multiplications) on an input word vector \\(x_i\\) we also need the result from applying the network on the previous word vector \\(x_{i-1}\\). We can not stack the word vectors together in a matrix and apply a neural network all at once.</p>\n\n<p>Formulating algorithms to use matrix-matrix products as main computational element is a good step forward towards the efficient use of modern compute hardware. This is true on the small scale of a single processor to the large scale of supercomputers using thousands of GPUs. Matrix-matrix products are the key.</p>\n\n<p>Realizing this need, researchers started “having intuitions” about neural network architectures that employ these operations to learn to pay <em>attention</em> to other relevant words.</p>\n\n<h1 id=\"attention-please\">Attention please!</h1>\n<p>The so-called <em>attention</em> mechanism had been employed in the context of sequence models to give the model the opportunity to learn which words are relevant for the next word. The landmark paper “Attention is all you need” (2017) <sup id=\"fnref:Attention\" role=\"doc-noteref\"><a href=\"#fn:Attention\" class=\"footnote\" rel=\"footnote\">7</a></sup> showed that you do not need a recurrent network structure, and that the attention mechanism (together with some other tricks like positional encoding) is powerful enough for impressive results.  The resulting neural network architecture is called a transformer.</p>\n\n<p>In the following we describe a forward-pass through a (self-)attention layer, which forms the central element of a transformer block. A neural network architecture is called a transformer when it consists of several transformer blocks. Backpropagation is taken care of by using the automatic differentiation engines of frameworks such as PyTorch or TensorFlow.</p>\n\n<p>Consider a sequence of input tokens \\(x_1,\\dots, x_n\\in\\mathbb{R}^{n_\\text{model}}\\) represented by vectors. Tokens are the smallest building blocks into which word sequences are divided for processing. The process of getting a sequence of tokens (represented as a series of integers referring to a vocabulary) from a text string is called tokenization. The vector representation of a token is called an embedding and spatially encodes the meaning of tokens and their relationship towards each other. In the case of transformers, word embeddings are also learned during pre-training. You can think of this as a matrix with learned entries being multiplied with a one-hot vector, i.e. choosing row \\(i\\) when the token is encoded as integer \\(i\\).  A one-hot vector is called a (standard) unit vector in numerical linear algebra.</p>\n\n<p>The processing of the first three input vectors \\(x_1, x_2, x_3\\) to generate an output vector \\(y_3\\) is seen in the following diagram:<sup id=\"fnref:Speech:1\" role=\"doc-noteref\"><a href=\"#fn:Speech\" class=\"footnote\" rel=\"footnote\">2</a></sup></p>\n\n<p><a href=\"/assets/img/posts/2022-07-13-transformers-matmul/attention.svg\"><img alt=\"Operations in a self-attention layer. Adapted from \"Speech and Language Processing (3rd ed. draft)\" by Dan Jurafsky and James H. Martin \" title=\"Operations in a self-attention layer. Adapted from \"Speech and Language Processing (3rd ed. draft)\" by Dan Jurafsky and James H. Martin\" src=\"/assets/img/posts/2022-07-13-transformers-matmul/attention.svg\" /></a></p>\n\n<p>Among the learned parameters of a transformer block are three matrices \\(W_k\\), \\(W_q\\) and \\(W_v\\). They transform an input vector \\(x_i\\) to generate three vectors \\(k_i\\), \\(q_i\\) and \\(v_i\\). The convention is to treat the vectors as row vectors and apply the matrix from the right:</p>\n\n\\[k_i \\leftarrow  x_i W_k\\in\\mathbb{R}^{1\\times d_k} \\quad q_i \\leftarrow  x_i W_q \\in\\mathbb{R}^{1\\times d_k},\\quad v_i \\leftarrow  x_i W_v \\in\\mathbb{R}^{1\\times d_v}, \\\\ \\text{for } i=1,\\dots, n.\\]\n\n<p>The vectors \\(k_i\\), \\(q_i\\) and \\(v_i\\) are called queries, keys and values. There is some intuition behind these names that imagines the attention mechanism as retrieving information similar to a database. But I did not find this very helpful in understanding what is going on, so I will not go into more detail here.</p>\n\n<p>To compute the output vector \\(y_3\\), one first computes scalar products of the query vector \\(q_i\\) and all previous key vectors \\(k_1,\\dots, k_i\\). In order to prevent numerical overflow, the results are scaled by \\(\\sqrt{d_k}^{-1}\\).  Then the softmax activation function is applied.</p>\n\n\\[\\alpha_{i,j} \\leftarrow \\frac{q_i k_j^{T}}{\\sqrt{d_k}}\\quad \\text{for }j=1,\\dots, i\\\\\n\\alpha_{i,j} \\leftarrow \\text{softmax}(\\alpha_{i,j}) = \\frac{\\exp{(\\alpha_{i,j})}}{\\sum_{j=1}^i{\\exp{(\\alpha_{i,j})}}}\\quad \\text{for }j=1,\\dots, i\\]\n\n<p>The softmax function, applied on a set of \\(n\\) values, returns \\(n\\) values between 0 and 1 that sum up to one. Larger values are mapped closer to one and smaller values are mapped closer to zero following a sigmoid function. In a regular “max” function the largest value is mapped to 1 and all smaller values are mapped to 0. The name “softmax” comes from it being a “softer” version of this.</p>\n\n<p>Now the output vector is given as a sum of the scalars \\(a_{i,j}\\) and the value vectors.</p>\n\n\\[y_i \\leftarrow \\sum_{j=1}^i \\alpha_{i,j} v_j \\quad \\text{for }j=1,\\dots, i.\\]\n\n<p>The beauty of the attention mechanism is now that we can consider all input vectors at once by stacking them on top of each other forming a matrix</p>\n\n\\[X = \\begin{bmatrix}\n- x_1 -\\\\\n\\vdots\\\\\n- x_{n} - \n\\end{bmatrix}\\in\\mathbb{R}^{n\\times n_\\text{model}}.\\]\n\n<p>Keys, queries and values of all input vectors are computed via matrix-matrix multiplication as</p>\n\n\\[K= \\begin{bmatrix}\n- k_1 -\\\\\n\\vdots\\\\\n- k_{n} - \n\\end{bmatrix} \\leftarrow XW_k \\in\\mathbb{R}^{n\\times d_k},\\quad Q=\\begin{bmatrix}\n- q_1 -\\\\\n\\vdots\\\\\n- q_{n} - \n\\end{bmatrix} \\leftarrow XW_q \\in\\mathbb{R}^{n\\times d_k}, \\\\\n V=\\begin{bmatrix}\n- v_1 -\\\\\n\\vdots\\\\\n- v_{n} - \n\\end{bmatrix} \\leftarrow XW_v\\in\\mathbb{R}^{n\\times d_v}.\\]\n\n<p>The scalars \\(\\alpha_{i,j}\\) can now be computed as a softmax applied to the rows of a matrix-matrix product</p>\n\n\\[A =  [\\alpha_{i,j}]_{i,j=1,\\dots,n} \\leftarrow \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\in\\mathbb{R}^{n\\times n}.\\]\n\n<p>The next step is the summation of value vectors, weighted with the values \\(\\alpha_{i,1},\\dots,\\alpha_{i,n}\\) (line \\(i\\) of \\(A\\)). This is realized for all vectory \\(y_1,\\dots,y_n\\) at once by – you guessed it –  another matrix-matrix product. So in total we have</p>\n\n\\[Y = \\begin{bmatrix}\n- y_1 -\\\\\n\\vdots\\\\\n- y_{n} - \n\\end{bmatrix} \\leftarrow \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\in\\mathbb{R}^{n\\times n_v}.\\]\n\n<p>Further remarks on simplifications we made for clarity in the equations:</p>\n<ol>\n  <li>The softmax in the last assignments is not a matrix function. Instead it is just a shorthand for applying the softmax function to the rows of the matrix, i.e.\n\\(\\alpha_{i,j} \\leftarrow \\frac{\\exp{(\\alpha_{ij})}}{\\sum_{j=1}^i\\exp{(\\alpha_{ij})}}.\\)</li>\n  <li>The self-attention mechanism we described when working with vectors and in the diagram is called <em>masked</em> self-attention. This means that computing the output \\(y_i\\) only requires the inputs \\(x_1,\\dots,x_i\\). However, when we wrote down the computations using matrices, we forgot about this and also the query, key and value vectors of \\(x_{i+1},\\dots,x_n\\) are used to compute \\(y_i\\). When training a neural network as a language model predicting the next word this can be undesirable.  Then the upper triangular part of the scalar product matrix \\(A\\) represents “the future” and should not be used. To this end, the upper right half of the matrix is <em>masked</em>, i.e. the values are set to \\(-\\infty\\). With the convention \\(\\exp{(-\\infty)}=0\\), these values do not contribute to the softmax. In transformer architectures intended for encoding information from language, such as BERT, masking during training is realized differently. In this case the model is allowed to see context on the right side of a token.</li>\n  <li>Any matrix multiplication can also involve adding a bias vector (for low level enthusiasts: in typical <code class=\"language-plaintext highlighter-rouge\">gemm</code> fashion), which is not stated here explicitly.</li>\n</ol>\n\n<h1 id=\"from-attention-to-transformers\">From attention to transformers</h1>\n\n<p>Transformer neural networks arrange attention layers and other network layers in various configurations. A number of \\(h\\) attention layers (<em>attention heads</em>) are connected in parallel to form <em>multi-headed attention</em>. Every head has independent training parameters. The attention heads’ outputs (matrices of dimension \\(n \\times n_v\\)) are concatenated, forming a matrix of dimension \\(n\\times h n_v\\). This matrix is brought back into the right form by multiplying it with another trained matrix \\(W_O\\in\\mathbb{R}^{hn_v\\times n_\\text{model}}\\):</p>\n\n\\[Y \\leftarrow \\begin{bmatrix} Y_1&\\cdots & Y_h\\end{bmatrix} W_O \\in\\mathbb{R}^{n\\times n_\\text{model}}.\\]\n\n<p>Multi-headed attention together with normalization layers, feed-forward layers, and residual connections forms a transformer block. The input and the output of a transformer block have the same shape, so they can be connected in series. For example for GPT-1 a transformer block is repeated 12 times. In order to generate a probability distribution for the next word in a sequence, one more linear transformation layer and a softmax is employed at the very end.</p>\n\n<p>The exact transformer architecture can vary and depends on the training objective. The original paper (<em>Attention is all you need</em>) considered machine translation. Here, an encoder-decoder structure makes sense: First the sentence in the original language is encoded using a stack of transformer blocks as described above. Both directions of information flow are allowed. The decoder’s structure is mostly similar except that the self-attention is masked and there is a second (multi-head) attention layer in each transformer block. In contrast to the forms of attention we discussed before, this is not <em>self</em>-attention, but instead attention is paid to the outputs of the encoder: The output vectors of the encoder are used to compute key and value vectors which serve as input for the decoder’s attention block.</p>\n\n<p>I would suggest not to think too much about wether a network architecture is an “encoder” (BERT)<sup id=\"fnref:BERT\" role=\"doc-noteref\"><a href=\"#fn:BERT\" class=\"footnote\" rel=\"footnote\">8</a></sup> or a “decoder” (GPT)<sup id=\"fnref:GPT1\" role=\"doc-noteref\"><a href=\"#fn:GPT1\" class=\"footnote\" rel=\"footnote\">9</a></sup> and not try to relate them to the encoder-decoder architecture from the <em>Attention is all you need</em> paper. They are similar in the main ideas, and details vary anyway. The main difference is the masking during training as described above. My theory is that BERT decided to call itself an encoder, mainly to get an “E” for its acronym, to keep this running gag about sesame street characters going.</p>\n\n<h1 id=\"recent-developments-in-large-language-models\">Recent developments in large language models</h1>\n\n<p>In 2018 the GPT  (<em>Generative Pre-trained Transformer</em>) model <sup id=\"fnref:GPT1:1\" role=\"doc-noteref\"><a href=\"#fn:GPT1\" class=\"footnote\" rel=\"footnote\">9</a></sup> by the company OpenAI started an avalanche of publications describing pre-trained neural networks based on the transformer architecture.  Now models could become more powerful just by throwing more compute power and data at them. Larger and larger models were trained.  The BERT (<em>Bidirectional Encoder Representations from Transformers</em>)<sup id=\"fnref:BERT:1\" role=\"doc-noteref\"><a href=\"#fn:BERT\" class=\"footnote\" rel=\"footnote\">8</a></sup> model by Google followed in the same year (2018). Both have similar architectures corresponding to a series of transformer blocks, making them more simple than the encoder-decoder architecture presented in <em>Attention is all you need</em>.</p>\n\n<p>Each year, larger and more powerful models followed. GPT-2 <sup id=\"fnref:GPT2\" role=\"doc-noteref\"><a href=\"#fn:GPT2\" class=\"footnote\" rel=\"footnote\">10</a></sup> was published in 2019. GPT-3 <sup id=\"fnref:GPT3\" role=\"doc-noteref\"><a href=\"#fn:GPT3\" class=\"footnote\" rel=\"footnote\">11</a></sup> followed in 2020 and showed great powers in solving a variety of language related tasks. Modern large language models (since GPT-3) already show impressive performance on downstream tasks even without the fine-tuning step. To achieve this, in-context learning is incorporated in the pre-training loop and at inference time. This is called meta-learning in the GPT-3 paper.<sup id=\"fnref:GPT3:1\" role=\"doc-noteref\"><a href=\"#fn:GPT3\" class=\"footnote\" rel=\"footnote\">11</a></sup> Here, examples of the task and solution (e.g. sentiment analysis) are shown as part of the input at the forward pass (in pre-training or at inference). Showing few examples at inference time is called few-shot learning. One-shot learning shows just one example and zero-shot learning shows no example.</p>\n\n<p>Even though GPT-3 was developed by a company with “Open” in its name, the trained model is not in fact open, but only accessible for a fee.</p>\n\n<p>In 2022 the <a href=\"https://opengpt-x.de/en/\">OpenGPT-X</a> project, funded by the German Federal Ministry of Economics and Climate Protection (BMWK), was launched with the goal to provide an independent and open large language model based in Europe and trained on English and German data. Other efforts to provide models of similar capabilities as GPT-3 more openly include the <a href=\"https://bigscience.huggingface.co/\">BigScience Research Workshop</a> and OPT (<em>Open Pretrained Transformer</em>) by Meta.<sup id=\"fnref:OPT\" role=\"doc-noteref\"><a href=\"#fn:OPT\" class=\"footnote\" rel=\"footnote\">12</a></sup></p>\n\n<h1 id=\"takeaways-and-learnings\">Takeaways and learnings</h1>\n\n<ul>\n  <li>Large language models have an incredibly wide range of applications. They will play a big role in our every day lifes very soon.</li>\n  <li>OpenGPT-X is the European answer to GPT-3.</li>\n  <li>Everybody interested in large-scale deep learning should look into the transformer architecture.</li>\n</ul>\n\n<p>I recently moved from numerical linear algebra, developing algorithms for solving structured eigenvalue problems, towards natural language processing with a focus on high performance computing. In my native language I would call a principal component analysis a singular value decomposition. This is why I have an instinct to look for matrices everywhere.  I want to conclude by sharing some of my personal learnings from switching fields.</p>\n<ul>\n  <li>AI research is extremely fast-paced. There are new interesting preprints coming out every week and it is hard to keep up. However, I have the feeling that the algorithms are on some level still immature just because the field is so young. Compared to algorithms from applied mathematics (say Krylow subspace methods to just name one example), the transformer architecture feels unpolished and arbitrary. There is a lot of research to be done on WHY it works as well as it does.</li>\n  <li>The open source spirit is alive and strong. The common development of code bases across multiple companies such as Nvidia, Microsoft, Meta, and HuggingFace, is something I could not have imagined to be a reality before seeing it with my own eyes.</li>\n  <li>Both these factors contribute to a wide availability of not only research publications but also didactic materials teaching state-of-the art research in an accessible manner.</li>\n</ul>\n\n<h1 id=\"sources\">Sources</h1>\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:Coursera\" role=\"doc-endnote\">\n      <p>Coursera course by Andrew Ng: <a href=\"https://www.coursera.org/learn/nlp-sequence-models/\">Sequence models</a> <a href=\"#fnref:Coursera\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:Speech\" role=\"doc-endnote\">\n      <p>Book by Dan Jurafsky and James H. Martin: <a href=\"https://web.stanford.edu/~jurafsky/slp3/\">Speech and Language Processing (3rd ed. draft)</a> <a href=\"#fnref:Speech\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:Speech:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a></p>\n    </li>\n    <li id=\"fn:Transfer\" role=\"doc-endnote\">\n      <p>Presentation by Thomas Wolf: <a href=\"https://www.youtube.com/watch?v=qWUslmU7BjY&t=695s\">An Introduction to Transfer Learning in NLP and HuggingFace</a> <a href=\"#fnref:Transfer\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:DL-CourseR\" role=\"doc-endnote\">\n      <p>Lecture series by Sebastian Raschka: <a href=\"https://sebastianraschka.com/blog/2021/dl-course.html\">Deep learning lecture videos by Sebastian Raschka</a>, in particular lecture <a href=\"https://sebastianraschka.com/blog/2021/dl-course.html#l19-self-attention-and-transformer-networks\">L19: Self-attention and transformer networks</a> <a href=\"#fnref:DL-CourseR\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:DL-Course-MIT\" role=\"doc-endnote\">\n      <p>Lecture series by MIT: <a href=\"http://introtodeeplearning.com/\">Introduction to Deep Learning</a>, in particular lecture 2 by Ava Solemany <a href=\"https://youtu.be/QvkQ1B3FBqA\">Deep Sequence Modeling</a> <a href=\"#fnref:DL-Course-MIT\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:LSTMs\" role=\"doc-endnote\">\n      <p>Blog post by Christopher Olah: <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTMS</a> <a href=\"#fnref:LSTMs\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:Attention\" role=\"doc-endnote\">\n      <p>Original transformer paper: <a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Attention is all you need</a>, 2017 <a href=\"#fnref:Attention\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:BERT\" role=\"doc-endnote\">\n      <p>BERT paper: <a href=\"https://arxiv.org/pdf/1810.04805v2.pdf\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, 2018 <a href=\"#fnref:BERT\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:BERT:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a></p>\n    </li>\n    <li id=\"fn:GPT1\" role=\"doc-endnote\">\n      <p>GPT-1 paper: <a href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\">Improving Language Understanding by Generative Pre-Training</a>, 2018 <a href=\"#fnref:GPT1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:GPT1:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a></p>\n    </li>\n    <li id=\"fn:GPT2\" role=\"doc-endnote\">\n      <p>GPT-2 paper: <a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">Language Models are Unsupervised Multitask Learners</a>, 2019 <a href=\"#fnref:GPT2\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:GPT3\" role=\"doc-endnote\">\n      <p>GPT-3 paper: <a href=\"https://arxiv.org/pdf/2005.14165.pdf\">Language models are few shot learners</a>, 2020 <a href=\"#fnref:GPT3\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a> <a href=\"#fnref:GPT3:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;<sup>2</sup></a></p>\n    </li>\n    <li id=\"fn:OPT\" role=\"doc-endnote\">\n      <p>Paper: <a href=\"https://arxiv.org/pdf/2205.01068.pdf\">OPT: Open Pre-trained Transformer Language Models</a>, 2022 <a href=\"#fnref:OPT\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>"},{"id":"https://x-dev.pages.jsc.fz-juelich.de//2022/06/29/10y-nvlab-workshop","title":"10 Year Anniversary Workshop of NVIDIA Application Lab at Jülich","link":"https://x-dev.pages.jsc.fz-juelich.de//2022/06/29/10y-nvlab-workshop","published":"2022-06-29T13:20:42+00:00","description":"On June 21 and 22, we held a workshop looking back on the last ten years of our lab together with NVIDIA – the NVIDIA Application Lab at Jülich (or NVLab, as I sometimes abbreviate it). The material can be...","isPermalink":false,"tags":["NVLab","Workshop"],"authors":[{"name":"Andreas","url":null}],"image":null,"modified":"2022-06-29T13:20:42+00:00","contentHtml":"<p>On June 21 and 22, we held a workshop looking back on the last ten years of our lab together with NVIDIA – the <a href=\"https://internet-live.fz-juelich.de/en/ias/jsc/about-us/structure/atml/atml-x-dev/nvlab/\">NVIDIA Application Lab at Jülich</a> (or <em>NVLab</em>, as I sometimes abbreviate it). The material can be found in the <a href=\"https://indico3-jsc.fz-juelich.de/event/35/timetable/\">agenda at Indico</a>.</p>\n\n<p><a href=\"/assets/img/posts/2022-06-29-10y-nvlab/mhrywniak-hopper.png\"><img alt=\"Screenshot from Markus Hrywniak's talk on Hopper H100\" title=\"A Kepler K20X had as many SMs as a Cluster Group of a Hopper H100.\" src=\"/assets/img/posts/2022-06-29-10y-nvlab/mhrywniak-hopper.tn.png\" srcset=\"/assets/img/posts/2022-06-29-10y-nvlab/mhrywniak-hopper.tn.png 1x, /assets/img/posts/2022-06-29-10y-nvlab/mhrywniak-hopper.tn.2x.png 2x, /assets/img/posts/2022-06-29-10y-nvlab/mhrywniak-hopper.tn.4x.png 4x\" /></a>\nWe invited a set of application owners with which we worked together during that time to present past developments, recent challenges, and future plans. On top of that, we had two other GPU-focused talks: <a href=\"https://www.linkedin.com/in/mhrywniak/\">Markus Hrywniak</a> from NVIDIA gave a presentation about some distinct features of NVIDIA’s next-generation GPU (<a href=\"https://www.nvidia.com/de-de/data-center/h100/\">Hopper H100</a>) and how they can be used for applications. <a href=\"/assets/img/posts/2022-06-29-10y-nvlab/dalvarez-juwelsbooster.png\"><img alt=\"Screenshot from Damian Alvarez's talk on JUWELS Booster\" title=\"History of GPU-equipped systems at JSC.\" src=\"/assets/img/posts/2022-06-29-10y-nvlab/dalvarez-juwelsbooster.tn.png\" srcset=\"/assets/img/posts/2022-06-29-10y-nvlab/dalvarez-juwelsbooster.tn.png 1x, /assets/img/posts/2022-06-29-10y-nvlab/dalvarez-juwelsbooster.tn.2x.png 2x, /assets/img/posts/2022-06-29-10y-nvlab/dalvarez-juwelsbooster.tn.4x.png 4x\" /></a>And <a href=\"https://www.fz-juelich.de/profile/alvarez_d\">Damian Alvarez</a> presented the current state of <a href=\"https://apps.fz-juelich.de/jsc/hps/juwels/booster-overview.html\">JUWELS Booster</a> and highlighted the work done in the lab to identify issues and shortcomings of the machine, seen and analyzed in close collaboration with specific users.</p>\n\n<p><a href=\"/assets/img/posts/2022-06-29-10y-nvlab/aherten-workshops.png\"><img alt=\"Screenshot from Andreas Herten's talk about 10 years of NVIDIA Application Lab at Jülich\" title=\"List of Workshops and Tutorials organized within the lab.\" src=\"/assets/img/posts/2022-06-29-10y-nvlab/aherten-workshops.tn.png\" srcset=\"/assets/img/posts/2022-06-29-10y-nvlab/aherten-workshops.tn.png 1x, /assets/img/posts/2022-06-29-10y-nvlab/aherten-workshops.tn.2x.png 2x, /assets/img/posts/2022-06-29-10y-nvlab/aherten-workshops.tn.4x.png 4x\" /></a>I also held a presentation – the opening presentation about all the things we did in the last ten years within the lab. Among other things, I counted 32 trainings held – with 11 additional trainings on conferences – and 18 workshops. I did not dare to count the optimized applications, in fear of forgetting one… Browsing through old material, I found a report about creation of the lab in the GCS <a href=\"https://www.gauss-centre.eu/news/publications/inside/\">InSiDE magazine</a> <em>Spring 2013</em> (<a href=\"https://www.gauss-centre.eu/fileadmin/user_upload/PR_News/Back_Issues_InSiDE/inside_spring13.pdf\">link to PDF</a>)<sup id=\"fnref:press-release\" role=\"doc-noteref\"><a href=\"#fn:press-release\" class=\"footnote\" rel=\"footnote\">1</a></sup>. An interesting snippet: “For many applications, using a single GPU is not sufficient, either because more computing power is required, or because the problem size is too large to fit into the memory of a single device. This forces application developers to not only consider parallelization at device level, but also to manage an additional level of parallelism.” – it seems to be a universal fact, still true today.</p>\n\n<p><a href=\"/assets/img/posts/2022-06-29-10y-nvlab/hderaedt-jucqs.png\"><img alt=\"Screenshot from Hans de Raedt's talk about the JUQCS Quantum Computer Simulator\" title=\"Scaling Visualization of JUQCS.\" src=\"/assets/img/posts/2022-06-29-10y-nvlab/hderaedt-jucqs.tn.png\" srcset=\"/assets/img/posts/2022-06-29-10y-nvlab/hderaedt-jucqs.tn.png 1x, /assets/img/posts/2022-06-29-10y-nvlab/hderaedt-jucqs.tn.2x.png 2x, /assets/img/posts/2022-06-29-10y-nvlab/hderaedt-jucqs.tn.4x.png 4x\" /></a>From application developers, we heard about quantum computer simulators – <a href=\"https://arxiv.org/abs/2104.03293\">general simulators</a> (<a href=\"https://www.fz-juelich.de/en/ias/jsc/about-us/structure/research-groups/qip/hans-de-raedt\">Hans de Raedt</a>) and simulators targeting specific aspects (<a href=\"https://www.fz-juelich.de/profile/willsch_d\">Dennis Willsch</a>) – which all have their own challenges, be it limited memory and extensive communication, or complicated communication patterns. <a href=\"https://www.hzdr.de/db/!ContMan.Visi.Card?pUid=4196&pNid=1483\">Alexander Debus</a> presented recent developments of <a href=\"https://github.com/ComputationalRadiationPhysics/picongpu\">PIConGPU</a>, a plasma physics simulator capable to scale to various large machines (including JUWELS Booster, of course) by using many sophisticated abstractions under the hood. In two talks held virtually from North America, we heard about current work done in brain image classification (<a href=\"https://www.fz-juelich.de/profile/schiffer_c\">Christian Schiffer</a>) and about simulations of polymeric systems (<a href=\"https://www.ludwigschneider.net/\">Ludwig Schneider</a>). <a href=\"/assets/img/posts/2022-06-29-10y-nvlab/cschiffer-brainz.png\"><img alt=\"Screenshot from Crhistian Schiffers's talk about Cytoarchitecture Classification in the Human Brain\" title=\"The computational needs of the semi-autonomous tool.\" src=\"/assets/img/posts/2022-06-29-10y-nvlab/cschiffer-brainz.tn.png\" srcset=\"/assets/img/posts/2022-06-29-10y-nvlab/cschiffer-brainz.tn.png 1x, /assets/img/posts/2022-06-29-10y-nvlab/cschiffer-brainz.tn.2x.png 2x, /assets/img/posts/2022-06-29-10y-nvlab/cschiffer-brainz.tn.4x.png 4x\" /></a>Christian presented a whole set of applications, which all work towards the goal of enabling semi-automatic, partly live classification of brain regions. Ludwig Schneider presented <a href=\"https://gitlab.com/InnocentBug/SOMA\">SOMA</a>, which uses OpenACC for acceleration, and was recently augmented by guidance functionality through Machine Learning. <a href=\"/assets/img/posts/2022-06-29-10y-nvlab/skesselheim-opengptx.png\"><img alt=\"Screenshot from Stefan Kesselheim's talk about OpenGPT-X\" title=\"The importance of largeness in Language Models.\" src=\"/assets/img/posts/2022-06-29-10y-nvlab/skesselheim-opengptx.tn.png\" srcset=\"/assets/img/posts/2022-06-29-10y-nvlab/skesselheim-opengptx.tn.png 1x, /assets/img/posts/2022-06-29-10y-nvlab/skesselheim-opengptx.tn.2x.png 2x, /assets/img/posts/2022-06-29-10y-nvlab/skesselheim-opengptx.tn.4x.png 4x\" /></a>In a talk about our fresh<sup id=\"fnref:new\" role=\"doc-noteref\"><a href=\"#fn:new\" class=\"footnote\" rel=\"footnote\">2</a></sup> <a href=\"https://opengpt-x.de/en/\">OpenGPT-X project</a>, <a href=\"https://www.fz-juelich.de/profile/kesselheim_s\">Stefan Kesselheim</a> highlighted the importance of large-scale language models and what exciting plans we have using JUWELS Booster for training open models.</p>\n\n<p>On the second day, a group of talks about weather and climate (<em>W&C</em>) simulations was started with a talk about <a href=\"https://github.com/slcs-jsc/mptrac\">MPTRAC</a> by <a href=\"https://www.fz-juelich.de/profile/hoffmann_l\">Lars Hoffmann</a>. MPTRAC is another OpenACC application we worked on in the past, which was recently augmented with sophisticated ideas to deal with the large amount of input data. Another W&C code is <a href=\"https://www.messy-interface.org/\">MESSy</a> – or rather a whole infrastructure for simulations – which we extensively work together with for quite some time now, but there are many pieces to this GPU puzzle, as shown in the talk by Kerstin Hartung. <a href=\"/assets/img/posts/2022-06-29-10y-nvlab/jhokkanen-parflow.png\"><img alt=\"Screenshot from Jaro Hokkanen's talk about ParFlow GPU\" title=\"Scaling of different GPU version of ParFlow.\" src=\"/assets/img/posts/2022-06-29-10y-nvlab/jhokkanen-parflow.tn.png\" srcset=\"/assets/img/posts/2022-06-29-10y-nvlab/jhokkanen-parflow.tn.png 1x, /assets/img/posts/2022-06-29-10y-nvlab/jhokkanen-parflow.tn.2x.png 2x, /assets/img/posts/2022-06-29-10y-nvlab/jhokkanen-parflow.tn.4x.png 4x\" /></a>Finally, our <a href=\"https://doi.org/10.1007/s10596-021-10051-4\">ParFlow GPU work</a> was presented by <a href=\"https://www.linkedin.com/in/jarohokkanen/?originalSubdomain=de\">Jaro Hokkannen</a>, who now works at CSC in Finland, but was so kind to share his past developments remotely. <a href=\"https://parflow.org/\">ParFlow</a> uses a custom, embedded <abbr title=\"Domain-Specific Language\">DSL</abbr>, to hide a specific backend behind pre-processor macros; with that, targeting different accelerators is comparable easy. Finally, two talks shared experiences with respect to handling Lattice-Boltzmann (<em>LB</em>) algorithms. For one, <a href=\"https://scholar.google.de/citations?user=_Zz89tQAAAAJ&hl=ja\">Fabio Schifano</a> presented about <em>D2Q37</em>, an LB application which has a long history with GPUs, but ventures into FPGAs right now. Funnily, Fabio and the D2Q37 code were already part of the very first Kick-Off Workshop 10 years ago! <a href=\"/assets/img/posts/2022-06-29-10y-nvlab/mgondrum-maiagpu.png\"><img alt=\"Screenshot from Miro Gondrum's/Moritz Waldmann's talk about M-AIA GPU porting activities\" title=\"Comparison of OpenMP and pSTL.\" src=\"/assets/img/posts/2022-06-29-10y-nvlab/mgondrum-maiagpu.tn.png\" srcset=\"/assets/img/posts/2022-06-29-10y-nvlab/mgondrum-maiagpu.tn.png 1x, /assets/img/posts/2022-06-29-10y-nvlab/mgondrum-maiagpu.tn.2x.png 2x, /assets/img/posts/2022-06-29-10y-nvlab/mgondrum-maiagpu.tn.4x.png 4x\" /></a>And as the last presentation, we heard about M-AIA (previously known as ZFS) and the efforts to port the application to GPUs using the parallel STL by <a href=\"https://de.linkedin.com/in/miro-gondrum-b61998186\">Miro Gondrum</a> and <a href=\"https://www.linkedin.com/in/moritzwaldmann91/\">Moritz Waldmann</a>; it was quite interesting to hear their views on portability.</p>\n\n<p>All in all, it was an amazing workshop, seeing the fruits of many years of work on applications and how developers progressed after the various forms of collaborations we had with them.</p>\n\n<p>Let’s do that again!</p>\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:press-release\" role=\"doc-endnote\">\n      <p>For the lab creation, also <a href=\"https://www.fz-juelich.de/de/aktuelles/news/pressemitteilungen/2012/12-06-19nvidia_application_lab\">a press release was made</a>. It contains a pretty cool picture, which Jiri Kraus (lab member of day 1) reminded me of. <a href=\"#fnref:press-release\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:new\" role=\"doc-endnote\">\n      <p>Actually not so fresh anymore. Time passes by so quickly! <a href=\"#fnref:new\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>"},{"id":"https://x-dev.pages.jsc.fz-juelich.de//2022/06/24/group-website","title":"X-Dev Website at fz-juelich.de","link":"https://x-dev.pages.jsc.fz-juelich.de//2022/06/24/group-website","published":"2022-06-24T14:20:42+00:00","description":"At the end of May, Forschungszentrum Jülich got a new website. Finally, the group also has a dedicated page there, listing the things we do, the project we are part of, and all the members in the group. And...","isPermalink":false,"tags":["XDev","FZJ"],"authors":[{"name":"Andreas","url":null}],"image":null,"modified":"2022-06-24T14:20:42+00:00","contentHtml":"<p>At the end of May, Forschungszentrum Jülich got a new website. Finally, the group also has a dedicated page there, listing the things we do, the project we are part of, and all the members in the group.<br />\nAnd thanks to a photo shooting by Sebastian, we have great portraits and an amazing group picture.</p>\n\n<p>Check it out at</p>\n\n<p><span style=\"text-align: center; font-size: larger\"><a href=\"https://www.fz-juelich.de/en/ias/jsc/about-us/structure/atml/atml-x-dev\">→ fz-juelich.de/en/ias/jsc/about-us/structure/atml/atml-x-dev</a></span></p>\n\n<video width=\"100%\" controls=\"\">\n\t<source src=\"/assets/videos/posts/2022-06-24-xdev-website/xdev-website.mp4\" type=\"video/mp4\" />\n</video>"},{"id":"https://x-dev.pages.jsc.fz-juelich.de//2022/06/24/isc-maelstrom-benchmarks","title":"MAELSTROM: First benchmarks at ISC22","link":"https://x-dev.pages.jsc.fz-juelich.de//2022/06/24/isc-maelstrom-benchmarks","published":"2022-06-24T13:20:42+00:00","description":"On June 1, in the context of the AI and HPC for Weather and Climate Session at ISC2022, we presented the very first benchmark results for the MAELSTROM Machine Learning applications. The work presented...","isPermalink":false,"tags":["MAELSTROM","ISC22","Presentation"],"authors":[{"name":"Stepan","url":null}],"image":null,"modified":"2022-06-24T13:20:42+00:00","contentHtml":"<p>On June 1, in the context of the <a href=\"https://app.swapcard.com/widget/event/isc-high-performance-2022/planning/UGxhbm5pbmdfODYxMjc1\"><em>AI and HPC for Weather and Climate</em></a> Session at ISC2022, we presented the very first <a href=\"https://app.swapcard.com/widget/event/isc-high-performance-2022/planning/UGxhbm5pbmdfODg1NTcx\">benchmark results</a> for the MAELSTROM Machine Learning applications.</p>\n\n<p>The work presented highlights of some of the results obtained in the <a href=\"https://www.maelstrom-eurohpc.eu/content/docs/uploads/doc15.pdf\">MAELSTROM Deliverable D3.4</a> and was shown following a short presentation by Daniele Gregori (E4), who introduced the systems at E4 and JSC that were used to gather the benchmark results.</p>\n\n<p>We have seen that the applications have quite different performance behaviors and have identified multiple areas of interest for further and deeper investigation. We are looking forward to the months to come in MAELSTROM!</p>\n\n<iframe width=\"800\" height=\"500\" src=\"/assets/slides/posts/2022-06-01-isc-maelstrom-benchmarks/ISC2022-MAELSTROM.pdf\">Your browser is unable to display the PDF. Instead, please download it.</iframe>"}]}